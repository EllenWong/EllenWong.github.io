<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Weekly-221015</title>
    <link href="/2022/10/15/1-Weekly-221015/"/>
    <url>/2022/10/15/1-Weekly-221015/</url>
    
    <content type="html"><![CDATA[<h3 id="周记：论文阅读（不含工作日）"><a href="#周记：论文阅读（不含工作日）" class="headerlink" title="周记：论文阅读（不含工作日）"></a>周记：论文阅读（不含工作日）</h3><span id="more"></span><blockquote><div>            <input type="checkbox" disabled checked="checked">Foundation Transformers          </div></blockquote><h2 id="Foundation-Transformers"><a href="#Foundation-Transformers" class="headerlink" title="Foundation Transformers"></a>Foundation Transformers</h2><p><sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://arxiv.org/abs/2210.06423">[1]</span></a></sup>随着Transformer的发展与完善，人工智能领域的许多任务都有了相关的应用（如Vision、Language与Speech等）。由于任务的不同，我们通常会采用不同的网络架构。</p><h3 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h3><ul><li><strong>目的：</strong> 提出<code>MAGNETO</code>，建立Transformer的通用模型，介绍其不同的变种（针对不同任务），保证训练稳定性。</li><li><strong>基础的Transformer模型：</strong> <sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://arxiv.org/abs/1706.03762">[2]</span></a></sup>如下图所示，以注意力机制连接编码器（左）与解码器（右），是一个实现seq2seq的模型，最初在机器翻译的任务上体现了其独特的优势（对长距离信息提取十分有效）。<br><img src="/images/1-weeklynote/221015/1.png" alt="Transformer基本架构"></li><li><code>MAGNETO</code><strong>的改进</strong>：网络结构与初始化方法。<ul><li><strong>网络结构：</strong> 与<code>Pre-LN</code>相比，<code>Sub-LN</code>对每一个子层(multi-head self-attention、feed-forward network)的输入投影之前和输出投影之前引入了另一种层归一化机制。</li><li><strong>初始化方法：</strong> 采用<code>DeepNet</code><sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://arxiv.org/abs/2203.00555">[3]</span></a></sup>的初始化机制，这一做法极大提高了训练稳定性，可无副作用地使模型尺寸放大(也就是层数变多)。</li></ul></li></ul><p><img src="/images/1-weeklynote/221015/2.png" alt="论文主要贡献（我觉得最主要的一幅图)"></p><h3 id="2-网络结构：Sub-LayerNorm"><a href="#2-网络结构：Sub-LayerNorm" class="headerlink" title="2. 网络结构：Sub-LayerNorm"></a>2. 网络结构：Sub-LayerNorm</h3><p><img src="/images/1-weeklynote/221015/3.png" alt="网络结构对比(注意看红色部分，即本文的改进)"></p><ul><li><strong>在输入的qkv投影之前加入的LN层：</strong> 其中，W代表多头注意力机制的一些参数。<script type="math/tex; mode=display">\begin{aligned}Q, K, V &=W^{Q} \mathrm{LN}(x), W^{K} \mathrm{LN}(x), W^{V} \mathrm{LN}(x) \\\operatorname{MSA}(x) &=x+W^{O} \mathrm{LN}(\operatorname{Attention}(Q, K, V))\end{aligned}</script></li><li><strong>在前馈网络中加入的LN层：</strong> 其中，<script type="math/tex">\phi</script> 代表非线性激活层。<script type="math/tex; mode=display">\begin{aligned}\mathrm{FC}_{1}(x) &=W^{1} \mathrm{LN}(x) \\\mathrm{FC}_{2}(x) &=W^{2} \mathrm{LN}(x) \\\mathrm{FFN}(x) &=\mathrm{FC}_{2}\left(\phi\left(\mathrm{FC}_{1}(x)\right)\right)\end{aligned}</script></li></ul><h3 id="3-初始化方法：详细推导见DeepNet"><a href="#3-初始化方法：详细推导见DeepNet" class="headerlink" title="3. 初始化方法：详细推导见DeepNet"></a>3. 初始化方法：详细推导见DeepNet</h3><ul><li><strong>结论：</strong> MAGNETO的初始化方法相比于原始模型更具稳定性（可适用于更大的模型）。</li><li><p><strong>Pre-LN的预期模型更新：</strong> </p><ul><li><strong>前向传播过程：</strong> 其中 <script type="math/tex">x^{l-1}</script> 与 <script type="math/tex">x^{l}</script> 分别代表 <script type="math/tex">l-th</script> 子层 <script type="math/tex">G^{l}</script>（Self- Attention MSA）的输入和输出。<script type="math/tex; mode=display">\begin{array}{l}F(x ; \theta)=W^{v o c a b} x^{e} \\x^{e}=\operatorname{LN}\left(x+\sum_{l=1}^{L} G^{l}\left(x^{l-1}, \theta_{e l}\right)\right), \quad x^{l}=G^{l}\left(x^{l-1}, \theta_{e l}\right) \text { and } x^{0}=x\end{array}</script></li><li><strong>前向计算表示：</strong> 其中W代表模型参数，参数量越多，说明模型越大。<script type="math/tex; mode=display">x^{l}=x^{l-1}+W^{l, 2} \phi\left(W^{l, 1} \mathrm{LN}\left(x^{l-1}\right)\right)</script></li></ul></li><li><p><strong>MAGNETO的预期模型更新：</strong> </p><ul><li><strong>前向计算表示：</strong> 与上式对比。<script type="math/tex; mode=display">x^{l}=x^{l-1}+W^{l, 2} \mathrm{LN}\left(\phi\left(W^{l, 1} \mathrm{LN}\left(x^{l-1}\right)\right)\right)</script></li></ul></li><li><p><strong>方法对比：</strong></p><ul><li><strong>Pre-LN 参数更新</strong><br><img src="/images/1-weeklynote/221015/5.png" alt=""></li><li><strong>MAGNETO 参数更新</strong><br><img src="/images/1-weeklynote/221015/6.png" alt=""></li><li><strong>对权重的处理：</strong> 见论文主要贡献图中表格部分。</li></ul></li></ul><h3 id="4-实验部分：详见论文（贴一张视觉部分的）"><a href="#4-实验部分：详见论文（贴一张视觉部分的）" class="headerlink" title="4. 实验部分：详见论文（贴一张视觉部分的）"></a>4. 实验部分：详见论文（贴一张视觉部分的）</h3><p><img src="/images/1-weeklynote/221015/4.png" alt=""></p><h3 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h3><ul><li>本文主要目的是进一步提高Transformer的性能，其一是改变网络结构，增加LN层，其二是提高模型稳定性，对模型的参数进行一定的约束，得到一种更加通用的架构，并适用于尽可能多的模型规模。</li></ul><h3 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h3><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://arxiv.org/abs/2210.06423">https://arxiv.org/abs/2210.06423</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a><a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span><a href="https://arxiv.org/abs/2203.00555">https://arxiv.org/abs/2203.00555</a><a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    
    <tags>
      
      <tag>Transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Weekly-220619</title>
    <link href="/2022/06/19/1-Weekly-220619/"/>
    <url>/2022/06/19/1-Weekly-220619/</url>
    
    <content type="html"><![CDATA[<h3 id="本周学习汇报"><a href="#本周学习汇报" class="headerlink" title="本周学习汇报"></a>本周学习汇报</h3><span id="more"></span><blockquote><div>            <input type="checkbox" disabled checked="checked">Elucidating the Design Space of Diffusion-Based Generative Models          </div><div>            <input type="checkbox" disabled checked="checked">Improved Vector Quantized Diffusion Models          </div><div>            <input type="checkbox" disabled checked="checked">关于对比学习          </div><div>            <input type="checkbox" disabled checked="checked">关于遥感图像迁移学习          </div></blockquote><h2 id="1-扩散模型子空间设计"><a href="#1-扩散模型子空间设计" class="headerlink" title="1. 扩散模型子空间设计"></a>1. 扩散模型子空间设计</h2><h2 id="Elucidating-the-Design-Space-of-Diffusion-Based-Generative-Models"><a href="#Elucidating-the-Design-Space-of-Diffusion-Based-Generative-Models" class="headerlink" title="Elucidating the Design Space of Diffusion-Based Generative Models"></a>Elucidating the Design Space of Diffusion-Based Generative Models</h2><p><img src="/images/1-weeklynote/220619/1-1.png" alt=""></p><h3 id="1-1-Introduction"><a href="#1-1-Introduction" class="headerlink" title="1.1. Introduction"></a>1.1. Introduction</h3><ul><li>Identify several changes to both <strong>the sampling</strong> and <strong>training processes</strong>, as well as preconditioning of <strong>the score networks</strong>.</li><li><strong>Literatures:</strong> dense on theory, and derivations of sampling schedule, training dynamics, noise level parameterization.</li><li><strong>Corresponding dangers:</strong> obscuring the available design space— a proposed model may appear as a tightly coupled package where <strong>no individual component can be modified</strong> without breaking the entire system. </li><li><strong>Contributions:</strong><ul><li><strong>About denoising score matching:</strong> to obtain better insights into how these components are linked together and what degrees of freedom are available in the design of the overall system.</li><li><strong>A higher-order Runge–Kutta method:</strong> concerns <strong>the sampling processes</strong> used to synthesize images using diffusion models.</li><li><strong>the training of the score-modeling neural network:</strong> Derive best practices for improving the training dynamics. </li></ul></li></ul><h3 id="1-2-Methodology"><a href="#1-2-Methodology" class="headerlink" title="1.2. Methodology"></a>1.2. Methodology</h3><h4 id="1-2-1-Expressing-diffusion-models-in-a-common-framework"><a href="#1-2-1-Expressing-diffusion-models-in-a-common-framework" class="headerlink" title="1.2.1 Expressing diffusion models in a common framework"></a>1.2.1 Expressing diffusion models in a common framework</h4><p><img src="/images/1-weeklynote/220619/1-2.png" alt=""></p><h4 id="1-2-2-Improvements-to-deterministic-sampling"><a href="#1-2-2-Improvements-to-deterministic-sampling" class="headerlink" title="1.2.2 Improvements to deterministic sampling"></a>1.2.2 Improvements to deterministic sampling</h4><p><img src="/images/1-weeklynote/220619/1-3.png" alt=""></p><h3 id="1-3-Conclusion-and-Future-Work"><a href="#1-3-Conclusion-and-Future-Work" class="headerlink" title="1.3. Conclusion and Future Work"></a>1.3. Conclusion and Future Work</h3><ul><li><strong>Modular design:</strong> putting diffusion models to a common framework.</li><li><strong>The current high-resolution diffusion models:</strong> separate super-resolution steps, subspace projection, very large networks, or hybrid approaches.</li></ul><h2 id="2-VQVAE与扩散模型的融合"><a href="#2-VQVAE与扩散模型的融合" class="headerlink" title="2. VQVAE与扩散模型的融合"></a>2. VQVAE与扩散模型的融合</h2><h2 id="Improved-Vector-Quantized-Diffusion-Models"><a href="#Improved-Vector-Quantized-Diffusion-Models" class="headerlink" title="Improved Vector Quantized Diffusion Models"></a>Improved Vector Quantized Diffusion Models</h2><p><img src="/images/1-weeklynote/220619/2-1.png" alt=""></p><h3 id="2-1-Introduction"><a href="#2-1-Introduction" class="headerlink" title="2.1. Introduction"></a>2.1. Introduction</h3><ul><li><p><strong>Denoising Diffusion Probability Models:</strong></p><ul><li>Continuous diffusion models: More. </li><li>Discrete diffusion models: This paper.</li></ul></li><li><p><strong>Studies:</strong> </p><ul><li>Better network architecture.</li><li>Hierarchical structure design. </li><li>Alternative loss function. </li><li>Fast sampling strategy.</li></ul></li><li><p><strong>Contribution:</strong></p><ul><li>Explore <strong>classifier-free guidance sampling</strong> for <code>discrete denoising diffusion model</code> and propose a more general and effective implementation of classifier-free guidance. </li><li>Present a <strong>high-quality inference strategy</strong> to alleviate the joint distribution issue in VQ-Diffusion.</li></ul></li></ul><h3 id="2-2-Related-Work"><a href="#2-2-Related-Work" class="headerlink" title="2.2. Related Work"></a>2.2. Related Work</h3><ul><li><strong>VQ-Diffusion</strong><br><img src="/images/1-weeklynote/220619/2-2.png" alt=""></li></ul><h3 id="2-3-Methodology"><a href="#2-3-Methodology" class="headerlink" title="2.3. Methodology"></a>2.3. Methodology</h3><ul><li><p><strong>Discrete Classifier-free Guidance</strong><br><img src="/images/1-weeklynote/220619/2-3.png" alt=""></p></li><li><p><strong>High-quality Inference Strategy</strong><br><img src="/images/1-weeklynote/220619/2-4.png" alt=""></p></li></ul><h3 id="2-4-Experiments"><a href="#2-4-Experiments" class="headerlink" title="2.4. Experiments"></a>2.4. Experiments</h3><ul><li><p><strong>Quantitative Analysis：</strong><br><img src="/images/1-weeklynote/220619/2-5.png" alt=""></p></li><li><p><strong>Qualitative Analysis：</strong><br><img src="/images/1-weeklynote/220619/2-6.png" alt=""></p></li></ul><h3 id="2-5-Conclusion-and-Future-Work"><a href="#2-5-Conclusion-and-Future-Work" class="headerlink" title="2.5. Conclusion and Future Work"></a>2.5. Conclusion and Future Work</h3><ul><li><strong>Two issues:</strong>  the posterior issue and the joint distribution issue.</li><li><strong>Proposed:</strong> two techniques and improve the quality of the generated samples and their consistency with the input text by a large margin.</li></ul><h2 id="3-关于对比学习的一个报告"><a href="#3-关于对比学习的一个报告" class="headerlink" title="3. 关于对比学习的一个报告"></a>3. 关于对比学习的一个报告</h2><p><a href="https://www.bilibili.com/video/BV1CS4y1i7ct?spm_id_from=333.999.0.0&amp;vd_source=b4fea842ec93dbb4bd4be786633acac3">参考视频</a><br><img src="/images/1-weeklynote/220619/3-1.png" alt=""></p><ul><li><strong>个人OneNote笔记位置：</strong> <code>1-Note|完备 / 1-Learning｜学习 / 103-Method-CL｜对比学习 / 对比学习报告——如何更好地理解对比学习</code></li></ul><h2 id="4-关于遥感图像迁移学习"><a href="#4-关于遥感图像迁移学习" class="headerlink" title="4. 关于遥感图像迁移学习"></a>4. 关于遥感图像迁移学习</h2><h3 id="4-1-问题描述—域间差异问题"><a href="#4-1-问题描述—域间差异问题" class="headerlink" title="4.1 问题描述—域间差异问题"></a>4.1 问题描述—域间差异问题</h3><ul><li><strong>动机：</strong> 遥感图像场景分类中受到的一些限制—-&gt;需提高模型整体泛化性(<code>迁移学习</code>)<ul><li><strong>样本数量角度：</strong> 遥感图像样本数量少.</li><li><strong>遥感任务角度：</strong> 遥感图像的处理一般是在同一个域中，即训练集和测试集会来源于同一类，当存在一个新的无标签信息的数据集需要场景分类时，则需要<strong>跨域场景分类</strong>。</li></ul></li><li><strong>目的：</strong> 减少域间差异，提高分类(下游任务)性能。</li></ul><h3 id="4-2-遥感图像关于域的部分研究现状"><a href="#4-2-遥感图像关于域的部分研究现状" class="headerlink" title="4.2 遥感图像关于域的部分研究现状"></a>4.2 遥感图像关于域的部分研究现状</h3><p><a href="https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&amp;dbname=CMFD202201&amp;filename=1021900387.nh&amp;uniplatform=NZKPT&amp;v=d-Dqokomep73gNWNjkw5TvnF_eiQsuhxMmXV5xw3CxI0AmFcdIh6jLvWzw4PF6lD">论文参考—基于深度迁移学习的遥感图像场景分类方法研究</a><br><img src="/images/1-weeklynote/220619/4-1.png" alt=""></p><h3 id="4-3-方法分析—部分解决方案的可行性"><a href="#4-3-方法分析—部分解决方案的可行性" class="headerlink" title="4.3 方法分析—部分解决方案的可行性"></a>4.3 方法分析—部分解决方案的可行性</h3><ul><li><strong>传统数学的角度：</strong> 采用机器学习的方式，将域A的图像迁移到域B上，对于每张图像<code>Xa</code>（真实图像）而言，其坐标或其他特征与图像<code>Xb</code>（生成图像）一一对应（像素级任务）。<ul><li><strong>难点：</strong> 映射公式(模型或线性关系)的创建 + 图像真实性的判断（损失函数）+ 结果评估（评价指标）</li><li><strong>重点：</strong> 不同设备不同时间拍摄图像之间的相似与不同，如何一一对应的问题。</li></ul></li><li><strong>深度学习的角度：</strong> <ul><li><strong>难点：</strong> 隐式关系的学习（公共特征空间的构建）+ 距离度量（KL散度？MMD？）+ 具体模型构建（对比学习？其他特征提取网络？）+ 其他问题（如何应对数据集少以及分辨率高的问题？域自适应中的—半监督/无监督学习）</li><li><strong>重点：</strong> 怎样实现单一图像之间的转变，并保证图像信息量无缺失。</li></ul></li></ul><h2 id="5-下一步计划（关于遥感图像）"><a href="#5-下一步计划（关于遥感图像）" class="headerlink" title="5. 下一步计划（关于遥感图像）"></a>5. 下一步计划（关于遥感图像）</h2><ul><li>对图像转换任务的整体性把握</li><li>相关方向论文的调研</li><li>相关数据集考察（需请教师兄）</li><li>具体实现baseline确立</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Contrastive Learning</tag>
      
      <tag>Diffusion Model</tag>
      
      <tag>Remote Sensing Image</tag>
      
      <tag>Transfer Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Weekly-220612</title>
    <link href="/2022/06/12/1-Weekly-220612/"/>
    <url>/2022/06/12/1-Weekly-220612/</url>
    
    <content type="html"><![CDATA[<h3 id="本周学习汇报"><a href="#本周学习汇报" class="headerlink" title="本周学习汇报"></a>本周学习汇报</h3><span id="more"></span><blockquote><div>            <input type="checkbox" disabled checked="checked">电子信息实践课大作业          </div><div>            <input type="checkbox" disabled checked="checked">扩散模型相关          </div><div>            <input type="checkbox" disabled checked="checked">Hyperspherical Consistency Regularization          </div><div>            <input type="checkbox" disabled checked="checked">CoMatch- Semi-supervised Learning with Contrastive Graph Regularization          </div><div>            <input type="checkbox" disabled checked="checked">DiVAE : Photorealistic Images Synthesis with Denoising Diffusion Decoder          </div></blockquote><h2 id="1-电子信息实践课大作业"><a href="#1-电子信息实践课大作业" class="headerlink" title="1. 电子信息实践课大作业"></a>1. 电子信息实践课大作业</h2><p><img src="/images/1-weeklynote/220612/1-1.png" alt=""></p><h2 id="2-关于扩散模型"><a href="#2-关于扩散模型" class="headerlink" title="2. 关于扩散模型"></a>2. 关于扩散模型</h2><p><a href="https://developer.nvidia.com/zh-cn/blog/improving-diffusion-models-as-an-alternative-to-gans-part-1/">参考1</a><br><a href="https://developer.nvidia.com/zh-cn/blog/improving-diffusion-models-as-an-alternative-to-gans-part-2/">参考2</a></p><h3 id="2-1-生成式学习的三个需求"><a href="#2-1-生成式学习的三个需求" class="headerlink" title="2.1 生成式学习的三个需求"></a>2.1 生成式学习的三个需求</h3><p><img src="/images/1-weeklynote/220612/2-1.png" alt=""></p><h3 id="2-2-扩散模型的思想"><a href="#2-2-扩散模型的思想" class="headerlink" title="2.2 扩散模型的思想"></a>2.2 扩散模型的思想</h3><ul><li><p><strong>正向扩散：</strong> 前向扩散过程通过逐渐扰动输入数据将数据映射为噪声。这是通过一个简单的随机过程正式实现的，该过程从数据样本开始，使用简单的高斯扩散核迭代生成噪声较大的样本。也就是说，在这个过程的每一步，高斯噪声都会逐渐添加到数据中。<br><img src="/images/1-weeklynote/220612/2-2.png" alt=""></p></li><li><p><strong>参数化反向扩散：</strong> 参数化的反向过程，取消正向扩散并执行迭代去噪。这个过程代表数据合成，并经过训练，通过将随机噪声转换为真实数据来生成数据。它也被正式定义为一个随机过程，使用可训练的深度神经网络对输入图像进行迭代去噪。<br><img src="/images/1-weeklynote/220612/2-3.png" alt=""></p></li></ul><h3 id="2-3-扩散模型存在的问题-生成过程慢、采样慢"><a href="#2-3-扩散模型存在的问题-生成过程慢、采样慢" class="headerlink" title="2.3 扩散模型存在的问题-生成过程慢、采样慢"></a>2.3 扩散模型存在的问题-生成过程慢、采样慢</h3><h4 id="2-3-1-潜空间扩散模型"><a href="#2-3-1-潜空间扩散模型" class="headerlink" title="2.3.1 潜空间扩散模型"></a>2.3.1 潜空间扩散模型</h4><ul><li>NVIDIA 推出了 <code>基于潜在分数的生成模型 （ LSGM ）</code>, 可以在潜在空间而不是直接在数据空间中训练扩散模型。<br><img src="/images/1-weeklynote/220612/2-4.png" alt=""></li><li>优势：合成速度、表现力，以及定制的编码器和解码器。</li><li>基本上简化了数据本身，首先将其嵌入平滑的潜在空间，在那里可以训练更有效的扩散模型。</li></ul><h4 id="2-3-2-临界阻尼朗之万扩散-CLD"><a href="#2-3-2-临界阻尼朗之万扩散-CLD" class="headerlink" title="2.3.2 临界阻尼朗之万扩散(CLD)"></a>2.3.2 临界阻尼朗之万扩散(CLD)</h4><ul><li><strong>问题：</strong> 扩散模型中的一个关键因素是固定前向扩散过程，以逐渐扰动数据。与数据本身一起，它唯一地决定了去噪模型学习的难度。<br><img src="/images/1-weeklynote/220612/2-5.png" alt=""></li><li><strong>优势：</strong> 使用定制 SDE 解算器加速采样，有更简单的评价函数。</li><li>是一种改进的前向扩散过程，特别适合于更简单、更快的去噪和生成。</li></ul><h4 id="2-3-3-扩散算子去噪"><a href="#2-3-3-扩散算子去噪" class="headerlink" title="2.3.3 扩散算子去噪"></a>2.3.3 扩散算子去噪</h4><ul><li>使用条件 GAN 对去噪分布进行建模。<br><img src="/images/1-weeklynote/220612/2-6.gif" alt=""></li><li>通过表达性多峰去噪分布，直接学习显著加速的反向去噪过程。</li></ul><h3 id="2-4-与VAE的对比"><a href="#2-4-与VAE的对比" class="headerlink" title="2.4 与VAE的对比"></a>2.4 与VAE的对比</h3><p><img src="/images/1-weeklynote/220612/2-7.png" alt=""></p><h2 id="3-关于对比学习中的正则化"><a href="#3-关于对比学习中的正则化" class="headerlink" title="3. 关于对比学习中的正则化"></a>3. 关于对比学习中的正则化</h2><h2 id="题目：-Hyperspherical-Consistency-Regularization"><a href="#题目：-Hyperspherical-Consistency-Regularization" class="headerlink" title="题目： Hyperspherical Consistency Regularization"></a>题目： Hyperspherical Consistency Regularization</h2><p><img src="/images/1-weeklynote/220612/3-1.png" alt=""><br><code>/2-Learn Method/4-Contrastive Learning/2022-Hyperspherical Consistency Regularization(CVPR).pdf</code></p><h3 id="3-1-Introduction"><a href="#3-1-Introduction" class="headerlink" title="3.1. Introduction"></a>3.1. Introduction</h3><ul><li><strong>Common scheme of contrasive learning:</strong> Jointly training <code>supervised learning</code> and <code>unsupervised learning</code> with a shared feature encoder. Taking advantage of both <code>feature-dependent information</code> from <code>self-supervised learning</code> and <code>label-dependent information</code> from <code>supervised learning</code>.</li><li><strong>Motivation:</strong> This scheme remains suffering from <strong>bias of the classifier</strong>- the classifier which determines the ultimate predictions still suffers from the bias of semi-supervision or weak-supervision.</li><li><strong>Contributions:</strong><ul><li>Analyze the relationship between the projection head and the classifier.</li><li><strong>Proposed:</strong>  hyperspherical consistency regularization (<strong>HCR</strong>)- to regularize the classifier using feature-dependent information and thus avoid bias from labels.<br><img src="/images/1-weeklynote/220612/3-2.png" alt=""></li></ul></li></ul><h3 id="3-2-Related-Work"><a href="#3-2-Related-Work" class="headerlink" title="3.2. Related Work"></a>3.2. Related Work</h3><ul><li><strong>Contrastive learning:</strong> HCR builds a bridge between classical <strong>supervised learning</strong> and <strong>pretext tasks in self-supervised learning</strong>, and the regularization is plug-and-play to apply in these joint-learning methods.</li><li><strong>Learning on the hypersphere:</strong> Reprojects <code>the Euclidean feature space</code> of the classifier into <code>a hypersphere</code> and explores its connection with the projection head’s hypersphere.</li></ul><h3 id="3-3-Methodology"><a href="#3-3-Methodology" class="headerlink" title="3.3. Methodology"></a>3.3. Methodology</h3><p><img src="/images/1-weeklynote/220612/3-3.png" alt=""></p><h3 id="3-4-Experiments"><a href="#3-4-Experiments" class="headerlink" title="3.4. Experiments"></a>3.4. Experiments</h3><ul><li>Semi-supervised learning</li><li>Fine-grained classification</li><li>Noisy label learning<br><img src="/images/1-weeklynote/220612/3-4.png" alt=""></li></ul><h3 id="3-5-Conclusion-and-Future-Work"><a href="#3-5-Conclusion-and-Future-Work" class="headerlink" title="3.5. Conclusion and Future Work"></a>3.5. Conclusion and Future Work</h3><ul><li>Propose a <strong>novel consistency regularization method</strong> for semi-supervised and weak-supervised learning, called hyperspherical consistency regularization (HCR).</li><li><strong>HCR:</strong> Encourage <strong>the pairwise distance distribution of the classifier</strong> to be similar to the distribution of the projection head in the latent space.</li></ul><h2 id="4-半监督学习中的对比图正则化"><a href="#4-半监督学习中的对比图正则化" class="headerlink" title="4. 半监督学习中的对比图正则化"></a>4. 半监督学习中的对比图正则化</h2><h2 id="CoMatch-Semi-supervised-Learning-with-Contrastive-Graph-Regularization"><a href="#CoMatch-Semi-supervised-Learning-with-Contrastive-Graph-Regularization" class="headerlink" title="CoMatch- Semi-supervised Learning with Contrastive Graph Regularization"></a>CoMatch- Semi-supervised Learning with Contrastive Graph Regularization</h2><p><img src="/images/1-weeklynote/220612/4-1.png" alt=""><br><a href="https://github.com/salesforce/CoMatch/">GitHub</a><br><code>/2-Learn Method/4-Contrastive Learning/2021-CoMatch- Semi-supervised Learning with Contrastive Graph Regularization(ICCV).pdf</code></p><h3 id="4-1-Introduction"><a href="#4-1-Introduction" class="headerlink" title="4.1. Introduction"></a>4.1. Introduction</h3><ul><li><strong>Semi-supervised learning:</strong>  learning from few labeled data and a large amount of unlabeled data.<ul><li>using the model’s class prediction to produce <strong>a pseudo-label for each unlabeled sample</strong> as the label to train against.BUT heavily rely on the quality of the model’s class prediction.</li><li><strong>unsupervised or self-supervised pre-training</strong>, followed by supervised fine-tuning and pseudo-labeling.BUT Self-supervised learning methods are task-agnostic.</li></ul></li><li><strong>graph-based semi-supervised learning:</strong> Not shown competitive performance on ImageNet.</li><li><strong>CoMatch:</strong> a new semi-supervised learning method that addresses the existing limitations.<ul><li><strong>The classification head:</strong> Using memory-smoothed pseudo-labels, where pseudo-labels are refined by aggregating information from nearby samples in the embedding space.</li><li><strong>The projection head:</strong> using contrastive learning on a pseudo-label graph, where samples with similar pseudo-labels are trained to have similar embeddings.<br><img src="/images/1-weeklynote/220612/4-2.png" alt=""></li></ul></li></ul><h3 id="4-2-Related-Work"><a href="#4-2-Related-Work" class="headerlink" title="4.2. Related Work"></a>4.2. Related Work</h3><ul><li>Consistency regularization.</li><li>Entropy minimization.</li><li>Self-supervised contrastive learning.</li><li>Graph-based semi-supervised learning.</li></ul><h3 id="4-3-Methodology"><a href="#4-3-Methodology" class="headerlink" title="4.3. Methodology"></a>4.3. Methodology</h3><ul><li>Overview</li><li>CoMatch<ul><li>Memory-smoothed pseudo-labeling</li><li>Graph-based contrastive learning</li></ul></li><li>Scalable learning with an EMA model<br><img src="/images/1-weeklynote/220612/4-3.png" alt=""></li></ul><h3 id="4-4-Experiments"><a href="#4-4-Experiments" class="headerlink" title="4.4. Experiments"></a>4.4. Experiments</h3><p><img src="/images/1-weeklynote/220612/4-4.png" alt=""></p><h3 id="4-5-Conclusion-and-Future-Work"><a href="#4-5-Conclusion-and-Future-Work" class="headerlink" title="4.5. Conclusion and Future Work"></a>4.5. Conclusion and Future Work</h3><ul><li><strong>Co-training</strong> of class probabilities and image embeddings.</li><li>Memory-smoothed pseudo-labeling to <strong>mitigate confirmation bias</strong>.</li><li>Graph-based contrastive learning to <strong>learn better representations</strong>.</li></ul><h2 id="5-图像生成—去噪扩散解码器"><a href="#5-图像生成—去噪扩散解码器" class="headerlink" title="5. 图像生成—去噪扩散解码器"></a>5. 图像生成—去噪扩散解码器</h2><h2 id="DiVAE-Photorealistic-Images-Synthesis-with-Denoising-Diffusion-Decoder"><a href="#DiVAE-Photorealistic-Images-Synthesis-with-Denoising-Diffusion-Decoder" class="headerlink" title="DiVAE : Photorealistic Images Synthesis with Denoising Diffusion Decoder"></a>DiVAE : Photorealistic Images Synthesis with Denoising Diffusion Decoder</h2><p><img src="/images/1-weeklynote/220612/5-1.png" alt=""><br><code>/3-Generate/1-Network/5-DPM/2022-DiVAE - Photorealistic Images Synthesis with Denoising Diffusion Decoder.pdf</code></p><h3 id="5-1-Introduction"><a href="#5-1-Introduction" class="headerlink" title="5.1. Introduction"></a>5.1. Introduction</h3><ul><li><strong>Diffusion models</strong> have shown be capacity to generate high-quality synthetic images.</li><li><strong>Propose:</strong> VQ-VAE architecture model with a diffusion decoder (<strong>DiVAE</strong>)—work as <strong>the reconstructing component</strong> in image synthesis.</li><li><strong>Motivation:</strong> generate more detailed and photorealistic images to improve the reconstructing stage of <strong>multi stage image synthesis</strong>.</li><li><strong>Compare:</strong><br><img src="/images/1-weeklynote/220612/5-2.png" alt=""></li></ul><h3 id="5-2-Related-Work"><a href="#5-2-Related-Work" class="headerlink" title="5.2. Related Work"></a>5.2. Related Work</h3><ul><li><strong>Visual Synthesis:</strong> AR、GANs、VAEs、diffusion models and flow-based models.<ul><li><strong>GANs:</strong> often difficult to train and defective in capturing of diversity.</li><li><strong>Auto-regressive (AR):</strong>  generate model have advantages in density modeling and stable training.</li></ul></li><li><strong>Diffusion models:</strong> likelihood-based models.<ul><li><strong>Denoising diffusion probabilistic models (DDPM):</strong> Produce high-quality images and the promising prospect in visual synthesis.</li><li><strong>Denoising diffusion implicit models (DDIM):</strong> developed a approach to fast sampling. </li><li><strong>Guided Diffusion:</strong> find that samples from a class conditional diffusion model with a independent classifier guidance can be significantly improved.</li><li><strong>Classifier-Free Diffusion:</strong> propose classifier-free guidance that does not need to train a separate classifier model.</li></ul></li></ul><h3 id="5-3-Methodology"><a href="#5-3-Methodology" class="headerlink" title="5.3. Methodology"></a>5.3. Methodology</h3><p><img src="/images/1-weeklynote/220612/5-3.png" alt=""></p><h3 id="5-4-Experiments"><a href="#5-4-Experiments" class="headerlink" title="5.4. Experiments"></a>5.4. Experiments</h3><ul><li><p><strong>Quantitative Analysis：</strong><br><img src="/images/1-weeklynote/220612/5-4.png" alt=""></p></li><li><p><strong>Qualitative Analysis：</strong><br><img src="/images/1-weeklynote/220612/5-5.png" alt=""></p></li></ul><h3 id="5-5-Conclusion-and-Future-Work"><a href="#5-5-Conclusion-and-Future-Work" class="headerlink" title="5.5. Conclusion and Future Work"></a>5.5. Conclusion and Future Work</h3><ul><li>Proposes a <strong>DiVAE</strong> with <code>a diffusion decoder</code> to generate more photorealistic and detailed images to improve the reconstructing stage of multi stage image synthesis.</li><li>Achieves <strong>state-of-the-art results</strong> on reconstruction of images comparing with existing approach and samples more detailed images on Text-to-Image tasks.</li></ul><h2 id="6-下一步计划"><a href="#6-下一步计划" class="headerlink" title="6. 下一步计划"></a>6. 下一步计划</h2><ul><li>扩散模型相关论文阅读</li><li>对比学习与视觉表征学习</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Contrastive Learning</tag>
      
      <tag>Diffusion Model</tag>
      
      <tag>Regularization</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Weekly-220605</title>
    <link href="/2022/06/05/1-Weekly-220605/"/>
    <url>/2022/06/05/1-Weekly-220605/</url>
    
    <content type="html"><![CDATA[<h3 id="本周学习汇报"><a href="#本周学习汇报" class="headerlink" title="本周学习汇报"></a>本周学习汇报</h3><span id="more"></span><blockquote><div>            <input type="checkbox" disabled checked="checked">自然辩证法考试          </div><div>            <input type="checkbox" disabled checked="checked">原型学习综述          </div><div>            <input type="checkbox" disabled checked="checked">Prototypical Networks for Few-shot Learning          </div><div>            <input type="checkbox" disabled checked="checked">Rethinking Semantic Segmentation: A Prototype View          </div></blockquote><h2 id="1-自然辩证法考试"><a href="#1-自然辩证法考试" class="headerlink" title="1. 自然辩证法考试"></a>1. 自然辩证法考试</h2><h3 id="1-1-部分资料整理"><a href="#1-1-部分资料整理" class="headerlink" title="1.1 部分资料整理"></a>1.1 部分资料整理</h3><p><img src="/images/1-weeklynote/220605/1-1.png" alt=""></p><h3 id="1-2-考试题目"><a href="#1-2-考试题目" class="headerlink" title="1.2 考试题目"></a>1.2 考试题目</h3><p><img src="/images/1-weeklynote/220605/1-2.png" alt=""></p><h2 id="2-关于原型学习"><a href="#2-关于原型学习" class="headerlink" title="2. 关于原型学习"></a>2. 关于原型学习</h2><p>张幸幸, 朱振峰, 赵亚威, 等. 机器学习中原型学习研究进展[J]. 软件学报, 2021: 0-0.</p><h3 id="2-1-原型学习目的"><a href="#2-1-原型学习目的" class="headerlink" title="2.1 原型学习目的"></a>2.1 原型学习目的</h3><ul><li>为<strong>消除数据冗余、发现数据结构、提高数据质量</strong>,原型学习是一种行之有效的方式.通过寻找一个原型集来表示目标集,以从样本空间进行数据约简,在增强数据可用性的同时,提升机器学习算法的执行效率.</li><li><strong>数据约简的两种方式</strong><ul><li>针对特征空间：特征降维、特征选择</li><li>针对样本空间：原型生成、原型选择</li></ul></li></ul><h3 id="2-2-原型学习的应用"><a href="#2-2-原型学习的应用" class="headerlink" title="2.2 原型学习的应用"></a>2.2 原型学习的应用</h3><p><img src="/images/1-weeklynote/220605/2-1.png" alt=""></p><ul><li><strong>对机器学习而言：</strong> 主动学习、自步学习、生成对抗网络、支持向量机、模型压缩</li><li><strong>对实际应用而言：</strong> 计算机视觉、模式识别、图像和自然语言处理、生物医学、传感网络、信息推荐等领域的众多应用</li><li><strong>对数据处理而言：</strong> 数据的快速存储、压缩、生成、清洗、可视化和标注</li></ul><h3 id="2-3-原型学习的相关工作"><a href="#2-3-原型学习的相关工作" class="headerlink" title="2.3 原型学习的相关工作"></a>2.3 原型学习的相关工作</h3><p><img src="/images/1-weeklynote/220605/2-2.png" alt=""></p><ul><li><strong>主要研究方向：</strong> 无监督原型学习。</li></ul><h3 id="2-4-原型学习监督方式"><a href="#2-4-原型学习监督方式" class="headerlink" title="2.4 原型学习监督方式"></a>2.4 原型学习监督方式</h3><h4 id="2-4-1-无监督原型学习"><a href="#2-4-1-无监督原型学习" class="headerlink" title="2.4.1 无监督原型学习"></a>2.4.1 无监督原型学习</h4><ul><li><strong>优化：</strong> 一个特定的准则，如设施选址、最大边缘相关度、稀疏编码等。<ul><li><strong>稀疏编码准则：</strong> 假定目标数据位于一个或多个子空间中,这样便于将原型选择问题转换为稀疏字典选择问题,并用字典重构误差衡量原型集在目标集中的重要性.</li><li><strong>设施选址准则：</strong> 一般则是基于给定的成对相似性或相异性,选择编码损失(服务成本)最小的数据点作为原型.其中原型集编码目标集的损失与原型的重要性成反比关系</li></ul></li><li><strong>目标：</strong> 选取目标数据集中最具代表性的一个子集。</li></ul><h4 id="2-4-2-半监督原型学习"><a href="#2-4-2-半监督原型学习" class="headerlink" title="2.4.2 半监督原型学习"></a>2.4.2 半监督原型学习</h4><ul><li><strong>目标：</strong> 了解每个原型的特定类别。</li><li><strong>方式：</strong> 引入一个源集,并将原型选择建模成设施选址问题,但是利用它从目标集中而不是源集中找出代表性样本。<br><img src="/images/1-weeklynote/220605/2-3.png" alt=""></li></ul><h4 id="2-4-3-全监督原型学习"><a href="#2-4-3-全监督原型学习" class="headerlink" title="2.4.3 全监督原型学习"></a>2.4.3 全监督原型学习</h4><ul><li><strong>CPL：</strong> 通过最大化类内聚集度和类间散度,设计一个基于距离度量的原型损失函数,从而学习目标集中每一类别的数据原型.进而,将测试数据与所有原型做匹配,可以对测试集做出判决,最终实现高精度和强鲁棒的模式分类。</li></ul><p><img src="/images/1-weeklynote/220605/2-4.png" alt=""></p><h3 id="2-5-原型学习相关方法"><a href="#2-5-原型学习相关方法" class="headerlink" title="2.5 原型学习相关方法"></a>2.5 原型学习相关方法</h3><h4 id="2-5-1-基于相似度的原型学习"><a href="#2-5-1-基于相似度的原型学习" class="headerlink" title="2.5.1 基于相似度的原型学习"></a>2.5.1 基于相似度的原型学习</h4><ul><li><strong>方法：</strong> 最小化目标集和原型集之间的全局差异。</li><li><strong>K-Medoids</strong><script type="math/tex; mode=display">\min _{\mu_{j} \in Y, r_{i j}} \sum_{i=1}^{n} \sum_{j=1}^{k} r_{i j} D\left(y_{i}, \mu_{j}\right)</script>其中D代表一种距离测量方式，如欧氏距离。</li></ul><h4 id="2-5-2-基于行列式点过程的原型选择"><a href="#2-5-2-基于行列式点过程的原型选择" class="headerlink" title="2.5.2 基于行列式点过程的原型选择"></a>2.5.2 基于行列式点过程的原型选择</h4><ul><li><strong>目标集上的点模式的概率测度：</strong><script type="math/tex; mode=display">\begin{array}{c}P(\Omega ; L)=\frac{\operatorname{det}\left(L_{\Omega}\right)}{\operatorname{det}(L+I)} \\\sum_{\Omega \subseteq Y} \operatorname{det}\left(L_{\Omega}\right)=\operatorname{det}(L+I)\end{array}</script></li><li>可以很好地解决抽样中的互斥性的问题，有利于形成原型集的多样性。</li></ul><h4 id="2-5-3-基于数据重构的原型学习"><a href="#2-5-3-基于数据重构的原型学习" class="headerlink" title="2.5.3 基于数据重构的原型学习"></a>2.5.3 基于数据重构的原型学习</h4><ul><li><strong>方法：</strong> 通过最小化原型集重构目标集的残差,来保证原型的可解释性.<br><img src="/images/1-weeklynote/220605/2-5.png" alt=""></li></ul><h4 id="2-5-4-基于低秩逼近的原型选择"><a href="#2-5-4-基于低秩逼近的原型选择" class="headerlink" title="2.5.4 基于低秩逼近的原型选择"></a>2.5.4 基于低秩逼近的原型选择</h4><ul><li>思想： 主要思想是通过矩阵分解,并利用随机或贪婪算法来寻找低秩矩阵列的子集,使得目标集矩阵的几行(列)能够近似整个低秩矩阵,而这几行(列)即目标集的原型集.</li><li><strong>CUR 分解的本质</strong><script type="math/tex; mode=display">\min _{C, U, R}\|Y-C U R\|_{F}^{2}+f(C, R)</script></li></ul><p>其中f (C,R)表示施加在矩阵 C 和 R 上的约束,且矩阵 C 由目标集 Y 中的几列组成,而矩阵 R 由目标集 Y 中的几行组成.</p><h3 id="2-6-原型学习的未来方向"><a href="#2-6-原型学习的未来方向" class="headerlink" title="2.6 原型学习的未来方向"></a>2.6 原型学习的未来方向</h3><ul><li><strong>知识迁移驱动的原型生成：</strong> 根据其他有标注的样本提升原型学习可利用的信息量。</li><li><strong>有缺陷数据的原型生成：</strong> 给定的目标集数据,通常在底层特征空间和高层语义空间容易出现缺陷。</li><li><strong>原型分布式学习：</strong> 通过最大化利用分步式计算的效能,不仅保护各个工作站的数据信息,同时还可以有效解决传统原型学习算法的准确率与效率无法同时满足的问题。</li><li><strong>面向深度学习的原型学习：</strong> 原型学习方法通过识别信息量最大的训练实例来提高大规模数据集机器学习的数据效率。</li><li><strong>原型的质量评价体系：</strong> 原型学习问题迫切需要一个统一的数据驱动的评价标准,而非任务驱动的评价标准,以精准度量原型的质量,尤其是可解释性、代表性和多样性。</li></ul><h2 id="3-元学习-初步了解"><a href="#3-元学习-初步了解" class="headerlink" title="3. 元学习(初步了解)"></a>3. 元学习(初步了解)</h2><h3 id="3-1-题目-元学习研究综述"><a href="#3-1-题目-元学习研究综述" class="headerlink" title="3.1 题目: 元学习研究综述"></a>3.1 题目: 元学习研究综述</h3><p><a href="http://cjc.ict.ac.cn/online/bfpub/lcf-20201214103607.pdf">论文链接</a></p><h3 id="3-2-内容"><a href="#3-2-内容" class="headerlink" title="3.2 内容"></a>3.2 内容</h3><ul><li><strong>提出目的：</strong> 针对传统神经网络模型泛化性能不足、对新种类任务适应性较差的特点.<strong>元学习的目的</strong>就是为了设计一种机器学习模型，这种模型有类似上面提到的人的学习特性，即使用少量样本数据，快速学习新的概念或技能.</li><li><strong>实例：</strong> 小样本学习.</li><li><strong>主要表现：</strong> 提高泛化性能、获取好的初始参数、通过少量计算和新训练数据即可在模型上实现和海量训练数据一样的识别准确度.</li><li><strong>元学习的三个要求</strong><ul><li>包含一个学习子系统；</li><li>利用以前学习中提取的元知识来获得经验，这些元知识来自单个数据集或不同领域；</li><li>动态选择学习偏差.<br><img src="/images/1-weeklynote/220605/3-1.png" alt=""></li></ul></li></ul><h2 id="4-小样本学习的原型网络"><a href="#4-小样本学习的原型网络" class="headerlink" title="4. 小样本学习的原型网络"></a>4. 小样本学习的原型网络</h2><h4 id="Prototypical-Networks-for-Few-shot-Learning"><a href="#Prototypical-Networks-for-Few-shot-Learning" class="headerlink" title="Prototypical Networks for Few-shot Learning"></a>Prototypical Networks for Few-shot Learning</h4><p><img src="/images/1-weeklynote/220605/4-1.png" alt=""><br><code>/2-Learn Method/2-Meta Learning/2-Prototypical Networks/2017-Prototypical Networks for Few-shot Learning(NIPS).pdf</code></p><h3 id="4-1-Introduction"><a href="#4-1-Introduction" class="headerlink" title="4.1 Introduction"></a>4.1 Introduction</h3><ul><li><strong>Problem:</strong> Few-shot classification,a classifier must generalize to new classes not seen in the training set.</li><li><strong>Method:</strong>  Learn a metric space,computing distances to prototype representations of each class.</li><li><strong>Advantages:</strong> they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results.</li><li><strong>Contributions</strong><ul><li>Formulate Prototypical Networks for both the <code>few-shot</code> and <code>zero-shot</code> settings.</li><li>Draw connections to <code>Matching Networks</code> in the one-shot setting, and analyze the underlying <code>distance function</code> used in the model.</li><li>Relate Prototypical Networks to <code>clustering</code> in order to justify the use of class means as prototypes when distances are computed with a <code>Bregman divergence</code>, such as <code>squared Euclidean distance</code>.</li></ul></li></ul><h3 id="4-2-Related-Work"><a href="#4-2-Related-Work" class="headerlink" title="4.2 Related Work"></a>4.2 Related Work</h3><ul><li><p><strong>Matching Networks:</strong> Uses an <strong>attention mechanism</strong> over a learned embedding of the labeled set of examples (the support set) to predict classes for the unlabeled points (the query set).Matching Networks can be interpreted as a <strong>weighted nearest-neighbor </strong>classifier applied within an embedding space.</p></li><li><p><strong>Meta-Learning:</strong> Training an LSTM to produce the updates to a classifier, given an episode, such that it will generalize well to<br>a test-set.</p></li></ul><h3 id="4-3-Methodology"><a href="#4-3-Methodology" class="headerlink" title="4.3 Methodology"></a>4.3 Methodology</h3><h4 id="4-3-1-Prototypical-Networks-in-the-few-shot-and-zero-shot-scenarios"><a href="#4-3-1-Prototypical-Networks-in-the-few-shot-and-zero-shot-scenarios" class="headerlink" title="4.3.1 Prototypical Networks in the few-shot and zero-shot scenarios."></a>4.3.1 Prototypical Networks in the few-shot and zero-shot scenarios.</h4><p><img src="/images/1-weeklynote/220605/4-2.png" alt=""></p><h4 id="4-3-2-Training-Algorithm"><a href="#4-3-2-Training-Algorithm" class="headerlink" title="4.3.2 Training Algorithm"></a>4.3.2 Training Algorithm</h4><p><img src="/images/1-weeklynote/220605/4-3.png" alt=""></p><h3 id="4-4-Experiments"><a href="#4-4-Experiments" class="headerlink" title="4.4 Experiments"></a>4.4 Experiments</h3><ul><li><strong>Quantitative Analysis：</strong><br><img src="/images/1-weeklynote/220605/4-4.png" alt=""><ul><li>Omniglot Few-shot Classification</li><li>miniImageNet Few-shot Classification</li><li>CUB Zero-shot Classification</li></ul></li></ul><h3 id="4-5-Conclusion-and-Future-Work"><a href="#4-5-Conclusion-and-Future-Work" class="headerlink" title="4.5 Conclusion and Future Work"></a>4.5 Conclusion and Future Work</h3><ul><li><strong>Prototypical Networks:</strong> Few-shot learning based on the idea that we can represent each class by the mean of its examples in a representation space learned by a neural network.</li><li>Achieve <strong>state-of-the-art</strong> results on the CUB-200 dataset.</li></ul><h2 id="5-原型视图在语义分割中的应用"><a href="#5-原型视图在语义分割中的应用" class="headerlink" title="5. 原型视图在语义分割中的应用"></a>5. 原型视图在语义分割中的应用</h2><h4 id="Rethinking-Semantic-Segmentation-A-Prototype-View"><a href="#Rethinking-Semantic-Segmentation-A-Prototype-View" class="headerlink" title="Rethinking Semantic Segmentation: A Prototype View"></a>Rethinking Semantic Segmentation: A Prototype View</h4><p><img src="/images/1-weeklynote/220605/5-1.png" alt=""><br><a href="https://github.com/tfzhou/ProtoSeg">GitHub</a><br><code>/6-Segmentation/1-Network/2022-Rethinking Semantic Segmentation- A Prototype View(CVPR oral).pdf</code></p><h3 id="5-1-Introduction"><a href="#5-1-Introduction" class="headerlink" title="5.1 Introduction"></a>5.1 Introduction</h3><ul><li><strong>Parametric Prototype Learning</strong><ul><li><strong>parametric softmax:</strong> pixel-wise features for dense prediction.</li><li><strong>query vectors:</strong> utilize a set of learnable vectors to query the dense embeddings for mask prediction.</li></ul></li><li><strong>Non-Parametric Prototype Learning</strong>: This paper.</li></ul><h4 id="5-1-1-Questions"><a href="#5-1-1-Questions" class="headerlink" title="5.1.1 Questions"></a>5.1.1 Questions</h4><h5 id="A-What-are-the-relation-and-difference-between-them-parametric-softmax-query-vectors"><a href="#A-What-are-the-relation-and-difference-between-them-parametric-softmax-query-vectors" class="headerlink" title="A. What are the relation and difference between them(parametric softmax/query vectors )?"></a>A. What are the relation and difference between them(parametric softmax/query vectors )?</h5><ul><li>parametric models based on learnable prototypes. Consider a segmentation task with C semantic classes. Most existing efforts seek to <strong>directly learn C class-wise prototypes</strong> – <strong>softmax weights or query vectors</strong> – for parametric, pixel-wise classification.</li></ul><h5 id="B-If-the-learnable-query-vectors-indeed-implicitly-capture-some-intrinsic-properties-of-data-is-there-any-better-way-to-achieve-this"><a href="#B-If-the-learnable-query-vectors-indeed-implicitly-capture-some-intrinsic-properties-of-data-is-there-any-better-way-to-achieve-this" class="headerlink" title="B. If the learnable query vectors indeed implicitly capture some intrinsic properties of data, is there any better way to achieve this?"></a>B. If the learnable query vectors indeed implicitly capture some intrinsic properties of data, is there any better way to achieve this?</h5><ul><li>more fundamental—&gt;&gt;C&amp;D.</li></ul><h5 id="C-What-are-the-limitations-of-this-learnable-prototype-based-parametric-paradigm"><a href="#C-What-are-the-limitations-of-this-learnable-prototype-based-parametric-paradigm" class="headerlink" title="C. What are the limitations of this learnable prototype based parametric paradigm?"></a>C. What are the limitations of this learnable prototype based parametric paradigm?</h5><ul><li><strong>representative ability:</strong> insufficient to describe <strong>rich intra-class variance</strong>. The prototypes are simply learned in a fully parametric manner, without considering their representative ability;</li><li><strong>parameters need:</strong> generalizability especially in the <strong>large-vocabulary</strong> case;</li><li><strong>intra-class compactness:</strong> only the relative relations between intra-class and inter-class distances are optimized; the actual distances between pixels and prototypes, i.e., <strong>intra-class compactness</strong>, are ignored.</li></ul><h5 id="D-How-to-address-these-limitations"><a href="#D-How-to-address-these-limitations" class="headerlink" title="D. How to address these limitations?"></a>D. How to address these limitations?</h5><ul><li><strong>Contribution:</strong> develop a nonparametric segmentation framework, based on non-learnable prototypes.</li><li><strong>Advantages:</strong><ul><li>each class is abstracted by <strong>a set of prototypes</strong>, well capturing class-wise characteristics and intra-class variance;</li><li>due to the <strong>nonparametric nature</strong>, the generalizability is improved;</li><li>via <strong>prototype-anchored metric learning</strong>, the pixel embedding space is shaped as well-structured, benefiting segmentation prediction eventually.<br><img src="/images/1-weeklynote/220605/5-2.png" alt=""></li></ul></li></ul><h3 id="5-2-Related-Work"><a href="#5-2-Related-Work" class="headerlink" title="5.2 Related Work"></a>5.2 Related Work</h3><ul><li><strong>Semantic Segmentation.</strong></li><li><strong>Prototype Learning:</strong>  Based on the nearest neighbors rule– <code>the earliest prototype learning method</code>.</li><li><strong>Metric Learning:</strong> The goal of metric learning is to learn a <code>distance metric/embedding</code> such that similar samples are pulled together and dissimilar samples are pushed away.</li></ul><h3 id="5-3-Methodology"><a href="#5-3-Methodology" class="headerlink" title="5.3 Methodology"></a>5.3 Methodology</h3><p><img src="/images/1-weeklynote/220605/5-3.png" alt=""></p><ul><li>Non-Learnable Prototype based Pixel Classification</li><li>Within-Class Online Clustering</li><li>Pixel-Prototype Contrastive Learning</li><li>Pixel-Prototype Distance Optimization</li><li>Network Learning and Prototype Update</li></ul><h3 id="5-4-Experiments"><a href="#5-4-Experiments" class="headerlink" title="5.4 Experiments"></a>5.4 Experiments</h3><ul><li><p><strong>Quantitative Analysis：</strong><br><img src="/images/1-weeklynote/220605/5-4.png" alt=""></p></li><li><p><strong>Qualitative Analysis：</strong><br><img src="/images/1-weeklynote/220605/5-5.png" alt=""></p></li></ul><h3 id="5-5-Conclusion-and-Future-Work"><a href="#5-5-Conclusion-and-Future-Work" class="headerlink" title="5.5 Conclusion and Future Work"></a>5.5 Conclusion and Future Work</h3><p><strong>Conclusion</strong></p><ul><li>explicit prototypical representation for class-level statistics modeling;</li><li>better generalization with nonparametric pixel-category prediction;</li><li>direct optimization of the feature embedding space.</li></ul><p><strong>Future Work</strong></p><ul><li>directly resemble pixel- or region- level observations.</li></ul><h2 id="6-下一步计划"><a href="#6-下一步计划" class="headerlink" title="6. 下一步计划"></a>6. 下一步计划</h2><ul><li>调研扩散模型的一些内容</li><li>了解元学习和迁移学习的主要思想</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Meta Learning</tag>
      
      <tag>Prototypical Networks</tag>
      
      <tag>Semantic segmentation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Weekly-220529</title>
    <link href="/2022/05/29/1-Weekly-220529/"/>
    <url>/2022/05/29/1-Weekly-220529/</url>
    
    <content type="html"><![CDATA[<h3 id="本周学习汇报"><a href="#本周学习汇报" class="headerlink" title="本周学习汇报"></a>本周学习汇报</h3><span id="more"></span><blockquote><div>            <input type="checkbox" disabled checked="checked">自然辩证法的小论文作业          </div><div>            <input type="checkbox" disabled checked="checked">A Survey on Curriculum Learning          </div><div>            <input type="checkbox" disabled checked="checked">GraphGAN: Graph Representation Learning with Generative Adversarial Nets(AAAI)          </div></blockquote><h2 id="1-自然辩证法作业"><a href="#1-自然辩证法作业" class="headerlink" title="1. 自然辩证法作业"></a>1. 自然辩证法作业</h2><h3 id="题目：计算机视觉领域的创新思维方式剖析-——从科学技术方法论的角度谈创新思维能力的提升"><a href="#题目：计算机视觉领域的创新思维方式剖析-——从科学技术方法论的角度谈创新思维能力的提升" class="headerlink" title="题目：计算机视觉领域的创新思维方式剖析 ——从科学技术方法论的角度谈创新思维能力的提升"></a>题目：计算机视觉领域的创新思维方式剖析 ——从科学技术方法论的角度谈创新思维能力的提升</h3><p><img src="/images/1-weeklynote/220529/1-1.png" alt=""></p><ul><li>注：主要想借此回顾一下之前看过的内容，顺便思考一下如何创新的问题（科学技术方法论的角度）。无奈由于个人水平以及字数限制，并且对科学技术方法论了解不透彻，所以先就这样吧。不过自然辩证法这门学科还挺不错的哈哈哈。</li></ul><h2 id="2-课程学习综述阅读"><a href="#2-课程学习综述阅读" class="headerlink" title="2. 课程学习综述阅读"></a>2. 课程学习综述阅读</h2><p><img src="/images/1-weeklynote/220529/2-1.png" alt=""></p><h3 id="2-1-简介"><a href="#2-1-简介" class="headerlink" title="2.1 简介"></a>2.1 简介</h3><ul><li>课程学习(curriculum Learning)是机器学习的一种训练策略，让机器学习由简单的数据到复杂的数据逐步进行学习（类似于学生学习的一些课程，由易到难），借用此方式提升模型的泛化性能和收敛速度。主要包含<code>Difficulty Measurer + Training Scheduler</code>两部分。<br><img src="/images/1-weeklynote/220529/2-2.png" alt=""></li></ul><h3 id="2-2-框架结构"><a href="#2-2-框架结构" class="headerlink" title="2.2 框架结构"></a>2.2 框架结构</h3><p><img src="/images/1-weeklynote/220529/2-3.png" alt=""><br><img src="/images/1-weeklynote/220529/2-4.png" alt=""><br><img src="/images/1-weeklynote/220529/2-5.png" alt=""></p><h3 id="2-3-应用与发展"><a href="#2-3-应用与发展" class="headerlink" title="2.3 应用与发展"></a>2.3 应用与发展</h3><ul><li>应用：主要有导向和去噪两种应用。<ul><li>导向是指加快训练收敛速度，主要场景是当目标任务复杂且有不同的分布时，比如多任务学习、GAN的训练、NAS、domain adaption、样本不平衡的分类等等。</li><li>去噪是指提高模型泛化能力和鲁棒性，主要场景是当任务由很多噪声时，样本质量不一，存在异构数据等情况，比如弱监督或者无监督学习。</li></ul></li><li>发展：<ul><li>更好的评估标准</li><li>更先进的理论：<code>“easier first</code>还是<code>harder first</code></li><li>更多的CL算法与更多样的应用</li></ul></li></ul><h2 id="3-图表征学习与生成对抗网络"><a href="#3-图表征学习与生成对抗网络" class="headerlink" title="3. 图表征学习与生成对抗网络"></a>3. 图表征学习与生成对抗网络</h2><p><img src="/images/1-weeklynote/220529/3-1.png" alt=""><br><a href="https://github.dev/hwwang55/GraphGAN">Github</a></p><h3 id="3-1-Introduction"><a href="#3-1-Introduction" class="headerlink" title="3.1. Introduction"></a>3.1. Introduction</h3><ul><li><strong>Graph representation learning：</strong> Embed each vertex in a graph into a low-dimensional vector space.</li><li><strong>Categories:</strong> generative models and discriminative models.</li><li><strong>proposed:</strong> Graph softmax.</li></ul><h3 id="3-2-Related-Work"><a href="#3-2-Related-Work" class="headerlink" title="3.2. Related Work"></a>3.2. Related Work</h3><ul><li>GAN</li><li>Graph Representation Learning</li></ul><h3 id="3-3-Methodology"><a href="#3-3-Methodology" class="headerlink" title="3.3. Methodology"></a>3.3. Methodology</h3><h4 id="3-3-1-GraphGAN-Framework"><a href="#3-3-1-GraphGAN-Framework" class="headerlink" title="3.3.1 GraphGAN Framework"></a>3.3.1 GraphGAN Framework</h4><p><img src="/images/1-weeklynote/220529/3-2.png" alt=""></p><ul><li><strong>G：</strong> Generates (or selects, if more precise) the most likely vertices to be connected with vc from vertex set V.</li><li><strong>D：</strong> Discriminate the connectivity for the vertex pair (v, vc)</li></ul><h4 id="3-3-2-Graph-Softmax-for-Generator"><a href="#3-3-2-Graph-Softmax-for-Generator" class="headerlink" title="3.3.2 Graph Softmax for Generator"></a>3.3.2 Graph Softmax for Generator</h4><p><img src="/images/1-weeklynote/220529/3-3.png" alt=""></p><h4 id="3-3-3-The-Code-Tree"><a href="#3-3-3-The-Code-Tree" class="headerlink" title="3.3.3 The Code Tree"></a>3.3.3 The Code Tree</h4><ul><li><a href="https://github.dev/hwwang55/GraphGAN">https://github.dev/hwwang55/GraphGAN</a></li><li><strong>data</strong>  <ul><li>CA-GrQc</li></ul></li><li><strong>pre_train</strong><ul><li>CA…pre_train . emb</li></ul></li><li><strong>src</strong><ul><li>evaluation<ul><li>link_prediction.py : used to evaluate the application of link prediction.</li></ul></li><li>GraphGAN<ul><li>graph_gan.py:<br><img src="/images/1-weeklynote/220529/3-4.png" alt=""></li><li>generator： the G model</li><li>discriminator： the D model</li><li>config： some settings </li></ul></li></ul></li></ul><h3 id="3-4-Experiments"><a href="#3-4-Experiments" class="headerlink" title="3.4. Experiments"></a>3.4. Experiments</h3><ul><li><strong>Quantitative Analysis：</strong><br><img src="/images/1-weeklynote/220529/3-5.png" alt=""></li></ul><h3 id="3-5-Conclusion-and-Future-Work"><a href="#3-5-Conclusion-and-Future-Work" class="headerlink" title="3.5. Conclusion and Future Work"></a>3.5. Conclusion and Future Work</h3><ul><li>Unifies two schools of graph representation learning methodologies</li><li>Solves the inherent limitations of the traditional softmax</li></ul><h2 id="4-下一步计划"><a href="#4-下一步计划" class="headerlink" title="4. 下一步计划"></a>4. 下一步计划</h2><ul><li>期末考试</li><li>了解原型学习的相关内容</li><li>重新阅读图像超分辨率的内容</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>GAN</tag>
      
      <tag>curriculum Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Weekly-220522</title>
    <link href="/2022/05/22/1-Weekly-220522/"/>
    <url>/2022/05/22/1-Weekly-220522/</url>
    
    <content type="html"><![CDATA[<h3 id="本周学习汇报"><a href="#本周学习汇报" class="headerlink" title="本周学习汇报"></a>本周学习汇报</h3><span id="more"></span><blockquote><div>            <input type="checkbox" disabled checked="checked">Rethinking Minimal Sufficient Representation in Contrastive Learning          </div><div>            <input type="checkbox" disabled checked="checked">Correlation Verification for Image Retrieval          </div><div>            <input type="checkbox" disabled checked="checked">MAXIM: Multi-Axis MLP for Image Processing          </div><div>            <input type="checkbox" disabled checked="checked">Semantically Accurate Super-Resolution Generative Adversarial Networks          </div></blockquote><h2 id="1-对比学习中的最小充分表示"><a href="#1-对比学习中的最小充分表示" class="headerlink" title="1. 对比学习中的最小充分表示"></a>1. 对比学习中的最小充分表示</h2><h3 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h3><ul><li>文章指出，忽略<strong>非共享（指不同view之间）的任务相关信息</strong>，可能导致下游任务的性能下降,理论表明，最小充分表示在对比学习中并不足够（对下游任务而言），所以其改进方式可以从<strong>互信息</strong>（一个变量对另一个变量的影响程度）入手，增加表征和输入之间的互信息，从而近似的引入任务相关的信息。<br><img src="/images/1-weeklynote/220522/1.png" alt=""></li></ul><h3 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h3><ul><li><strong>对比学习：</strong> 着重于学习同类实例之间的共同特征，区分非同类实例之间的不同之处，与生成式学习相比，更注重抽象的语义级别的特征空间而不是一些繁琐的细节，因此泛化能力更强。</li><li><strong>信息瓶颈理论：</strong> 模型提取第一阶段学习的任务相关信息保障充分性，第二阶段则负责压缩任务无关信息。</li></ul><h3 id="3-方法"><a href="#3-方法" class="headerlink" title="3. 方法"></a>3. 方法</h3><p><img src="/images/1-weeklynote/220522/2.png" alt=""></p><h3 id="4-实验效果"><a href="#4-实验效果" class="headerlink" title="4. 实验效果"></a>4. 实验效果</h3><ul><li><strong>定量分析：</strong><br><img src="/images/1-weeklynote/220522/3.png" alt=""></li></ul><h3 id="5-总结与展望"><a href="#5-总结与展望" class="headerlink" title="5. 总结与展望"></a>5. 总结与展望</h3><p><img src="/images/1-weeklynote/220522/4.png" alt=""></p><h2 id="2-图像检索"><a href="#2-图像检索" class="headerlink" title="2. 图像检索"></a>2. 图像检索</h2><h3 id="1-引言-1"><a href="#1-引言-1" class="headerlink" title="1. 引言"></a>1. 引言</h3><ul><li><strong>一个有趣的任务：</strong> 图像检索，根据一张图像<code>query</code> 判断与它相似的图像，难点在于<code>hard samples</code>。</li><li><strong>几何验证：</strong> 只依赖稀疏和与之特征的相关性。不可以进行多尺度的操作。在测试过程仍需要非常昂贵的推理。</li></ul><h3 id="2-相关工作-1"><a href="#2-相关工作-1" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h3><ul><li>Image retrieval</li><li>Diffusion / Query expansion</li><li>4D convolutional neural network.（用以捕获不同的clip之间的信息）</li><li>Hide-and-Seek.</li></ul><h3 id="3-方法-1"><a href="#3-方法-1" class="headerlink" title="3. 方法"></a>3. 方法</h3><h4 id="3-1-CVNet-Global"><a href="#3-1-CVNet-Global" class="headerlink" title="3.1 CVNet-Global"></a>3.1 CVNet-Global</h4><p><img src="/images/1-weeklynote/220522/5.png" alt=""></p><h4 id="3-2-CVNet-Rerank"><a href="#3-2-CVNet-Rerank" class="headerlink" title="3.2 CVNet-Rerank"></a>3.2 CVNet-Rerank</h4><p><img src="/images/1-weeklynote/220522/6.png" alt=""></p><h3 id="4-实验效果-1"><a href="#4-实验效果-1" class="headerlink" title="4. 实验效果"></a>4. 实验效果</h3><ul><li><strong>定量分析：</strong><br><img src="/images/1-weeklynote/220522/7.png" alt=""></li></ul><h3 id="5-总结与展望-1"><a href="#5-总结与展望-1" class="headerlink" title="5. 总结与展望"></a>5. 总结与展望</h3><ul><li>文章从单个推理中构建跨尺度相关关系，实现跨尺度匹配而不需要昂贵的多尺度推理。</li><li>限制：速度和存储问题，采用了核稀疏、通道缩减和量化的方式解决。</li></ul><h2 id="3-图像处理中的多维MLP"><a href="#3-图像处理中的多维MLP" class="headerlink" title="3. 图像处理中的多维MLP"></a>3. 图像处理中的多维MLP</h2><h3 id="1-引言-2"><a href="#1-引言-2" class="headerlink" title="1. 引言"></a>1. 引言</h3><ul><li>领域： 用于去雾、去雨、去噪、去模糊、润色等五种不同的工作；<br><img src="/images/1-weeklynote/220522/8.png" alt=""></li></ul><h3 id="2-相关工作-2"><a href="#2-相关工作-2" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h3><ul><li>图像修复模型</li><li>视觉Transformer</li><li>MLP视觉模型</li></ul><h3 id="3-方法-2"><a href="#3-方法-2" class="headerlink" title="3. 方法"></a>3. 方法</h3><p><img src="/images/1-weeklynote/220522/9.png" alt=""><br><img src="/images/1-weeklynote/220522/10.png" alt=""></p><h3 id="4-实验效果-2"><a href="#4-实验效果-2" class="headerlink" title="4. 实验效果"></a>4. 实验效果</h3><ul><li><p><strong>定量分析：</strong><br><img src="/images/1-weeklynote/220522/12.png" alt=""></p></li><li><p><strong>定性分析：</strong> 以去雾为例。<br><img src="/images/1-weeklynote/220522/11.png" alt=""></p></li></ul><h3 id="5-总结与展望-2"><a href="#5-总结与展望-2" class="headerlink" title="5. 总结与展望"></a>5. 总结与展望</h3><ul><li>Our work suggests an effective and efficient approach for applying gMLP to low-level vision tasks to gain global attention, a missing attribute of basic CNNs. Our gMLP initialization of the MAXIM family significantly advances state-of-the-arts in several image enhancement and restoration tasks with moderate complexity.</li></ul><h2 id="4-基于GAN的一种超分辨率算法"><a href="#4-基于GAN的一种超分辨率算法" class="headerlink" title="4. 基于GAN的一种超分辨率算法"></a>4. 基于GAN的一种超分辨率算法</h2><h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><ul><li><strong>Problem：</strong> Semantic segmentation and image super-resolution by jointly considering the performance of both in training a Generative Adversarial Network (GAN).</li><li><strong>proposed：</strong> a novel architecture and <code>domain-specific feature loss</code>.</li></ul><h3 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h3><ul><li><strong>Super-resolution:</strong> SRCNN、VDSR、SRGAN、ESRGAN.</li><li><strong>Super-Resolution in Remote Sensing</strong></li><li><strong>Semantic Segmentation in Remote Sensing</strong></li></ul><h3 id="3-Methodology"><a href="#3-Methodology" class="headerlink" title="3. Methodology"></a>3. Methodology</h3><p><img src="/images/1-weeklynote/220522/13.png" alt=""></p><h3 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h3><ul><li><p><strong>Quantitative Analysis：</strong><br><img src="/images/1-weeklynote/220522/14.png" alt=""></p></li><li><p><strong>Qualitative Analysis：</strong><br><img src="/images/1-weeklynote/220522/15.png" alt=""></p></li></ul><h3 id="5-Conclusion-and-Future-Work"><a href="#5-Conclusion-and-Future-Work" class="headerlink" title="5. Conclusion and Future Work"></a>5. Conclusion and Future Work</h3><ul><li>Demonstrated the benefits of <strong>super-resolution image enhancement</strong> for <code>improving semantic segmentation performance</code> in remote sensing applications.</li><li>Envision jointly learning <strong>additional semantically meaningful tasks</strong>, and <strong>incorporating multiple views</strong> into semantically aware super resolution.</li></ul><h2 id="5-下一步计划"><a href="#5-下一步计划" class="headerlink" title="5. 下一步计划"></a>5. 下一步计划</h2><ul><li>写课程的小论文作业</li><li>了解原型学习与课程式学习的相关内容</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Contrastive Learning</tag>
      
      <tag>Image Retrieval</tag>
      
      <tag>Super Resolution</tag>
      
      <tag>MLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch分布式数据并行运算和一个小工具</title>
    <link href="/2022/05/06/0-Pytorch%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%BF%90%E7%AE%97%E5%92%8C%E4%B8%80%E4%B8%AA%E5%B0%8F%E5%B7%A5%E5%85%B7/"/>
    <url>/2022/05/06/0-Pytorch%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E8%BF%90%E7%AE%97%E5%92%8C%E4%B8%80%E4%B8%AA%E5%B0%8F%E5%B7%A5%E5%85%B7/</url>
    
    <content type="html"><![CDATA[<h1 id="Byobu的介绍：终端复用方法（类似tmux）"><a href="#Byobu的介绍：终端复用方法（类似tmux）" class="headerlink" title="Byobu的介绍：终端复用方法（类似tmux）"></a>Byobu的介绍：终端复用方法（类似tmux）</h1><h2 id="1-ssh链接中断解决方式"><a href="#1-ssh链接中断解决方式" class="headerlink" title="1. ssh链接中断解决方式"></a>1. ssh链接中断解决方式</h2><p><strong>原因分析：</strong> 挂断信号（SIGHUP）默认的动作是终止程序。当终端接口检测到<strong>网络连接断开</strong>，将挂断信号发送给控制进程（会话期首进程）。如果会话期首进程终止，则该信号发送到该会话期前台进程组。一个进程退出导致一个孤儿进程组中产生时，如果任意一个<strong>孤儿进程组进程处于STOP状态</strong>，发送<code>SIGHUP</code>和<code>SIGCONT</code>信号到该进程组中所有进程。因此当网络断开或终端窗口关闭后，控制进程收到<code>SIGHUP</code>信号退出，会导致该会话期内其他进程退出。这里我认为我们的进程被杀掉也就是因为ssh与服务器之间的通信断掉了，这个通信断掉之后linux程序就默认将该连接下的所有进程都杀掉。</p><p><strong>解决方式：</strong> 一个是使用<code>nohup</code>指令，一个是使用<code>screen</code>指令，最后一个是<code>screen</code>的升级版<code>byobu</code>。(我觉得<code>byobu</code>比较好)</p><h2 id="2-Byobu是什么"><a href="#2-Byobu是什么" class="headerlink" title="2. Byobu是什么"></a>2. Byobu是什么</h2><p><strong>维基百科：</strong> <code>Byobu</code>是与Linux计算机操作系统一起使用的<code>GNU Screen</code> 终端多路复用器或<code>tmux</code>的增强功能，可用于提供屏幕通知或状态，以及选项卡式多窗口管理。它旨在改善用户连接到远程服务器时的终端会话。<br><strong>我的服务器上的效果：</strong></p><p><img src="/images/0-helloworld/220506/1.png" alt=""></p><p>注：忽略最下面一行，那一行是我本地的<code>tmux</code></p><h2 id="3-Byobu的使用"><a href="#3-Byobu的使用" class="headerlink" title="3. Byobu的使用"></a>3. Byobu的使用</h2><h3 id="3-1-加载与退出"><a href="#3-1-加载与退出" class="headerlink" title="3.1 加载与退出"></a>3.1 加载与退出</h3><p><strong>加载</strong></p><ul><li>登录启动<br><code>byobu-enable</code> 表示Byobu窗口管理器将在每次文本登录时自动启动<br><code>byobu-disable</code> 表示Byobu窗口管理器将不再在登录时自动启动</li><li>色彩提示<br><code>byobu-enable-prompt</code> 启动Byobu的彩色提示<br><code>byobu-disable-prompt</code> 禁用Byobu的彩色提示</li></ul><p><strong>退出</strong><br><code>byobu detach</code></p><h3 id="3-2-使用方法"><a href="#3-2-使用方法" class="headerlink" title="3.2 使用方法"></a>3.2 使用方法</h3><p><a href="https://blog.csdn.net/toigkiss/article/details/55509685">参考</a></p><p><strong>指令形式</strong></p><p>注：如果在<code>tmux</code>中再连接服务器使用<code>Byobu</code>，可能会有按键冲突，一是可以通过电脑的快捷键修改解决，而是别这么嵌套使用。我的<code>tmux</code>主要依赖<code>Ctrl-b</code>,<code>Byobu</code>依赖<code>Ctrl-a</code>，但是涉及<code>Fn</code>时就很迷。</p><p>Ctrl-a k - 关闭当前窗口（y/n）<br>Ctrl-a ↑ - 将焦点移动到上边分割区域 （如果上边存在分割区的话）<br>Ctrl-a ↓ - 将焦点移动到下边分割区域 （如果下边存在分割区的话）<br>Ctrl-a ← - 将焦点移动到左边分割区域 （如果左边存在分割区的话）<br>Ctrl-a → - 将焦点移动到右边分割区域 （如果右边存在分割区的话）<br>Ctrl-a 数字 - 移动到指定窗口<br>Ctrl-a $ - 显示详细状态 （不知道具体作用）<br>Ctrl-a R - 重新加载配置文件 （不知道具体作用）<br>Ctrl-a ! - 打开和关闭键绑定 （不知道具体作用）<br>Ctrl-a ~ - 保存当前窗口的回滚缓冲区 （不知道具体作用）<br>Ctrl+a | 垂直分割当前窗口<br>Ctrl+a % 水平分割当前窗口<br>Ctrl+a Ctrl+键盘方向键 设置分隔窗口大小<br>修改默认绑定的ctrl+a键,F9-&gt;change escape sequence-&gt;直接进行修改,比如改成ctrl+z. 举例,比如修改成ctrl+z后,分隔窗口之间的切换就是先ctrl+z然后在按方向键(不再是ctrl+a后再方向键了)</p><p>Byobu页低状态栏信息说明(版本不一样状态略有差别):<br>第一部分是ubuntu的标志logo,第二部分是ubuntu的版本,第三部分是byobu开启的窗口列表,当前列表会有一个”*”的标志,第四部分是开机时间和负载信息(uptime命令),第五部分是系统盘使用统计信息,最后面是日期时间.</p><p>p.s.:可自行修改底部状态栏:F9后tab键选择”Toggle status notifications(通知状态开关)”</p><p><strong>快捷键形式（也可以直接Fn + Shift + 1在Byobu中查看或者 man byobu）</strong></p><p>F2 - 创建一个新的窗口<br>F3 - 移动到下一个窗口<br>F4 - 移动到上一个窗口<br>F5 - 重新加载配置文件<br>F6 - 断开链接(可以通过 byobu -r 恢复)<br>F7 - 进入复制/回滚模式。这允许您将当前窗口中的文本及其历史记录复制到粘贴缓冲区中。在此模式下，一个类似于vi的全屏编辑器处于活动状态.<br>F8 - 重命名窗口<br>F9 - 菜单配置<br>F12 - 锁定当前命令行（不知道具体作用）<br>shift-F2 - 水平分割当前窗口<br>ctrl-F2 - 垂直分割当前窗口<br>shift-F3 - 将焦点移动到前一个分割区域<br>ctrl-F3 - 将当前分割区域与前一个分割区域替换<br>shift-F4 - 将焦点移动到下一个分割区域<br>ctrl-F4 - 将当前分割区域与下一个分割区域替换<br>shift-↑↓←→ 切换分割区<br>shift-F5 - 加入所有分割区域（没有尝试成功）<br>ctrl-F6 - 删除此拆分割区域<br>ctrl-F5 - 重新连接GPG和SSH套接字（不知道具体作用）<br>shift-F6 - 分离，但不会退出(可以通过 byobu -r 恢复)<br>ctrl-shift-F2 创建一个新的Session会话<br>alt-pgup - 进入回滚模式 往前寻找Session会话<br>alt-pgdn - 进入回滚模式 往后寻找Session会话</p><h1 id="Pytorch分布式数据并行运算"><a href="#Pytorch分布式数据并行运算" class="headerlink" title="Pytorch分布式数据并行运算"></a>Pytorch分布式数据并行运算</h1><p><a href="https://blog.csdn.net/qq_37541097/article/details/109736159">参考1</a><br><a href="https://www.cnblogs.com/yh-blog/p/12877922.html">参考2</a></p><h2 id="1-多GPU的使用"><a href="#1-多GPU的使用" class="headerlink" title="1. 多GPU的使用"></a>1. 多GPU的使用</h2><h3 id="1-1-多GPU的两种情况"><a href="#1-1-多GPU的两种情况" class="headerlink" title="1.1 多GPU的两种情况"></a>1.1 多GPU的两种情况</h3><p><img src="/images/0-helloworld/220506/4.png" alt=""></p><ul><li>左边模型并行化，右边数据并行化。</li></ul><h3 id="1-2-多GPU数据处理需要考虑的问题"><a href="#1-2-多GPU数据处理需要考虑的问题" class="headerlink" title="1.2 多GPU数据处理需要考虑的问题"></a>1.2 多GPU数据处理需要考虑的问题</h3><ul><li>数据如何分配至各设备当中。</li><li>误差梯度如何在不同的设备之间进行通信。</li><li>BatchNormalization如何在不同设备间同步。</li></ul><h2 id="2-python的os-environ模块"><a href="#2-python的os-environ模块" class="headerlink" title="2. python的os.environ模块"></a>2. python的os.environ模块</h2><p><a href="https://github.com/WZMIAOMIAO/deep-learning-for-image-processing/tree/master/pytorch_classification/train_multi_GPU">参考</a></p><h3 id="2-1-对Linux服务器而言"><a href="#2-1-对Linux服务器而言" class="headerlink" title="2.1 对Linux服务器而言"></a>2.1 对Linux服务器而言</h3><ul><li>os.environ[‘USER‘]:当前使用用户。</li><li>os.environ[‘LC_COLLATE’]:路径扩展的结果排序时的字母顺序。</li><li>os.environ[‘SHELL’]:使用shell的类型。</li><li>os.environ[‘LAN’]:使用的语言。</li><li>os.environ[‘SSH_AUTH_SOCK‘]:ssh的执行路径。</li></ul><h3 id="2-2-对分布式训练而言"><a href="#2-2-对分布式训练而言" class="headerlink" title="2.2 对分布式训练而言"></a>2.2 对分布式训练而言</h3><ul><li>RANK：通常假定rank 0是第一个进程或者主进程，其他每个进程都有对应的进程号。</li><li>LOCAL_RANK：进程内部的GPU的编号，非显式参数，由<code>torch.distributed.launch</code> 内部指定。一个进程可以对应多块GPU，编号从0开始。</li><li>WORLD_SIZE：全局的进程个数。</li><li>SLURM_PROCID：可以用作全局的RANK。</li></ul><h2 id="3-关于-nn-DataParallel-和-DistributedDataParallel-的区别"><a href="#3-关于-nn-DataParallel-和-DistributedDataParallel-的区别" class="headerlink" title="3. 关于 nn.DataParallel 和 DistributedDataParallel 的区别"></a>3. 关于 nn.DataParallel 和 DistributedDataParallel 的区别</h2><h3 id="3-1-问题描述"><a href="#3-1-问题描述" class="headerlink" title="3.1 问题描述"></a>3.1 问题描述</h3><ul><li><strong>问题：</strong> <code>UserWarning: nn.ParameterDict is being used with DataParallel but this is not supported. This dict will appear empty for the models replicated on each GPU except the original one.</code></li><li><strong>解释：</strong> 在现行版本的PyTorch中如果你的模型包含ParameterList（或者ParameterDict），那么使用DataParallel进行多卡训练会导致这个Parameter{List/Dict}没法成功复制到其他卡上。<a href="https://zhuanlan.zhihu.com/p/206467852">来源</a></li></ul><h3 id="3-2-DataParallel的并行化机制"><a href="#3-2-DataParallel的并行化机制" class="headerlink" title="3.2 DataParallel的并行化机制"></a>3.2 DataParallel的并行化机制</h3><p><img src="/images/0-helloworld/220506/2.png" alt=""></p><ul><li>GPU0作为master进行梯度的汇总和模型的更新，再将计算任务下发给其他GPU，所以它内存和使用率会比其他的高。</li></ul><h3 id="3-3-DistributedDataParallel的并行化机制"><a href="#3-3-DistributedDataParallel的并行化机制" class="headerlink" title="3.3 DistributedDataParallel的并行化机制"></a>3.3 DistributedDataParallel的并行化机制</h3><p><img src="/images/0-helloworld/220506/3.png" alt=""></p><ul><li>torch自动将其分配给n个进程，分别在n个GPU上运行。不再有主GPU，每个GPU执行相同的任务。</li></ul><h3 id="3-4-并行化方式的区别"><a href="#3-4-并行化方式的区别" class="headerlink" title="3.4 并行化方式的区别"></a>3.4 并行化方式的区别</h3><ul><li>DDP通过<strong>多进程</strong>实现的。也就是说操作系统会为每个GPU创建一个进程,从而避免了Python解释器GIL带来的性能开销。而<code>DataParallel()</code>是通过<strong>单进程控制多线程</strong>来实现的。还有一点,DDP也不存在前面DP提到的负载不均衡问题。</li><li>参数更新的方式不同。DDP在各进程梯度计算完成之后,各进程需要将梯度进行汇总平均,然后再由 <code>rank=0</code> 的进程,将其 <code>broadcast</code> 到所有进程后,各进程用该梯度来独立的更新参数而 DP是梯度汇总到<code>GPU0</code>,反向传播更新参数,再广播参数给其他剩余的GPU。由于DDP各进程中的模型,初始参数一致 (初始时刻进行一次 <code>broadcast</code>),而每次用于更新参数的梯度也一致,因此,各进程的模型参数始终保持一致。而在DP中,全程维护一个 <code>optimizer</code>,对各个GPU上梯度进行求平均,而在主卡进行参数更新,之后再将模型参数 <code>broadcast</code> 到其他GPU.相较于DP, <strong>DDP传输的数据量更少,因此速度更快,效率更高</strong>。</li><li>DDP支持 <code>all-reduce</code>(指汇总不同 GPU 计算所得的梯度,并同步计算结果),<code>broadcast</code>,<code>send</code> 和 <code>receive</code> 等等。通过 <code>MPI</code> 实现 <code>CPU</code>通信,通过 <code>NCCL</code> 实现 <code>GPU</code> 通信,缓解了进程间通信有大的开销问题。</li></ul><h3 id="3-5-结论"><a href="#3-5-结论" class="headerlink" title="3.5 结论"></a>3.5 结论</h3><ul><li>推荐使用<code>DistributedDataParallel</code>。</li><li>需要<code>python -m torch.distributed.launch --nproc_per_node=8 --use_env main.py</code>为每个节点启动多个进程以进行分布式训练，它在每个训练节点上产生多个分布式训练进程。</li></ul>]]></content>
    
    
    <categories>
      
      <category>Hello World</category>
      
    </categories>
    
    
    <tags>
      
      <tag>tools</tag>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Weekly-220410</title>
    <link href="/2022/04/10/1-Weekly-220410/"/>
    <url>/2022/04/10/1-Weekly-220410/</url>
    
    <content type="html"><![CDATA[<h3 id="本周学习汇报"><a href="#本周学习汇报" class="headerlink" title="本周学习汇报"></a>本周学习汇报</h3><span id="more"></span><blockquote><div>            <input type="checkbox" disabled checked="checked">CoordGAN: Self-Supervised Dense Correspondences Emerge from GANs<sup id=fnref:1 class=footnote-ref><a href=#fn:1 rel=footnote><span class=hint--top hint--rounded aria-label=https://arxiv.org/abs/2203.16521>[1]</span></a></sup>          </div><div>            <input type="checkbox" disabled checked="checked">L-Verse: Bidirectional Generation Between Image and Text<sup id=fnref:2 class=footnote-ref><a href=#fn:2 rel=footnote><span class=hint--top hint--rounded aria-label=https://arxiv.org/abs/2111.11133>[2]</span></a></sup>          </div><div>            <input type="checkbox" disabled checked="checked">Probabilistic Warp Consistency for Weakly-Supervised Semantic Correspondences<sup id=fnref:3 class=footnote-ref><a href=#fn:3 rel=footnote><span class=hint--top hint--rounded aria-label=https://deeplearn.org/arxiv/271386/probabilistic-warp-consistency-for-weakly-supervised-semantic-correspondences>[3]</span></a></sup>          </div><div>            <input type="checkbox" disabled checked="checked">Subspace Adversarial Training<sup id=fnref:4 class=footnote-ref><a href=#fn:4 rel=footnote><span class=hint--top hint--rounded aria-label=https://arxiv.org/abs/2111.12229>[4]</span></a></sup>          </div></blockquote><h2 id="1-CoordGAN"><a href="#1-CoordGAN" class="headerlink" title="1. CoordGAN"></a>1. CoordGAN</h2><p><img src="/images/1-weeklynote/220410/1.png" alt=""></p><h3 id="1-1-动机"><a href="#1-1-动机" class="headerlink" title="1.1 动机"></a>1.1 动机</h3><p>对一个Image而言，可分为Texture部分和Structure部分。而生成图像则期待学习一个连续平滑的变量空间，去表达不同图像之间像素层面的相关性，从而更好的对Texture和Structure部分进行解构（latent disentanglement），从而生成更多的有意义的图像。文章旨在学习一个显式的correspondence map，设计了一个新的坐标空间，以学习图像之间的密度关联。</p><h3 id="1-2-框架"><a href="#1-2-框架" class="headerlink" title="1.2 框架"></a>1.2 框架</h3><p><img src="/images/1-weeklynote/220410/2.png" alt=""></p><ul><li>分为四个部分：纹理映射网络、结构映射网络、坐标变换网络（对密度图的分析）、生成器模块</li></ul><h3 id="1-3-实验结果"><a href="#1-3-实验结果" class="headerlink" title="1.3 实验结果"></a>1.3 实验结果</h3><p><img src="/images/1-weeklynote/220410/3.png" alt=""></p><p><img src="/images/1-weeklynote/220410/4.png" alt=""></p><h3 id="1-4-总结展望"><a href="#1-4-总结展望" class="headerlink" title="1.4 总结展望"></a>1.4 总结展望</h3><ul><li>总结：设计了密度相关图，对GAN的latent space可以更好地进行解构。</li><li>展望：可以向3D方向进行扩展。</li></ul><h2 id="2-L-Verse"><a href="#2-L-Verse" class="headerlink" title="2. L-Verse"></a>2. L-Verse</h2><blockquote><p>题目：L-Verse（CVPR-oral）<br>L-Verse: Bidirectional Generation Between Image and Text</p></blockquote><p><img src="/images/1-weeklynote/220410/5.png" alt=""></p><ul><li>第一行：由图像生成文本效果</li><li>第二行：由文本生成图像效果</li></ul><h3 id="2-1-动机"><a href="#2-1-动机" class="headerlink" title="2.1 动机"></a>2.1 动机</h3><p>在高分辨率图像生成中，Transformer有着强大的性能。在文本与图像的转换方面，VQ-VAE则可以通过维护一个codebook来高效的提取图像中的信息，以转变为序列化的特征向量。本文提出了L-Verse，包括了一个<strong>特征增强变分自编码器（AugVAE）</strong>，一个<strong>双向的自回归Transformer</strong>，可以实现<code>Image-Text</code>以及<code>Text-Image</code>的转变，而不需要微调或者是额外的目标检测框架。</p><blockquote><p>贡献</p></blockquote><ul><li><strong>AugVAE：</strong> 表现了SOTA的图像重构性能。</li><li><strong>BiART：</strong> 有两个不同的embedding vector，可以分别以reference和target为条件进行生成，也就是可以根据图像生成文字，也可以根据文字生成图像。</li><li><strong>L-Verse:</strong>  包含以上两个模块，不需要任何额外的目标检测框架（比如Faster-RCNN，用于区域特征的提取）就可以进行由图像到文字的生成。</li></ul><h3 id="2-2-框架"><a href="#2-2-框架" class="headerlink" title="2.2 框架"></a>2.2 框架</h3><p><img src="/images/1-weeklynote/220410/6.png" alt=""></p><ul><li><strong>AugVAE：</strong> 图中蓝色部分，具有一个层级结构，可以有效提取图像中的特征到image token中。</li><li><strong>REF：</strong> 给定样式，即根据图像样式生成文本。</li><li><strong>GEN：</strong> 给定target，即根据文本生成图像。</li></ul><h3 id="2-3-实验"><a href="#2-3-实验" class="headerlink" title="2.3 实验"></a>2.3 实验</h3><ul><li><strong>定量分析：</strong> 重构FID指标达到了SOTA的效果（AugVAE）</li></ul><p><img src="/images/1-weeklynote/220410/7.png" alt=""></p><ul><li><strong>定性分析：</strong> I2T与T2I的效果</li></ul><p><img src="/images/1-weeklynote/220410/8.png" alt=""></p><h3 id="2-4-总结与展望"><a href="#2-4-总结与展望" class="headerlink" title="2.4 总结与展望"></a>2.4 总结与展望</h3><ul><li>本文的主要任务是做图像与文本之间的双向生成，提出了两个组件，分别是AugVAE与BiART，可以有效提取图像中的信息，并达到一个双向生成的效果。</li><li>本文受VQ-GAN的启发，以利用Transformer生成高分辨率图像，受CLIP的影响，更好地对图像进行采样。</li></ul><h2 id="3-Probabilistic-Warp-Consistency"><a href="#3-Probabilistic-Warp-Consistency" class="headerlink" title="3. Probabilistic Warp Consistency"></a>3. Probabilistic Warp Consistency</h2><h3 id="3-1-引言"><a href="#3-1-引言" class="headerlink" title="3.1 引言"></a>3.1 引言</h3><ul><li><strong>方法：</strong> 本文提出了概率扭曲一致性，一个为语义匹配任务设计的弱监督目标函数。本文的目标是<strong>更直接地监督由网络预测得到的密度匹配分数，编码成为一个条件概率分布。</strong> 方法是，首先构造一个图像三元组，应用一个已知扭曲在其中一张图像I上，同时与另一个图像J构成了一个图像三元组，I与J属于同一个类别中的不同实例，然后使用从生成的图像三元组产生的约束导出概率学习目标。通过一个可学习的特殊状态扩展概率输出空间，本文进一步考虑了遮挡问题以及背景噪声问题。为设计合理的监督，文章设计了一个图像对之间的目标函数，去描述不同的图像类别。</li><li><strong>实验方面：</strong>，本文将这一方法应用在了四个最近的语义匹配结构中，实验表明，本文的方法可达到一个SOTA的效果，同时，本文的方法与关键点注释相结合时，也会带来强监督效果的提升。</li><li><strong>研究领域：</strong> semantic matching，寻找同一个类别中不同实例的像素级别的相关性，可应用在<strong>语义分割</strong>与<strong>图像编辑</strong>的领域。</li><li><strong>解决问题：</strong> 强监督中需要大量人工标注的数据，弱监督（只有图像级别的标签，弱监督的一种）成本较低，但往往效果不佳。</li></ul><p><img src="/images/1-weeklynote/220410/9.png" alt=""></p><h3 id="3-2-相关工作"><a href="#3-2-相关工作" class="headerlink" title="3.2 相关工作"></a>3.2 相关工作</h3><ul><li><strong>语义匹配结构：</strong> 主要分为三步，1）feature extraction｜特征提取，2）cost volume construction｜存储两图像之间各像素的匹配程度，3） displacement estimation｜位移估计。</li><li><strong>无监督与弱监督语义匹配问题：</strong> use proxy losses on the cost volume constructed between real image pairs, with image labels as the only supervision.</li><li><strong>视频中的无监督学习：</strong> 提出自监督方法提取特征。</li></ul><h3 id="3-3-方法"><a href="#3-3-方法" class="headerlink" title="3.3 方法"></a>3.3 方法</h3><blockquote><p>概率公式</p></blockquote><script type="math/tex; mode=display">P_{I \leftarrow J}(i \mid j)=\frac{\exp \left(C_{I \leftarrow J}(i, j)\right)}{\sum_{k} \exp \left(C_{I \leftarrow J}(k, j)\right)}</script><blockquote><p>概率扭曲一致性约束</p></blockquote><p><img src="/images/1-weeklynote/220410/10.png" alt=""></p><blockquote><p>不匹配区域的建模</p></blockquote><script type="math/tex; mode=display">L_{\text {vis-PW-bi }}=\sum_{i \prime} \widehat{V}\left(i^{\prime}\right) \mathcal{H}\left(\widehat{P}_{I \leftarrow J \leftarrow I^{\prime}}\left(\cdot \mid i^{\prime}\right), P_{W}\left(\cdot \mid i^{\prime}\right)\right)</script><script type="math/tex; mode=display">L_{\mathrm{PNeg}}=\sum_{i} \mathcal{B}\left(\widehat{P}_{A \leftarrow I}(\emptyset \mid i), p_{\mathrm{neg}}\right)</script><blockquote><p>训练目标函数</p></blockquote><script type="math/tex; mode=display">L_{\text {weak }}=L_{\text {vis-PW-bi }}+\lambda_{\text {P-warp-sup }} L_{\text {P-warp-sup }}+\lambda_{\mathrm{PNeg}} L_{\mathrm{PNeg}}</script><script type="math/tex; mode=display">L_{\text {strong }}=L_{\text {vis-PW-bi }}+\lambda_{\text {P-warp-sup }} L_{\text {P-warp-sup }}+\lambda_{\mathrm{kp}} L_{\mathrm{kp}}</script><h3 id="3-4-实验效果"><a href="#3-4-实验效果" class="headerlink" title="3.4 实验效果"></a>3.4 实验效果</h3><ul><li><strong>定量分析：</strong> 消融实验</li></ul><p><img src="/images/1-weeklynote/220410/11.png" alt=""></p><ul><li><strong>定性分析：</strong> 直接预测一个Dirac-like分布</li></ul><p><img src="/images/1-weeklynote/220410/12.png" alt=""></p><h3 id="3-5-总结与展望"><a href="#3-5-总结与展望" class="headerlink" title="3.5 总结与展望"></a>3.5 总结与展望</h3><ul><li><strong>概率扭曲一致性</strong> ：适用于弱监督领域的目标函数。</li><li>根据基于真实图像对生成的三元组图像，引入多个概率损失。</li><li>在多个benchmark中达到SOTA效果。</li></ul><h2 id="4-Subspace-Adversarial-Training"><a href="#4-Subspace-Adversarial-Training" class="headerlink" title="4. Subspace Adversarial Training"></a>4. Subspace Adversarial Training</h2><h3 id="4-1-引言"><a href="#4-1-引言" class="headerlink" title="4.1 引言"></a>4.1 引言</h3><p>要解决<code>catastrophic overfitting</code> 问题。</p><p><img src="/images/1-weeklynote/220410/13.png" alt=""></p><ul><li><strong>对抗训练：</strong> Adversarial training (AT), which aims to minimize the model’s risk under the worst-case perturbations, is currently the most effective approach for improving the robustness of deep neural networks.</li><li><strong>单步对抗训练：</strong> Since the adversarial examples above are generated by onestep gradient propagation, the corresponding AT is called single-step AT.</li><li><strong>提出方法：</strong> Based on this discovery, we propose a new AT method called Subspace Adversarial Training (Sub-AT),<br>which identifies such an effective subspace and conducts AT in it.</li></ul><blockquote><p>贡献</p><ul><li>We approach the catastrophic overfitting in single-step AT from a novel view of optimization and firstly reveal the close link between <strong>the fast-growing gradient of each sample and overfitting</strong>, which can also be applied to explain the robust overfitting in multi-step AT.</li><li>We propose an efficient AT method, Sub-AT, <strong>which constrains AT in a carefully extracted subspace, to control the growth of gradient.</strong> It uniformly resolves both kinds of overfitting, significantly improves the robustness, and successfully overcomes the sensitivity to learning rates. It is also very easy to combine with other AT methods to bring consistent improvements.</li><li>Our Sub-AT achieves <strong>state-of-the-art adversarial robustness on single-step AT</strong> and can successfully train with larger steps and larger radius, which brings further improvements. Notably, our pure single-step AT achieves over 51% robust accuracy against PGD-50 attack of ϵ = 8/255 on CIFAR-10, competitive to the multi-step PGD-10 AT with great time benefits.</li></ul></blockquote><h3 id="4-2-方法"><a href="#4-2-方法" class="headerlink" title="4.2 方法"></a>4.2 方法</h3><p><img src="/images/1-weeklynote/220410/14.png" alt=""></p><h3 id="4-3-实验效果"><a href="#4-3-实验效果" class="headerlink" title="4.3 实验效果"></a>4.3 实验效果</h3><ul><li><strong>定量分析：</strong></li></ul><p><img src="/images/1-weeklynote/220410/15.png" alt=""></p><h3 id="4-4-总结与展望"><a href="#4-4-总结与展望" class="headerlink" title="4.4 总结与展望"></a>4.4 总结与展望</h3><ul><li>Reveal the close link between the fast-growing gradient of each sample and overfitting, which can also explain the robust overfitting in multi-step AT.</li><li>Constrain AT in a carefully extracted subspace.</li></ul><h2 id="5-下一步计划"><a href="#5-下一步计划" class="headerlink" title="5. 下一步计划"></a>5. 下一步计划</h2><ul><li>阅读特征匹配的相关论文及代码。</li><li>了解密度图相关的知识。</li></ul><h3 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h3><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://arxiv.org/abs/2203.16521">https://arxiv.org/abs/2203.16521</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://arxiv.org/abs/2111.11133">https://arxiv.org/abs/2111.11133</a><a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span><a href="https://deeplearn.org/arxiv/271386/probabilistic-warp-consistency-for-weakly-supervised-semantic-correspondences">https://deeplearn.org/arxiv/271386/probabilistic-warp-consistency-for-weakly-supervised-semantic-correspondences</a><a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:4" class="footnote-text"><span><a href="https://arxiv.org/abs/2111.12229">https://arxiv.org/abs/2111.12229</a><a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    
    <tags>
      
      <tag>GAN</tag>
      
      <tag>T2I</tag>
      
      <tag>Semantic matching</tag>
      
      <tag>I2T</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Weekly-220403</title>
    <link href="/2022/04/03/1-Weekly-220403/"/>
    <url>/2022/04/03/1-Weekly-220403/</url>
    
    <content type="html"><![CDATA[<h3 id="本周学习汇报"><a href="#本周学习汇报" class="headerlink" title="本周学习汇报"></a>本周学习汇报</h3><span id="more"></span><blockquote><div>            <input type="checkbox" disabled checked="checked">InstaFormer: Instance-Aware Image-to-Image Translation with Transformer<sup id=fnref:1 class=footnote-ref><a href=#fn:1 rel=footnote><span class=hint--top hint--rounded aria-label=https://arxiv.org/abs/2203.16248>[1]</span></a></sup>          </div><div>            <input type="checkbox" disabled checked="checked">CLIP-GEN: Language-Free Training of a Text-to-Image Generator with CLIP<sup id=fnref:2 class=footnote-ref><a href=#fn:2 rel=footnote><span class=hint--top hint--rounded aria-label=https://arxiv.org/abs/2203.00386>[2]</span></a></sup>          </div></blockquote><h2 id="1-InstraFormer"><a href="#1-InstraFormer" class="headerlink" title="1. InstraFormer"></a>1. InstraFormer</h2><p><img src="/images/1-weeklynote/220403/1.png" alt=""></p><blockquote><p><strong>动机：</strong> </p></blockquote><p>在I2IT领域，很多任务仅仅是在做图像层面的转化，而忽略了具体的一些实例，因此就出现了对instance-aware I2I的研究，也就是对图片中的某一个具体的物体进行风格转换，此时就需要考虑图像层面和实例层面的一致性，即高效地整合全局与局部的信息。论文提出了一个新的单向网络，引入ViT，以达到高分辨率的图像与图像之间风格的转换。</p><blockquote><p><strong>应用：</strong> </p></blockquote><p>风格迁移、超分辨率、图像修复、图像着色等等。</p><blockquote><p><strong>框架：</strong> </p></blockquote><p>单向的端到端的网络，因此可以不使用循环一致损失。</p><blockquote><p><strong>训练方法：</strong> </p></blockquote><p>包括了对抗损失(判断最终生成的图像是否足够像目标域中的图像)、Global Content Loss（判断x到y的内容损失）、Instance-level Content Loss（对实例中的内容损失进行考量）Image Reconstruction Loss.（期待生成器可以很好的重构出图像）、Style Reconstruction Loss（更好地对解构的信息进行表达）。</p><blockquote><p><strong>效果对比</strong></p></blockquote><p><img src="/images/1-weeklynote/220403/2.png" alt=""></p><h2 id="2-CLIP-GEN"><a href="#2-CLIP-GEN" class="headerlink" title="2. CLIP-GEN"></a>2. CLIP-GEN</h2><p><img src="/images/1-weeklynote/220403/3.png" alt=""></p><blockquote><p>动机：</p></blockquote><p>训练实现文本图像生成过程中需要大量的<strong>文本-图像配对数据</strong>，但往往这类数据的获取十分昂贵。本文考虑使用<strong>CLIP的Encoder的部分</strong>，提取一个图像的embedding，使其在一个联合图像和文本的embedding space中，第二部则采用<strong>VQGAN</strong>中的codebook将图像转换成一连串的离散的token序列，最后，训练一个<strong>自回归的Transformer</strong>，来匹配图像的token序列。</p><blockquote><p>框架：</p></blockquote><ul><li>包含：CLIP、VQGAN、Condition Transformer等。</li><li>训练方法：分为两个阶段，其中第一个阶段是对VQGAN的codebook的学习，第二个阶段是对Transformer和CLIP的学习。 </li></ul><blockquote><p>应用</p></blockquote><ul><li>Text-to-Image</li></ul><blockquote><p>效果对比</p></blockquote><p><img src="/images/1-weeklynote/220403/4.png" alt=""></p><h2 id="3-下一步计划"><a href="#3-下一步计划" class="headerlink" title="3. 下一步计划"></a>3. 下一步计划</h2><ol><li>查阅GiTHub上的相关代码。</li><li>继续阅读相关论文。</li></ol><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://arxiv.org/abs/2203.16248">https://arxiv.org/abs/2203.16248</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://arxiv.org/abs/2203.00386">https://arxiv.org/abs/2203.00386</a><a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    
    <tags>
      
      <tag>I2I</tag>
      
      <tag>T2I</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>从VAE到MaskGIT(二)</title>
    <link href="/2022/03/19/2-%E4%BB%8EVAE%E5%88%B0MaskGIT-2/"/>
    <url>/2022/03/19/2-%E4%BB%8EVAE%E5%88%B0MaskGIT-2/</url>
    
    <content type="html"><![CDATA[<blockquote><p><strong>图像生成的一般要求：</strong></p><ul><li>生成的图像与真实图像相似，且清晰度比较高（具有真实性）</li><li>生成的图像不在原始数据集中，且尽可能地分散（具有多样性）</li></ul></blockquote><h2 id="2-VAE-based生成式模型的改进"><a href="#2-VAE-based生成式模型的改进" class="headerlink" title="2. VAE-based生成式模型的改进"></a>2. VAE-based生成式模型的改进</h2><p>自编码器最早出现在1986年，而今年的一月份，TOELT LLC的AI科学家对自编码器进行了综述性的介绍<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://arxiv.org/pdf/2201.03898.pdf">[1]</span></a></sup>。自编码器包含了<code>encoder</code>和<code>decoder</code>以及一个<code>latent code</code>。</p><p class="note note-warning">学习尽可能低的误差重建观测值，以更好地学习数据的特征表示</p><ul><li><code>Encoder</code>: 用于对原始的观测图像<code>x</code>进行压缩，以得到原始图像中基本的一些信息，称为<strong>latent code</strong>。</li><li><code>Decoder</code>: 对<strong>latent code</strong>进行重构，以得到重构后的图像<code>x&#39;</code>，</li><li>重构误差：</li></ul><script type="math/tex; mode=display">L_{\mathrm{AE}}(E, D)=\frac{1}{n} \sum_{i=1}^{n}\left(\mathbf{x}^{(i)}-D\left(E\left(\mathbf{x}^{(i)}\right)\right)\right)^{2}</script><p>然而，在自编码器的学习过程中，建立的是一对一的映射，因此<strong>latent code</strong>在隐空间里是不连续的，它的生成过程是不可控的，也就是说它对输入的噪声会十分敏感。一个常见的做法就是更改<strong>latent code</strong>的分布形式，使其尽可能地连续化。</p><h3 id="2-1-Variational-Autoencoder-VAE"><a href="#2-1-Variational-Autoencoder-VAE" class="headerlink" title="2.1 Variational Autoencoder(VAE)"></a>2.1 Variational Autoencoder(VAE)</h3><p class="note note-warning">既然我们无法直接获得样本x的分布，那么我们就可以假设存在一个x对应的隐式表征z，z满足某种先验分布（人为指定），z经过解码以后可以映射到x的近似真实分布。这样做的好处是，我们可以在标准正态分布上采样得到样本近似分布，再在样本近似分布上采样来生成样本</p><h4 id="2-1-1-变分自编码器的结构"><a href="#2-1-1-变分自编码器的结构" class="headerlink" title="2.1.1 变分自编码器的结构"></a>2.1.1 变分自编码器的结构</h4><p><img src="/images/2-cv/220329/1.png" alt="图2.1 变分自编码器结构"></p><p>如图，变分自编码器<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://arxiv.org/abs/1312.6114">[2]</span></a></sup>的结构与自编码器相似，不同点在于对<strong>latent code</strong>的处理。</p><ul><li><strong>latent code</strong>：属性的分布既可以描述成离散的形式，也可以描述成连续的概率分布的形式，而使用连续的概率分布形式来进行属性表示可以更好地对分布进行描述，从而在生成图像过程中更好地进行采样。</li><li><code>Encoder</code>: 学习 <script type="math/tex">\mu</script> 和 <script type="math/tex">\sigma</script> 两个编码，同时随机采样一个正态分布的编码 <script type="math/tex">\epsilon</script> 来对<strong>latent code</strong>进行表示。</li><li><code>Decoder</code>: 根据<strong>latent code</strong>得到生成图像<code>x&#39;</code>。</li></ul><p><strong>训练过程</strong></p><p class="note note-warning"> 通过设计latent code的分布形式，进而控制图片生成的过程。 </p><p><img src="/images/2-cv/220329/2.png" alt="图2.2 变分自编码器训练过程"></p><p>可以看出，编码器和解码器分别对应给出了z和x的条件概率分布。当训练好变分自编码器之后，生成数据只需要解码器网络，即，先从设定好的z的先验分布中采样，接着对数据x进行采样。</p><h4 id="2-1-2-变分自编码器的后验坍塌问题（posterior-collapse）"><a href="#2-1-2-变分自编码器的后验坍塌问题（posterior-collapse）" class="headerlink" title="2.1.2 变分自编码器的后验坍塌问题（posterior collapse）"></a>2.1.2 变分自编码器的后验坍塌问题（posterior collapse）</h4><p><img src="/images/2-cv/220329/3.png" alt="图2.3 后验坍塌问题"></p><ul><li><strong>噪声太大：</strong>均值和方差的学习不稳定，<code>Decoder</code>开始忽略z，直接自行重构。</li><li><strong>信号太弱：</strong>均值和方差的学习与输入x几乎没什么关系，从而崩溃于常数值a与b，此时的z没有什么价值。</li></ul><p>注：NeurlPS 2019 的一篇文章<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://arxiv.org/abs/1911.02469">[3]</span></a></sup>对此现象进行了分析，表明一味的假设单一隐变量z，企图用z求边际分布来拟合真实分布，这可能是导致后验失效的真正原因。因此需要<strong>增加z分布的复杂度，而不是用单一的高斯分布</strong>。</p><h3 id="2-2-Vector-Quantised-Variational-Autoencoder-VQ-VAE"><a href="#2-2-Vector-Quantised-Variational-Autoencoder-VQ-VAE" class="headerlink" title="2.2 Vector Quantised Variational Autoencoder(VQ-VAE)"></a>2.2 Vector Quantised Variational Autoencoder(VQ-VAE)</h3><blockquote><p><strong>摘要<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://arxiv.org/abs/1711.00937">[4]</span></a></sup></strong><br>Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), <strong>differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static.</strong> In order to learn a discrete latent representation, we incorporate ideas from <strong>vector quantisation (VQ)</strong>. Using the VQ method allows the model to circumvent issues of “posterior collapse” -— where the latents are ignored when they are paired with a powerful autoregressive decoder -— typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.</p></blockquote><h4 id="2-2-1-离散化表达——向量量化"><a href="#2-2-1-离散化表达——向量量化" class="headerlink" title="2.2.1 离散化表达——向量量化"></a>2.2.1 离散化表达——向量量化</h4><p><img src="/images/2-cv/220329/4.png" alt="图2.4 阶段1-向量量化"></p><ul><li><strong>向量量化：</strong>找到既定点，当作某一区间的代表。既定点的依据是失真最小的少数代表向量组，也就是字典向量（codebook）。</li><li><strong>方法：</strong>学习一个codebook，实现 <script type="math/tex">z_{e}(x)</script> 到 <script type="math/tex">z_{q}(x)</script> 的映射。</li><li><strong>损失函数：</strong>第一部分是重构误差，第二部分和第三部分通过stop gradient的方式，分别对codebook和<code>Encoder</code>进行学习。</li></ul><h4 id="2-2-2-自回归先验的引入"><a href="#2-2-2-自回归先验的引入" class="headerlink" title="2.2.2 自回归先验的引入"></a>2.2.2 自回归先验的引入</h4><p><img src="/images/2-cv/220329/5.png" alt="图2.5 阶段2-自回归先验的引入"></p><ul><li><strong>目的：</strong>得知整张图像在所有图像中出现的概率，就可以通过此概率进行采样，以得到生成的图像。</li><li><strong>方式：</strong>通过类推连乘的方式，得到最终某一张图像出现的概率。根据条件概率依次抽取剩余像素数值，这个过程称为自回归。</li><li><strong>损失函数：</strong>负对数似然。</li></ul><h4 id="2-2-3-层级结构——VQ-VAE2"><a href="#2-2-3-层级结构——VQ-VAE2" class="headerlink" title="2.2.3 层级结构——VQ-VAE2"></a>2.2.3 层级结构——VQ-VAE2</h4><p><img src="/images/2-cv/220329/6.png" alt="图2.6 训练过程"></p><p><img src="/images/2-cv/220329/7.png" alt="图2.7 生成过程"></p><h3 id="2-3-Vector-Quantised-Generative-Adversarial-Network-VQ-GAN"><a href="#2-3-Vector-Quantised-Generative-Adversarial-Network-VQ-GAN" class="headerlink" title="2.3 Vector Quantised Generative Adversarial Network(VQ-GAN)"></a>2.3 Vector Quantised Generative Adversarial Network(VQ-GAN)</h3><blockquote><p><strong>摘要<sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://arxiv.org/abs/2012.09841">[5]</span></a></sup></strong><br>Designed to learn <strong>long-range interactions on sequential data</strong>, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they containno <strong>inductive bias</strong> that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as <strong>high-resolution images</strong>. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to <strong>(i) use CNNs to learn a contextrich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images.</strong> Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semanticallyguided synthesis of megapixel images with transformers. Project page at <a href="https://git.io/JLlvY">https://git.io/JLlvY</a>.</p></blockquote><h4 id="2-3-1-阶段1-损失函数的改进与GAN的引入"><a href="#2-3-1-阶段1-损失函数的改进与GAN的引入" class="headerlink" title="2.3.1 阶段1:损失函数的改进与GAN的引入"></a>2.3.1 阶段1:损失函数的改进与GAN的引入</h4><p><img src="/images/2-cv/220329/8.png" alt="图2.8"></p><h4 id="2-3-2-阶段2-利用Transformer学习自回归先验"><a href="#2-3-2-阶段2-利用Transformer学习自回归先验" class="headerlink" title="2.3.2 阶段2:利用Transformer学习自回归先验"></a>2.3.2 阶段2:利用Transformer学习自回归先验</h4><p><img src="/images/2-cv/220329/9.png" alt="图2.9"></p><h4 id="2-3-3-条件图像生成"><a href="#2-3-3-条件图像生成" class="headerlink" title="2.3.3 条件图像生成"></a>2.3.3 条件图像生成</h4><p>VQ-GAN也对不同条件的图像生成进行了探索，包括<strong>有空间信息的（语义图、深度图等等）条件图像生成</strong>，以及<strong>无空间信息的条件图像生成</strong>。条件可以在自回归先验的学习过程中引入进去，而对有空间信息的条件则需要进一步的对图像进行量化，用一个具体的向量进行条件表示。</p><p><img src="/images/2-cv/220329/10.png" alt="图2.10"></p><h3 id="2-4-Masked-Generative-Image-Transformer-MaskGIT"><a href="#2-4-Masked-Generative-Image-Transformer-MaskGIT" class="headerlink" title="2.4 Masked Generative Image Transformer(MaskGIT)"></a>2.4 Masked Generative Image Transformer(MaskGIT)</h3><blockquote><p><strong>摘要<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://arxiv.org/abs/2202.04200">[6]</span></a></sup></strong><br>Generative transformers have experienced rapid popularity growth in the computer vision community in synthesizing high-fidelity and high-resolution images. <strong>The best generative transformer models so far, however, still treat an image naively as a sequence of tokens, and decode an image sequentially following the raster scan ordering</strong> (i.e. lineby-line). We find this strategy neither optimal nor efficient. This paper proposes a novel image synthesis paradigm using a bidirectional transformer decoder, which we term MaskGIT. During training, <strong>MaskGIT learns to predict randomly masked tokens by attending to tokens in all directions. At inference time, the model begins with generating all tokens of an image simultaneously, and then refines the image iteratively conditioned on the previous generation.</strong> Our experiments demonstrate that MaskGIT significantly outperforms the state-of-the-art transformer model on the ImageNet dataset, and accelerates autoregressive decoding by up to 64x. Besides, we illustrate that MaskGIT can be easily extended to various image editing tasks, such as inpainting, extrapolation, and image manipulation.</p></blockquote><h4 id="2-4-1-改进原因"><a href="#2-4-1-改进原因" class="headerlink" title="2.4.1 改进原因"></a>2.4.1 改进原因</h4><ul><li><strong>直觉的角度：</strong>如画家绘画，先是框架，再逐步填充细节完善整张图像。</li><li><strong>计算的角度：</strong>图像作为平面序列意味着自回归序列长度呈二次方增长。</li></ul><h4 id="2-4-2-MVTM的设计与训练过程"><a href="#2-4-2-MVTM的设计与训练过程" class="headerlink" title="2.4.2 MVTM的设计与训练过程"></a>2.4.2 MVTM的设计与训练过程</h4><p><img src="/images/2-cv/220329/11.png" alt="图2.11"></p><h4 id="2-4-3-MVTM的图像生成步骤"><a href="#2-4-3-MVTM的图像生成步骤" class="headerlink" title="2.4.3 MVTM的图像生成步骤"></a>2.4.3 MVTM的图像生成步骤</h4><p><img src="/images/2-cv/220329/12.png" alt="图2.12"></p><h4 id="2-4-4-掩码策略"><a href="#2-4-4-掩码策略" class="headerlink" title="2.4.4 掩码策略"></a>2.4.4 掩码策略</h4><p><img src="/images/2-cv/220329/13.png" alt="图2.13"></p><p>注：MaskGIT第一阶段延续了VQ-GAN的设计，第二阶段采用了Bert的双向Transformer结构，可以更快更高效地学习到图像中的上下文信息。</p><h2 id="3-总结与展望"><a href="#3-总结与展望" class="headerlink" title="3. 总结与展望"></a>3. 总结与展望</h2><h3 id="3-1-总结"><a href="#3-1-总结" class="headerlink" title="3.1 总结"></a>3.1 总结</h3><p class="note note-warning"> 本质上都是解决密度估计问题，实现真实数据集与生成数据集的同分布。 </p><ul><li><p><strong>图像生成的需求：</strong></p><ul><li><strong>多样性：</strong>离散化设计latent space，学习codebook存储尽可能多的信息。</li><li><strong>高清：</strong>利用Transformer更好地获取全局信息。</li><li><strong>计算效率：</strong>并行解码效率优于顺序解码。</li></ul></li><li><p><strong>系列模型的改进：</strong></p><ul><li><strong>VAE：</strong>强制使latent code满足正态分布，以便生成新的图像。</li><li><strong>VQVAE：</strong>解决后验坍塌问题，使latent code有更好的表现力。</li><li><strong>VQVAE-2：</strong>采用多层结构，更好地获取图像结构信息与纹理信息。</li><li><strong>VQGAN：</strong>使用Transformer作为自回归模型，改进重构误差。</li><li><strong>MaskGIT：</strong>采用双向Transformer结构，并行解码，双向生成，提高计算效率。</li></ul></li></ul><h3 id="3-2-展望"><a href="#3-2-展望" class="headerlink" title="3.2 展望"></a>3.2 展望</h3><ul><li><strong>latent code的设计：</strong>是否可以对latent code进行更好的连续化设计，以便于存储连续<br>的属性信息，达到更好的图像编辑的效果？</li><li><strong>网络结构：</strong>对Encoder而言，是否能用Transformer代替CNN，更好地利用全局信息？ </li><li><strong>损失函数：</strong>有无更好的重构误差，以判断生成图像与真实图像的相似性。</li><li><strong>Mask的设计：</strong>从绘画的角度，画者可以对图像进行修改，但MaskGIT的掩码设计是固定的不可对前一轮的mask进行修改，如果使mask可修改，应采取什么策略，是否能达到更好的效果？</li></ul><h3 id="3-3-其他经典生成式模型"><a href="#3-3-其他经典生成式模型" class="headerlink" title="3.3 其他经典生成式模型"></a>3.3 其他经典生成式模型</h3><p><img src="/images/2-cv/220329/14.png" alt="图2.14"></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://arxiv.org/pdf/2201.03898.pdf">https://arxiv.org/pdf/2201.03898.pdf</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://arxiv.org/abs/1312.6114">https://arxiv.org/abs/1312.6114</a><a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span><a href="https://arxiv.org/abs/1911.02469">https://arxiv.org/abs/1911.02469</a><a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:4" class="footnote-text"><span><a href="https://arxiv.org/abs/1711.00937">https://arxiv.org/abs/1711.00937</a><a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:5" class="footnote-text"><span><a href="https://arxiv.org/abs/2012.09841">https://arxiv.org/abs/2012.09841</a><a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:6" class="footnote-text"><span><a href="https://arxiv.org/abs/2202.04200">https://arxiv.org/abs/2202.04200</a><a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Computer Vision</category>
      
    </categories>
    
    
    <tags>
      
      <tag>VAE</tag>
      
      <tag>VQVAE</tag>
      
      <tag>VQGAN</tag>
      
      <tag>MaskGIT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>从VAE到MaskGIT(一)</title>
    <link href="/2022/03/09/2-%E4%BB%8EVAE%E5%88%B0MaskGIT/"/>
    <url>/2022/03/09/2-%E4%BB%8EVAE%E5%88%B0MaskGIT/</url>
    
    <content type="html"><![CDATA[<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>本篇是关于一次组会汇报的整理，主题为《生成式模型的发展：从VAE到MaskGIT》，主要记录了三篇文章，包括了VQ-VAE、VQ-GAN以及MaskGIT。生成模型旨在模拟生成<font color = red ><b>与现实世界中观察到的真实数据非常相似的数据</b></font>，作为生成模型的一大分支，VAE和GAN的论文分别与2013年与2014年发表，是之后很多年图像生成任务的基石。而本次汇报讲述的是VAE（变分自编码器）的这一大分支的一些发展，包括它面临的一些问题与这些问题的解决方案。</p><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><h3 id="1-1-什么是生成式模型？"><a href="#1-1-什么是生成式模型？" class="headerlink" title="1.1 什么是生成式模型？"></a>1.1 什么是生成式模型？</h3><p>从概率分布的角度来讲，生成式模型是对<code>x</code>与<code>y</code>的联合概率<code>p(x,y)</code>建模，而判别式模型是对<code>x</code>的条件下，<code>y</code>的条件概率<code>p(y|x;θ)</code>的建模。</p><ul><li><strong>判别式模型（结果导向）：</strong> 根据原有数据集不同的类别数据，学习得到类别之间的差异。根据输入样本<code>x</code>的特征判别其类别。所以它不关心数据是怎么生成的，而是只关心样本之间的差别，根据这种差异性进行分类。</li><li><strong>生成式模型（源头导向）：</strong> 尝试找出数据是怎么生成的，了解每一类样本的一种分布。要判别样本<code>x</code>的类别，要与所有的<code>y</code>进行比较，实际预测结果取决于生成概率最大的<code>y</code>.一般而言生成式模型不做分类任务，因为它的分类效果远差于判别式模型。</li></ul><p class="note note-warning">生成式模型要学习数据分布，反映数据本身的相似度。</p><p>随着深度学习的出现以及神经网络的发展，利用神经网络模拟数据生成过程可以达到更好的效果，可以更好地在给定数据集的情况下，产生于数据集同分布的新样本。</p><ul><li><strong>训练数据集：</strong> 以图像为例（当然也可以是其他类型的数据，比如语音、文字、网络流量等等），每一张图像都可以看作是在一个分布 <script type="math/tex">P_{data}</script> 中抽样得到的样本,如果能够得到 <script type="math/tex">P_{data}</script> 我们就可以无限制地抽样出一些样本，也就可以得到一些与真实图像非常相近的生成图像（<font color = red><b>密度估计问题</b></font>）。但可惜这个分布是不可知的（我们无法根据有限的数据集直接推导出一个高维分布）。</li><li><strong>生成数据：</strong> 假设生成图像服从分布 <script type="math/tex">P_{model}</script>,使可控的已知的分布 <script type="math/tex">P_{model}</script> 与 <script type="math/tex">P_{data}</script> 尽可能接近，就可以近似得知 <script type="math/tex">P_{data}</script> 。</li></ul><p>根据对 <script type="math/tex">P_{model}</script> 的定义是显式还是隐式的可以把生成式模型分为以下两类，比较有代表性的是VAE与GAN。</p><p><img src="/images/2-cv/220309/2.png" alt="图1.1 GAN"></p><p><img src="/images/2-cv/220309/3.png" alt="图1.2 VAE"></p><ul><li><strong>显式密度估计：</strong> 显式构建样本的密度估计函数 <script type="math/tex">p(x;\theta)</script>,通过<strong>最大似然估计</strong>（优化模型参数来最大化在训练数据上的预测概率）来求解参数。</li><li><strong>隐式密度估计：</strong> 不需要构建样本的密度估计函数，只需要拟合模型，使其能够生成复合数据分布的样本。</li></ul><p><img src="/images/2-cv/220309/1.png" alt="图1.3 生成模型的分类"></p><h3 id="1-2-生成式模型有哪些应用？"><a href="#1-2-生成式模型有哪些应用？" class="headerlink" title="1.2 生成式模型有哪些应用？"></a>1.2 生成式模型有哪些应用？</h3><p>在视觉领域，生成式模型主要应用于<strong>图像生成</strong>，图像生成可以分为有条件和无条件两种，有条件的图像生成通常指的是生成具体某一种类别的图像，而无条件图像生成只要生成与真实图像相似的一批图像就可以了。</p><p>在图像生成的基础上，有着许多形形色色的子领域（促进了一波艺（鬼）术（畜）的发展），有代表性的包括，<strong>图像编辑、风格迁移、文本图像生成、图像扩展、图像修复、根据深度图合成图像、根据语义图合成图像等等</strong>，在我看来，这些不同的任务也就是在原始的图像生成上增加了不同的先验信息，只是这些先验知识有的简单有的复杂，最简单的也就是标签，而复杂的就会有一些空间的信息，比如语义图，深度图什么的。而模型的输出同样都是一些图像（尽可能真实）。</p><p><img src="/images/2-cv/220309/4.png" alt="图1.4 生成式模型的应用"></p><h4 id="碎碎念"><a href="#碎碎念" class="headerlink" title="碎碎念"></a>碎碎念</h4><blockquote><p>那么我们为什么要研究生成式模型？我个人认为它是和创造力有关，当然这么想也不一定全面和准确。当前人工智能应该还在一个初级阶段，但它的出现，已然可以取代许多工作了，那些不需要创造力的，重复性的工作，一方面确实会造成失业问题，另一方面也确实在把人们从重复性的工作中解放出来，所以我认为它是利大于弊的。</p><p>而生成模型，它赋予了计算机模拟生成的能力，使它去创造一些原本世界上没有的东西，艺术、文学、诗歌，这些人类独有的元素。我曾看过一个节目，节目中主持人让所有人判断两首诗，哪一首是人写的，哪一首是机器写的，结果确实存在少数部分的人判断错了。前不久我看视频，发现有些生成器居然可以生成小说了，虽然一眼就能看出生成的文章十分粗糙低质，但它居然有了这种能力，一种合成有意义的语句的能力。对视频，对图像而言，这种例子更是数不胜数，合成人物演讲视频，合成一些人眼都看不出真假照片，这甚至都能对安全造成威胁。这说明，生成模型也确实发展到了一个新的高度，是好是坏不太好评判，但是很有趣，我只能这么说。要论威胁，我觉得还是要警惕一些的，科技是双刃剑的道理谁都懂。要论优势，它是否能解放劳动力？是否能让我们的生活更加便利？它研究的意义到底是什么？我没有想明白，我只知道它很好玩，而且可以数据增强哈哈哈哈。但是啊，如果“生成”这种任务完全完成了的话，那我们的很多资源是不是取之不尽，用之不竭了呢？</p><p>”如果我们制造出了和人智力相当的机器人，那么我们可以肯定的说生成模型一定是其中的一部分。“我在某篇博客上看到了这句话。如果要对生成模型溯源，它最基本的理论基础也不过是概率分布，要解决的是密度估计的问题。可本篇的介绍也不过是冰山一角罢了。</p></blockquote><h3 id="1-3-怎样评价生成样本的优劣？"><a href="#1-3-怎样评价生成样本的优劣？" class="headerlink" title="1.3 怎样评价生成样本的优劣？"></a>1.3 怎样评价生成样本的优劣？</h3><p> 一般而言，我们需要一个可量化的评价指标来衡量模型优劣，比如使用分类准确率评价分类模型的性能，使用均方误差评价回归模型的性能。对生成模型来讲也是如此。只是它的指标似乎没有那么直观。</p><p> 生成样本的优劣一般指的是生成的图像的<strong>质量和多样性</strong>，其中质量衡量了生成的每一张图像是不是真实，这里的真实就包括了图像不能模糊，也不能清晰但是很奇怪，而多样性衡量了这一组生成的图像是不是足够分散，因为生成一组相似度很高的图像也是没什么意义的。以下是两类常用的指标。</p><h4 id="1-3-1-Inception-Score（IS）指标"><a href="#1-3-1-Inception-Score（IS）指标" class="headerlink" title="1.3.1 Inception Score（IS）指标"></a>1.3.1 Inception Score（IS）指标</h4><p><img src="/images/2-cv/220309/5.png" alt="图1.5 生成式模型的应用"></p><p>IS指标是基于一批图像送入Inception模型分类的结果来计算的，它的最基本的思想是，对一张图像而言，分类结果越集中，说明这张图像质量越好（毕竟质量足够好计算机才能清晰地认识它），而对于一批图像而言，分类结果应该尽可能分散，也就是说这批图像应该尽可能平均地分布在每个类别中，这样做的原因是出于对多样性的考虑。</p><p>而<strong>分散、集中</strong>等用信息的角度考虑，是熵的一种体现（<strong>熵：信息量越大，熵越大，数据越随机</strong>），综合两个指标，IS采用<strong>互信息</strong>（<strong>给定一个随机变量后，另一个随机变量的不确定性减少程度，也叫信息增益</strong>）的概念，用以描述给定生成样本以后，原始标签不确定性的减少程度（就是样本质量比较好嘛），IS的表达式简要推导：</p><script type="math/tex; mode=display">I(x ; y)=H(y)-H(y \mid x) = \mathbb{E}_{x:}\left[D_{K L}(p(y \mid x) \| p(y))\right]</script><p>其中，KL散度衡量两个分布之间的距离。</p><ul><li><strong>H(y)</strong>:最大化，也就是对于输入的样本，通过inception_v3模型后的类别要均衡，衡量模式坍塌。</li><li><strong>H(y|x)</strong>:最小化，说明对输入的样本，通过inception_v3模型后预测的某一类别的置信度要高，衡量图像生成的质量。</li></ul><p>所以IS指标越大越好。</p><p><strong>IS指标存在的问题：</strong></p><ul><li>偏爱ImageNet中的物体类别，泛化性差。</li><li>类内模式崩溃问题，某一类的图像集中到一个点。</li></ul><h4 id="1-3-2-Frechet-Inception-Distance（FID）指标"><a href="#1-3-2-Frechet-Inception-Distance（FID）指标" class="headerlink" title="1.3.2 Fréchet Inception Distance（FID）指标"></a>1.3.2 Fréchet Inception Distance（FID）指标</h4><p><img src="/images/2-cv/220309/6.png" alt="图1.6 生成式模型的应用"></p><p>FID指标比IS指标更看重真实数据的影响，它会取<strong>生成样本与真实样本的2048个中间特征</strong>进行比较，假设中间特征符合<strong>多元高斯分布</strong>，计算两个分布之间的<strong>弗雷歇距离</strong>，FID的数值越小，表示两个高斯分布越接近。</p><p>所以FID指标越小越好。</p><p>实践中发现，FID对噪声具有比较好的鲁棒性，能够对生成图像的质量有比较好的评价，其给出的分数与人类的视觉判断比较一致，并且FID的计算复杂度并不高。整体而言，FID还是比较有效的，其理论上的不足之处在于：高斯分布的简化假设在实际中并不成立。</p>]]></content>
    
    
    <categories>
      
      <category>Computer Vision</category>
      
    </categories>
    
    
    <tags>
      
      <tag>VAE</tag>
      
      <tag>VQVAE</tag>
      
      <tag>VQGAN</tag>
      
      <tag>MaskGIT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Weekly-211226</title>
    <link href="/2021/12/27/1-Weekly-211226/"/>
    <url>/2021/12/27/1-Weekly-211226/</url>
    
    <content type="html"><![CDATA[<h3 id="本周学习汇报"><a href="#本周学习汇报" class="headerlink" title="本周学习汇报"></a>本周学习汇报</h3><span id="more"></span><blockquote><div>            <input type="checkbox" disabled checked="checked">数学复习          </div><div>            <input type="checkbox" disabled checked="checked">算法分析大作业          </div></blockquote><h2 id="1-工程数学的矩阵理论"><a href="#1-工程数学的矩阵理论" class="headerlink" title="1. 工程数学的矩阵理论"></a>1. 工程数学的矩阵理论</h2><p><a href="https://github.com/EllenWong/EllenWong.github.io/tree/master/images/1-weeklynote/211226">笔记链接</a></p><h2 id="2-算法分析大作业"><a href="#2-算法分析大作业" class="headerlink" title="2. 算法分析大作业"></a>2. 算法分析大作业</h2><ul><li>动态规划算法大作业</li></ul><h2 id="3-其他"><a href="#3-其他" class="headerlink" title="3. 其他"></a>3. 其他</h2><ul><li><strong>组会：</strong>《Dynamic Anchor Learning for Arbitrary-Oriented Object 》；</li><li><strong>工程数学：</strong>复习；</li><li><strong>高级算法分析：</strong>大作业问题求解-动态规划算法；</li><li><strong>高级机器学习：</strong>论文分享；</li></ul><h2 id="4-下一步计划"><a href="#4-下一步计划" class="headerlink" title="4. 下一步计划"></a>4. 下一步计划</h2><ul><li>复习</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Final review</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Weekly-211219</title>
    <link href="/2021/12/19/1-Weekly-211219/"/>
    <url>/2021/12/19/1-Weekly-211219/</url>
    
    <content type="html"><![CDATA[<h3 id="本周学习汇报"><a href="#本周学习汇报" class="headerlink" title="本周学习汇报"></a>本周学习汇报</h3><span id="more"></span><blockquote><div>            <input type="checkbox" disabled checked="checked">课堂展示(人机交互)<sup id=fnref:1 class=footnote-ref><a href=#fn:1 rel=footnote><span class=hint--top hint--rounded aria-label=https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445588>[1]</span></a></sup>          </div><div>            <input type="checkbox" disabled checked="checked">Image-to-Image Translation: Methods and Applications<sup id=fnref:2 class=footnote-ref><a href=#fn:2 rel=footnote><span class=hint--top hint--rounded aria-label=https://arxiv.org/abs/2101.08629>[2]</span></a></sup>          </div><div>            <input type="checkbox" disabled checked="checked">数学复习1-2章          </div><div>            <input type="checkbox" disabled >算法分析大作业          </div></blockquote><h2 id="1-情绪与代理对虚拟现实临场感的影响"><a href="#1-情绪与代理对虚拟现实临场感的影响" class="headerlink" title="1. 情绪与代理对虚拟现实临场感的影响"></a>1. 情绪与代理对虚拟现实临场感的影响</h2><p><a class="btn" href="https://github.com/EllenWong/HomeWork/blob/2021/presentation%20for%20Human-computer%20interaction.pptx" title="PPT">HCI-presentation</a> </p><h2 id="2-I2I任务综述论文阅读"><a href="#2-I2I任务综述论文阅读" class="headerlink" title="2. I2I任务综述论文阅读"></a>2. I2I任务综述论文阅读</h2><h3 id="2-1-任务介绍"><a href="#2-1-任务介绍" class="headerlink" title="2.1 任务介绍"></a>2.1 任务介绍</h3><ul><li><strong>简介：</strong>I2I任务即Image-to-Image Translation，也就是图像与图像之间转换的任务，属于图像生成的一个子领域。它旨在完成图像从source domain到target domain的转变，并且在转变过程中保留原图像的一些内容特性。此任务可具体应用于<strong>图像合成、分割、风格迁移、图像修复、姿态估计</strong>等领域，常用的模型为生成模型，可分为 <strong>two-domain I2I tasks</strong> 和 <strong>multi-domain I2I tasks</strong>。</li><li><strong>主要方法：</strong>VAE、GAN</li></ul><h3 id="2-2-方法分类"><a href="#2-2-方法分类" class="headerlink" title="2.2 方法分类"></a>2.2 方法分类</h3><p><img src="/images/1-weeklynote/211219/4.png" alt="图2-1 I2I方法分类"></p><p>&emsp;&emsp;可见对此任务而言，有监督学习方法和无监督学习方法，其中无监督学习方法为主，同时在Few-Shot I2I领域也有着广泛的研究。</p><h3 id="2-3-评价指标"><a href="#2-3-评价指标" class="headerlink" title="2.3 评价指标"></a>2.3 评价指标</h3><ul><li><strong>主观评估：</strong>AMT perceptual studies</li><li><strong>客观评估：</strong>PSNR、SSIM、IS、FID、CIS、PD、KID、DC（最新）等等；</li></ul><h3 id="2-4-SOTA模型"><a href="#2-4-SOTA模型" class="headerlink" title="2.4 SOTA模型"></a>2.4 SOTA模型</h3><p><img src="/images/1-weeklynote/211219/5.png" alt="图2-2 I2I方法对比"></p><p>&emsp;&emsp;由此可见，相对最优的是StyleGANv2与StarGANv2.</p><h2 id="3-数学复习"><a href="#3-数学复习" class="headerlink" title="3. 数学复习"></a>3. 数学复习</h2><h3 id="3-1-第一章-线性空间与线性变换"><a href="#3-1-第一章-线性空间与线性变换" class="headerlink" title="3.1 第一章 线性空间与线性变换"></a>3.1 第一章 线性空间与线性变换</h3><p><img src="/images/1-weeklynote/211219/1.jpg" alt="图3-1 1.1"></p><p><img src="/images/1-weeklynote/211219/2.jpg" alt="图3-2 1.2"></p><h3 id="3-1-第二章-λ矩阵与矩阵的Jordan标准形"><a href="#3-1-第二章-λ矩阵与矩阵的Jordan标准形" class="headerlink" title="3.1 第二章 λ矩阵与矩阵的Jordan标准形"></a>3.1 第二章 λ矩阵与矩阵的Jordan标准形</h3><p><img src="/images/1-weeklynote/211219/3.jpg" alt="图3-3 2.1"></p><h2 id="4-其他"><a href="#4-其他" class="headerlink" title="4. 其他"></a>4. 其他</h2><ul><li><strong>组会：</strong>《基于深度学习的光学遥感图像目标检测介绍》；</li><li><strong>工程数学：</strong>Kronecker积,复习；</li><li><strong>网络安全：</strong>《深度学习赋能的恶意代码攻防研究进展》,《基于深度学习的图像隐写方法研究》；</li><li><strong>高级算法分析：</strong>大作业问题求解-动态规划算法；</li><li><strong>高级机器学习：</strong>论文分享；</li></ul><h2 id="5-下一步计划"><a href="#5-下一步计划" class="headerlink" title="5. 下一步计划"></a>5. 下一步计划</h2><ol><li>算法分析大作业；</li><li>机器学习综述大作业；</li><li>数学与英语的复习；</li><li>论文整理；</li></ol><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445588">https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445588</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://arxiv.org/abs/2101.08629">https://arxiv.org/abs/2101.08629</a><a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    
    <tags>
      
      <tag>presentation</tag>
      
      <tag>I2I</tag>
      
      <tag>Final review</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Weekly-211212</title>
    <link href="/2021/12/12/1-Weekly-211212/"/>
    <url>/2021/12/12/1-Weekly-211212/</url>
    
    <content type="html"><![CDATA[<h3 id="本周学习汇报"><a href="#本周学习汇报" class="headerlink" title="本周学习汇报"></a>本周学习汇报</h3><span id="more"></span><blockquote><div>            <input type="checkbox" disabled checked="checked">课堂展示(英语&&网安)<sup id=fnref:1 class=footnote-ref><a href=#fn:1 rel=footnote><span class=hint--top hint--rounded aria-label=https://github.com/EllenWong/HomeWork>[1]</span></a></sup>          </div><div>            <input type="checkbox" disabled >课堂展示(人机交互)          </div><div>            <input type="checkbox" disabled checked="checked">CoMoGAN: continuous model-guided image-to-image translation（CVPR 2021 oral）[^3]          </div></blockquote><h2 id="1-课堂展示-PPT"><a href="#1-课堂展示-PPT" class="headerlink" title="1. 课堂展示(PPT)"></a>1. 课堂展示(PPT)</h2><p><a class="btn" href="https://github.com/EllenWong/HomeWork" title="PPT">那俩PPT</a> </p><h2 id="2-连续模仿引导GAN"><a href="#2-连续模仿引导GAN" class="headerlink" title="2. 连续模仿引导GAN"></a>2. 连续模仿引导GAN</h2><ul><li><strong>领域：</strong>图像生成（图像域到图像域）</li><li><strong>摘要：</strong>CoMoGAN是一种连续的GAN，它依赖于目标数据在函数流型上的无监督重组。为此，我们引入了一个新的Functional Instance Normalization层和残差机制，从位置和目标流型上对图像内容进行解缠研究，我们依靠受物理启发的朴素模型来指导训练，同时允许私有模型/转换特征。CoMoGAN可以与任何GAN的backbone一起使用，并且允许新类型的图像转换，例如 timelapse generation和detached linear translation.</li></ul><p><img src="/images/1-weeklynote/211212/1.png" alt="图2-1 方法流程"></p><h2 id="3-其他"><a href="#3-其他" class="headerlink" title="3. 其他"></a>3. 其他</h2><ul><li><strong>组会：</strong>《农作物遥感估产综述》；</li><li><strong>工程数学：</strong>伪逆矩阵，广义逆矩阵，最小模解，Kronecker积；</li><li><strong>网络安全：</strong>《深度学习赋能的恶意代码攻防研究进展》；</li><li><strong>高级算法分析：</strong>大作业大纲的确定；</li><li><strong>高级机器学习：</strong>规则学习；</li></ul><h2 id="4-下一步计划"><a href="#4-下一步计划" class="headerlink" title="4. 下一步计划"></a>4. 下一步计划</h2><ol><li>人机交互：Presentation*1；</li><li>算法分析：动态规划算法的大作业论文撰写；</li><li>机器学习综述论文大纲的确定；</li><li>I2I综述*1；</li></ol><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://github.com/EllenWong/HomeWork">https://github.com/EllenWong/HomeWork</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://arxiv.org/abs/2103.06879">https://arxiv.org/abs/2103.06879</a><a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Weekly Note</category>
      
    </categories>
    
    
    <tags>
      
      <tag>presentation</tag>
      
      <tag>I2I</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Weekly-211205</title>
    <link href="/2021/12/05/1-Weekly-211205/"/>
    <url>/2021/12/05/1-Weekly-211205/</url>
    
    <content type="html"><![CDATA[<h3 id="本周学习汇报"><a href="#本周学习汇报" class="headerlink" title="本周学习汇报"></a>本周学习汇报</h3><span id="more"></span><blockquote><div>            <input type="checkbox" disabled checked="checked">人机交互论文_Effects of Emotion and Agency on Presence in Virtual Reality(PPT)<sup id=fnref:1 class=footnote-ref><a href=#fn:1 rel=footnote><span class=hint--top hint--rounded aria-label=https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445588>[1]</span></a></sup>          </div><div>            <input type="checkbox" disabled checked="checked">英语课堂展示_The Hundred Schools of Thought(PPT)          </div><div>            <input type="checkbox" disabled checked="checked">网络安全论文_深度学习赋能的恶意代码攻击(PPT)<sup id=fnref:2 class=footnote-ref><a href=#fn:2 rel=footnote><span class=hint--top hint--rounded aria-label=http://cjc.ict.ac.cn/online/bfpub/jtt-202058150207.pdf>[2]</span></a></sup>          </div></blockquote><p>//ppt待下周上传至GitHub仓库</p><h2 id="1-深度学习赋能的恶意代码攻击"><a href="#1-深度学习赋能的恶意代码攻击" class="headerlink" title="1. 深度学习赋能的恶意代码攻击"></a>1. 深度学习赋能的恶意代码攻击</h2><ul><li><strong>恶意代码：</strong>故意编制或设置的、对网络或系统会产生威胁或潜在威胁的计算机代码。有二进制、JavaScript、PowerShell等形式，均可被深度学习的方式处理；</li></ul><h3 id="1-1-赋能攻击链"><a href="#1-1-赋能攻击链" class="headerlink" title="1.1 赋能攻击链"></a>1.1 赋能攻击链</h3><p><img src="/images/1-weeklynote/211205/1.png" alt="图1-1 赋能攻击链"></p><ul><li>赋能攻击相当于一个武器（恶意代码）的制造、投放以及发挥其效应的过程。</li></ul><h3 id="1-2-深度学习助力攻击技术"><a href="#1-2-深度学习助力攻击技术" class="headerlink" title="1.2 深度学习助力攻击技术"></a>1.2 深度学习助力攻击技术</h3><h4 id="1-2-1-基于对抗样本生成的自动化免杀"><a href="#1-2-1-基于对抗样本生成的自动化免杀" class="headerlink" title="1.2.1 基于对抗样本生成的自动化免杀"></a>1.2.1 基于对抗样本生成的自动化免杀</h4><ul><li>免杀：使恶意代码不被杀毒软件查杀，两者能够共存。</li></ul><p><img src="/images/1-weeklynote/211205/2.png" alt="图1-2 自动化免杀"></p><h4 id="1-2-2-基于自然语言生成的自动化网络钓鱼"><a href="#1-2-2-基于自然语言生成的自动化网络钓鱼" class="headerlink" title="1.2.2 基于自然语言生成的自动化网络钓鱼"></a>1.2.2 基于自然语言生成的自动化网络钓鱼</h4><ul><li>从传统网络钓鱼方式到自动化的网络钓鱼</li></ul><p><img src="/images/1-weeklynote/211205/3.png" alt="图1-3 自动化网络钓鱼"></p><h4 id="1-2-3-基于神经网络的精准定位与打击"><a href="#1-2-3-基于神经网络的精准定位与打击" class="headerlink" title="1.2.3 基于神经网络的精准定位与打击"></a>1.2.3 基于神经网络的精准定位与打击</h4><p><img src="/images/1-weeklynote/211205/4.png" alt="图1-4 精准定位与打击"></p><ul><li>根据二分类模型对特定图像的检测，其全连接层的输出作为对称密钥，也就是说，只有当特定目标被检测到时，该攻击才会对攻击载荷加密，然后与二分类模型一起嵌入到正常应用程序中。</li></ul><h4 id="1-2-4-基于生成对抗网络的流量模仿"><a href="#1-2-4-基于生成对抗网络的流量模仿" class="headerlink" title="1.2.4 基于生成对抗网络的流量模仿"></a>1.2.4 基于生成对抗网络的流量模仿</h4><p><img src="/images/1-weeklynote/211205/5.png" alt="图1-5 流量模仿"></p><ul><li>图源IDSGAN：基于黑盒模型进行检测，设计判别器对两者进行判断。</li></ul><h4 id="1-2-5-基于黑盒模型的攻击意图隐藏"><a href="#1-2-5-基于黑盒模型的攻击意图隐藏" class="headerlink" title="1.2.5 基于黑盒模型的攻击意图隐藏"></a>1.2.5 基于黑盒模型的攻击意图隐藏</h4><p><img src="/images/1-weeklynote/211205/6.png" alt="图1-6 攻击意图隐藏"></p><h3 id="1-3-展望"><a href="#1-3-展望" class="headerlink" title="1.3 展望"></a>1.3 展望</h3><h4 id="1-3-1-存在的问题"><a href="#1-3-1-存在的问题" class="headerlink" title="1.3.1 存在的问题"></a>1.3.1 存在的问题</h4><ul><li><strong>重点问题：</strong>新型恶意代码的生成机理、入侵方式、攻击释放原理及生存对抗特性；</li><li><strong>防御措施不完善：</strong>针对基于神经网络的精准定位与打击、基于生成对抗网络的流量模仿、基于黑盒模型的攻击意图隐藏等攻击技术的防御措施尚未被提出。</li></ul><h4 id="1-3-2-未来发展"><a href="#1-3-2-未来发展" class="headerlink" title="1.3.2 未来发展"></a>1.3.2 未来发展</h4><ul><li><strong>攻击者主动利用神经网络：</strong>基于神经网络的精准定位与打击与基于黑盒模型的攻击意图隐藏主动利用深度学习模型作为恶意代码攻击的组件。</li><li><strong>数据集：</strong>需要建立有效、公开的数据集供研究者使用；</li><li><strong>僵尸网络：</strong>存在新的赋能研究的可能性，如僵尸程序的自治，智能僵尸网络(Hivenet和Swarmbot)的出现，这也是主动利用深度神经网络的一种潜在威胁。</li></ul><h2 id="2-情绪与代理对虚拟现实临场感的影响"><a href="#2-情绪与代理对虚拟现实临场感的影响" class="headerlink" title="2. 情绪与代理对虚拟现实临场感的影响"></a>2. 情绪与代理对虚拟现实临场感的影响</h2><h3 id="2-1-实验方法"><a href="#2-1-实验方法" class="headerlink" title="2.1 实验方法"></a>2.1 实验方法</h3><ul><li><strong>数据采集：</strong></li></ul><ol><li>实验时间：平均30分钟。</li><li>任务前问卷调查：参与者需要阅读一份描述该研究的信息表并表示同意。</li><li>实验前已知：VE被设计为愉悦诱导还是恐惧诱导，是否有代理。</li><li>实验中：参与者被协助佩戴HTC Vive。然后，参与者完成校准阶段，在此期间调整HMD Vive，直到他们能够读取呈现的最小文本大小。如果是被分配到HA或者FA条件下，参与者将被告知如何使用手动控制器。</li><li>实验后信息采集：参与者移除HMD，并在电脑屏幕上使用Qualtrics完成评估主观情绪强度、存在和其他测量的问题。在虚拟现实体验结束后，首先完成评估快乐与恐惧的问题。</li></ol><ul><li><strong>参与者：</strong></li></ul><ol><li>人数：121位参与者（39位男性，86位女性）？？？</li><li>年龄：18-45岁，均值21.4，标准差4.40；</li><li>实验分布：FA (n = 30), FNA (n = 29), HA (n = 31) , HNA(n = 31)；</li><li>参与费用：5英镑==42.19人民币；</li><li>其他：最初的问卷调查了参与者的神经疾病史、心理或情绪问题、药物使用、癫痫或医疗设备(如心脏泵)使用的情况。此外，他们还接受了犬恐惧症(害怕狗)的筛查。他们对虚拟现实的体验水平也进行了评估。所有参与者的视力和听力正常或矫正至正常。</li></ol><h3 id="2-2-结果分析"><a href="#2-2-结果分析" class="headerlink" title="2.2 结果分析"></a>2.2 结果分析</h3><ul><li>单因素方差分析<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://zhuanlan.zhihu.com/p/151572601">[3]</span></a></sup></li></ul><p><img src="/images/1-weeklynote/211205/7.png" alt="图2-1 单因素方差分析"></p><ul><li>皮尔逊相关系数<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://www.zhihu.com/question/19734616/answer/174098489">[4]</span></a></sup></li></ul><p><img src="/images/1-weeklynote/211205/8.png" alt="图2-2 皮尔逊相关系数"></p><ul><li>t检验<sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://zhuanlan.zhihu.com/p/138711532">[5]</span></a></sup><br><strong>1. 单样本均值检验（One-sample t-test）</strong><br>  用于检验总体方差未知、正态数据或近似正态的单样本的均值是否与已知的总体均值相等<br><strong>2. 两独立样本均值检验（Independent two-sample t-test）</strong><br>  用于检验两对独立的正态数据或近似正态的样本的均值是否相等，这里可根据总体方差是否相等分类讨论<br><strong>3. 配对样本均值检验（Dependent t-test for paired samples）</strong><br>  用于检验一对配对样本的均值的差是否等于某一个值<br><strong>4. 回归系数的显著性检验（t-test for regression coefficient significance）</strong><br>  用于检验回归模型的解释变量对被解释变量是否有显著影响</li></ul><h2 id="3-其他"><a href="#3-其他" class="headerlink" title="3. 其他"></a>3. 其他</h2><ul><li><strong>组会：</strong>《半监督目标检测任务及研究进展概述》；</li><li><strong>工程数学：</strong>矩阵指数函数、矩阵三角函数、函数矩阵对纯量的导数与积分、函数向量的线性相关性、矩阵的广义逆；</li><li><strong>网络安全：</strong>《基于卷积神经网络的低嵌入率空域隐写分析》；</li><li><strong>高级算法分析：</strong>聚类+大作业思考；</li><li><strong>高级机器学习：</strong>概率图模型；</li><li><strong>科技论文写作：</strong>method、results、conclusion、acknowledgments、references等的撰写；</li></ul><h2 id="4-下一步计划"><a href="#4-下一步计划" class="headerlink" title="4. 下一步计划"></a>4. 下一步计划</h2><ol><li>Presentation*3；</li><li>动态规划算法的大作业论文撰写；</li><li>机器学习综述论文大纲的确定及对之前阅读论文的整理；</li></ol><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445588">https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445588</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="http://cjc.ict.ac.cn/online/bfpub/jtt-202058150207.pdf">http://cjc.ict.ac.cn/online/bfpub/jtt-202058150207.pdf</a><a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span><a href="https://zhuanlan.zhihu.com/p/151572601">https://zhuanlan.zhihu.com/p/151572601</a><a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:4" class="footnote-text"><span><a href="https://www.zhihu.com/question/19734616/answer/174098489">https://www.zhihu.com/question/19734616/answer/174098489</a><a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:5" class="footnote-text"><span><a href="https://zhuanlan.zhihu.com/p/138711532">https://zhuanlan.zhihu.com/p/138711532</a><a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Weekly Note</category>
      
    </categories>
    
    
    <tags>
      
      <tag>presentation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Weekly-211128</title>
    <link href="/2021/11/28/1-Weekly-211128/"/>
    <url>/2021/11/28/1-Weekly-211128/</url>
    
    <content type="html"><![CDATA[<h3 id="本周学习汇报"><a href="#本周学习汇报" class="headerlink" title="本周学习汇报"></a>本周学习汇报</h3><span id="more"></span><blockquote><div>            <input type="checkbox" disabled >人机交互论文(PPT)          </div><div>            <input type="checkbox" disabled checked="checked">英语课堂展示(PPT)          </div><div>            <input type="checkbox" disabled >网络安全论文(PPT)          </div></blockquote><h2 id="1-PPT准备-未定稿"><a href="#1-PPT准备-未定稿" class="headerlink" title="1. PPT准备(未定稿)"></a>1. PPT准备(未定稿)</h2><p>//之后上传至GitHub仓库</p><h3 id="1-1-情绪与代理对虚拟现实临场感的影响"><a href="#1-1-情绪与代理对虚拟现实临场感的影响" class="headerlink" title="1.1 情绪与代理对虚拟现实临场感的影响"></a>1.1 情绪与代理对虚拟现实临场感的影响</h3><p><img src="/images/1-weeklynote/211128/1.png" alt="图1-1 情绪与代理对虚拟现实临场感的影响"></p><h3 id="1-2-英语课堂展示"><a href="#1-2-英语课堂展示" class="headerlink" title="1.2 英语课堂展示"></a>1.2 英语课堂展示</h3><ul><li><strong>Topic selection</strong>：The Hundred Schools of Thought</li><li><strong>PPT(Unfinalized)</strong>：Introduction、Writings、Influence、Biological Evolution(4’30’’)</li></ul><h3 id="1-3-深度学习赋能的恶意代码攻防研究进展"><a href="#1-3-深度学习赋能的恶意代码攻防研究进展" class="headerlink" title="1.3 深度学习赋能的恶意代码攻防研究进展"></a>1.3 深度学习赋能的恶意代码攻防研究进展</h3><p><img src="/images/1-weeklynote/211128/2.png" alt="图1-2 深度学习赋能的恶意代码攻防研究进展"></p><h2 id="2-其他"><a href="#2-其他" class="headerlink" title="2. 其他"></a>2. 其他</h2><ul><li><strong>组会：</strong>《Transformer在语义分割中的应用》；</li><li><strong>工程数学：</strong>矩阵多项式、矩阵函数多项式表示、矩阵函数Jordan表示、矩阵函数的幂级数表示；</li><li><strong>网络安全：</strong>《恶意代码流量动态伪装》、《知识图谱构建技术综述》；</li><li><strong>高级算法分析：</strong>线性方程求解方法；</li><li><strong>高级机器学习：</strong>半监督学习；</li><li><strong>高级人机交互技术：</strong>《AtaTouch Robust Finger Pinch Detection for a VR Controller》；</li></ul><h2 id="3-下一步计划"><a href="#3-下一步计划" class="headerlink" title="3. 下一步计划"></a>3. 下一步计划</h2><ol><li>准备接下来的课堂展示</li><li>一些科目的复习与大作业</li></ol>]]></content>
    
    
    <categories>
      
      <category>Weekly Note</category>
      
    </categories>
    
    
    <tags>
      
      <tag>presentation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Weekly-211121</title>
    <link href="/2021/11/21/1-Weekly-211121/"/>
    <url>/2021/11/21/1-Weekly-211121/</url>
    
    <content type="html"><![CDATA[<h3 id="本周学习汇报"><a href="#本周学习汇报" class="headerlink" title="本周学习汇报"></a>本周学习汇报</h3><span id="more"></span><blockquote><div>            <input type="checkbox" disabled checked="checked">Don’t Generate Me: Training Differentially Private Generative Models with Sinkhorn Divergence(NeurlPS2021)<sup id=fnref:1 class=footnote-ref><a href=#fn:1 rel=footnote><span class=hint--top hint--rounded aria-label=https://nv-tlabs.github.io/DP-Sinkhorn/>[1]</span></a></sup>          </div><div>            <input type="checkbox" disabled checked="checked">Do Vision Transformers See Like Convolutional Neural Networks?(NeurlPS2021)<sup id=fnref:2 class=footnote-ref><a href=#fn:2 rel=footnote><span class=hint--top hint--rounded aria-label=https://arxiv.org/pdf/2108.08810.pdf>[2]</span></a></sup>          </div> <div>            <input type="checkbox" disabled checked="checked">人机交互论文阅读及翻译(余两周准备)          </div></blockquote><h2 id="1-差分隐私——对私有数据的保护（NVIDIA）"><a href="#1-差分隐私——对私有数据的保护（NVIDIA）" class="headerlink" title="1. 差分隐私——对私有数据的保护（NVIDIA）"></a>1. 差分隐私——对私有数据的保护（NVIDIA）</h2><h3 id="1-1-HighLight"><a href="#1-1-HighLight" class="headerlink" title="1.1 HighLight"></a>1.1 HighLight</h3><p><img src="/images/1-weeklynote/211121/1.png" alt="图1-1 DP-Sinkhorn应用"></p><ul><li><strong>差分隐私:</strong>是一个<font color=red ><b>数据共享手段</b></font>，可以实现仅分享可以描述数据库的一些统计特征、而不公开具体到个人的信息。 <font color=red ><b>差分隐私背后的直观想法</b></font>是：如果随机修改数据库中的一个记录造成的影响足够小，求得的统计特征就不能被用来反推出单一记录的内容；这一特性可以被用来保护隐私。</li><li><strong>DP-Sinkhorn:</strong> 一种新的基于最优传输的<font color=red ><b>生成方法</b></font>，用于从具有差异隐私的私有数据中学习数据分布(最小化Sinkhorn divergence)。</li></ul><h3 id="1-2-Motivation"><a href="#1-2-Motivation" class="headerlink" title="1.2 Motivation"></a>1.2 Motivation</h3><ul><li><strong>目的：</strong>在满足差分隐私约束的同时学习生成模型；</li><li><strong>原有挑战：</strong>GAN在私有数据生成方面具有挑战性（不稳定，超参数难调节，容易模式坍塌）；</li><li><strong>非对抗生成学习方法：</strong>具有更稳定的收敛性，产生更高质量的输出，并且对超参数的选择更稳健。</li></ul><h3 id="1-3-Method"><a href="#1-3-Method" class="headerlink" title="1.3 Method"></a>1.3 Method</h3><p><img src="/images/1-weeklynote/211121/2.png" alt="图1-2 DP-Sinkhorn方法流程"></p><h3 id="1-4-Result"><a href="#1-4-Result" class="headerlink" title="1.4 Result"></a>1.4 Result</h3><p><img src="/images/1-weeklynote/211121/3.png" alt="图1-3 MNIST与Fashion-MNIST实验结果"></p><h2 id="2-ViT与CNN对比（Google-Research-Brain-Team）"><a href="#2-ViT与CNN对比（Google-Research-Brain-Team）" class="headerlink" title="2. ViT与CNN对比（Google Research, Brain Team）"></a>2. ViT与CNN对比（Google Research, Brain Team）</h2><h3 id="2-1-HighLight"><a href="#2-1-HighLight" class="headerlink" title="2.1 HighLight"></a>2.1 HighLight</h3><ul><li>ViTs与CNNs<font color=red ><b>内部表示结构</b></font>有显著差异，在浅层与深层之间ViT有着更统一的表示。</li><li>分析如何利用<font color=red ><b>局部/全局空间信息</b></font>，与浅层的ResNet相比，ViT包含更多的全局信息，可获得数量上更多的特征。</li><li>浅层合并局部信息仍至关重要，<font color=red ><b>大规模预训练数据</b></font>可帮助前期的注意力层做到这一点。</li><li>与ResNets相比，ViT中的<font color=red ><b>跳过连接</b></font>的影响更大，对性能和表示相似性有很强的影响。</li><li>研究<font color=red ><b>空间信息</b></font>的效果保留，发现了空间定位与分类方法之间的联系。</li><li>研究数据集规模对<font color=red ><b>迁移学习</b></font>的影响，线性探针研究揭示了其用于高质量的中间表示的重要性。</li></ul><h3 id="2-2-Motivation"><a href="#2-2-Motivation" class="headerlink" title="2.2 Motivation"></a>2.2 Motivation</h3><ul><li>探究ViT与CNN相比是如何处理<strong>视觉任务</strong>的。</li><li><font color=blue >Q1</font>: Vision Transformers如何解决这些<font color=red ><b>基于图像的问题</b></font>任务？ </li><li><font color=blue >Q2</font>: 它们是否像卷积一样，从头开始学习<font color=red ><b>相同的归纳偏差</b></font>？或者它们拥有<font color=red ><b>新的任务表示</b></font>？ </li><li><font color=blue >Q3</font>: <font color=red ><b>规模</b></font>在学习这些表示中的作用是什么？</li></ul><h3 id="2-3-Method"><a href="#2-3-Method" class="headerlink" title="2.3 Method"></a>2.3 Method</h3><p><font color=black ><b>数据集及网络</b></font>：</p><ul><li><font color=blue >在 JFT-300M </font>数据集上对 <font color=blue >ResNet50x1, ResNet152x2, ViT-B/32, ViT-B/16, ViT-L/16 和 ViT-H/14 </font>进行了对比</li></ul><p><font color=black ><b>度量方式</b></font>：</p><ol><li>Representation Similarity（<font color=red ><b>表示相似性</b></font>）.</li><li><font color=red ><b>CKA</b></font>（centered kernel alignment）——定量的对比网络之间的特征表达.</li></ol><p><img src="/images/1-weeklynote/211121/4.png" alt="图2-1 CKA计算方式"></p><h3 id="2-4-Result"><a href="#2-4-Result" class="headerlink" title="2.4 Result"></a>2.4 Result</h3><ul><li><strong>相似度比较</strong></li></ul><p><img src="/images/1-weeklynote/211121/5.png" alt="图2-2 相似度比较"></p><ul><li><strong>聚合全局信息能力比较</strong></li></ul><p><img src="/images/1-weeklynote/211121/6.png" alt="图2-3 聚合全局信息能力比较"></p><ul><li><strong>残差连接的影响</strong></li></ul><p><img src="/images/1-weeklynote/211121/7.png" alt="图2-4 残差连接的影响"></p><ul><li><strong>空间定位</strong></li></ul><p><img src="/images/1-weeklynote/211121/8.png" alt="图2-5 空间定位"></p><h2 id="3-情绪与代理对临场感的影响—人机交互相关研究"><a href="#3-情绪与代理对临场感的影响—人机交互相关研究" class="headerlink" title="3. 情绪与代理对临场感的影响—人机交互相关研究"></a>3. 情绪与代理对临场感的影响—人机交互相关研究</h2><h3 id="3-1-HighLight"><a href="#3-1-HighLight" class="headerlink" title="3.1 HighLight"></a>3.1 HighLight</h3><ul><li><p><strong>摘要：</strong>虚拟现实(VR)最重要的特征之一可以说是它可以<font color=red ><b>诱导更高的临场感</b></font>。研究者关于临场感如何受到人的影响仍然没有定论，如<font color=red ><b>情绪</b></font>和<font color=red ><b>代理</b></font>等因素。我们在这里采用了一种新颖的设计去研究测试虚拟现实引发的愉悦或是恐惧，有或者没有用户代理。来自121位参与者的结果表明，虚拟环境诱发的主导情绪与临场感呈正相关。此外，代理对临场感也存在着积极的影响，除此之外也缓和了情绪对临场感的影响。我们展示了首次情绪和代理对临场感的影响不是直接的，而是可以通过主观测量的分离的设计因素来建模。我们讨论了这些发现如何解释相关工作看似矛盾的结果，以及它们对VR设计的影响。</p></li><li><p><strong>关键词：</strong>虚拟现实；临场感；情绪；代理；</p></li></ul><h3 id="3-2-Motivation"><a href="#3-2-Motivation" class="headerlink" title="3.2 Motivation"></a>3.2 Motivation</h3><ul><li>了解如何更好地促进用户的<strong>临场感</strong>；</li><li>提高用户在VR中的满意度、愉悦度、<strong>参与度</strong>；</li></ul><blockquote><p><font color = blue size = 4><b>research question</b></font><br>RQ1：用户的情绪与VR中的临场感之间有关系吗？<br>RQ2：在VR中为用户提供的代理级别如何影响他们的临场感？<br>RQ3：我们如何模拟情绪和代理对VR中的临场感的影响？  </p></blockquote><h3 id="3-3-Related-Work"><a href="#3-3-Related-Work" class="headerlink" title="3.3 Related Work"></a>3.3 Related Work</h3><ul><li><strong>Felnhofer等人</strong>诱发VR中五种不同的情绪，并发现不同等级的临场感保持不同虚拟环境的一致性，在他们的效价变化时，引发更广泛的情绪。</li><li><strong>Baños等人</strong>的一个研究尝试去调查情绪强度与临场感的关系。他们发现情绪强度与临场感在引发放松或快乐的虚拟环境中呈正相关。</li><li><strong>Pallaviciniet等人</strong>比较了在2D屏幕和虚拟现实上玩电子游戏所带来的愉悦感水平，他们发现，与2D屏幕相比，虚拟现实条件引发的快乐和恐惧水平(取决于玩的游戏)明显更高。</li><li><strong>Piccioneetal</strong>最近的一项研究使用了一项高尔夫模拟任务，并发现在玩游戏的参与者群体和其他观看游戏的参与者群体之间没有差异。</li><li><strong>Borrego等人</strong>使用VR步进任务来比较中风患者和健康个体在VR中的临场感，有趣的是，他们发现用户感知的代理和临场感之间没有相关性。</li></ul><h3 id="3-4-Method"><a href="#3-4-Method" class="headerlink" title="3.4 Method"></a>3.4 Method</h3><ul><li><strong>外界刺激的选择：</strong>视觉研究、听觉研究、设备选择；</li><li><strong>测量方式：</strong>问卷调查；</li><li><strong>参与者：</strong>121名（39男，86女）；</li><li><strong>结果分析：</strong>ANOVA（重复测量方差分析）；</li></ul><h3 id="3-5-Result"><a href="#3-5-Result" class="headerlink" title="3.5 Result"></a>3.5 Result</h3><p><img src="/images/1-weeklynote/211121/9.png" alt="图3-1 关系散点图"></p><h2 id="4-其他"><a href="#4-其他" class="headerlink" title="4. 其他"></a>4. 其他</h2><ul><li><strong>工程数学：</strong>向量范数、矩阵范数、矩阵序列与极限、矩阵幂级数；</li><li><strong>英语：</strong>观看电影《猫鼠游戏》；</li><li><strong>网络安全：</strong>《恶意代码流量动态伪装》、《云数据综述》；</li><li><strong>高级算法分析：</strong>近似算法；</li><li><strong>高级机器学习：</strong>计算学习理论；</li><li><strong>高级人机交互技术：</strong>《XRgonomics Facilitating the Creation of Ergonomic 3D》；</li></ul><h2 id="5-下周计划"><a href="#5-下周计划" class="headerlink" title="5. 下周计划"></a>5. 下周计划</h2><ol><li>完成人机交互论文展示PPT。</li><li>完成网络安全论文展示PPT。</li><li>阅读BEIT和MAE的论文。</li></ol><h3 id="参考"><a href="#参考" class="headerlink" title="参考:"></a>参考:</h3><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://nv-tlabs.github.io/DP-Sinkhorn/">https://nv-tlabs.github.io/DP-Sinkhorn/</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://arxiv.org/pdf/2108.08810.pdf">https://arxiv.org/pdf/2108.08810.pdf</a><a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Weekly Note</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ViT</tag>
      
      <tag>Differential privacy</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Weekly-211114</title>
    <link href="/2021/11/14/1-Weekly-211114/"/>
    <url>/2021/11/14/1-Weekly-211114/</url>
    
    <content type="html"><![CDATA[<h3 id="本周学习汇报"><a href="#本周学习汇报" class="headerlink" title="本周学习汇报"></a>本周学习汇报</h3><span id="more"></span><blockquote><div>            <input type="checkbox" disabled checked="checked">DatasetGAN: Efficient Labeled Data Factory with Minimal Human Effort(CVPR2021oral)<sup id=fnref:1 class=footnote-ref><a href=#fn:1 rel=footnote><span class=hint--top hint--rounded aria-label=https://nv-tlabs.github.io/datasetGAN/>[1]</span></a></sup>          </div><div>            <input type="checkbox" disabled checked="checked">EditGAN: High-Precision Semantic Image Editing(NeurlPS2021)<sup id=fnref:2 class=footnote-ref><a href=#fn:2 rel=footnote><span class=hint--top hint--rounded aria-label=https://nv-tlabs.github.io/editGAN/>[2]</span></a></sup>          </div> <div>            <input type="checkbox" disabled checked="checked">分支限界法与堆排序的大作业          </div></blockquote><h2 id="1-DatasetGAN-标注图像的生成"><a href="#1-DatasetGAN-标注图像的生成" class="headerlink" title="1. DatasetGAN:标注图像的生成"></a>1. DatasetGAN:标注图像的生成</h2><h3 id="1-1-Motivation"><a href="#1-1-Motivation" class="headerlink" title="1.1 Motivation"></a>1.1 Motivation</h3><ul><li>现如今深度学习网络模型需要大量的数据标注；</li><li>对语义分割任务而言，数据标注耗时严重；</li><li><strong>目的：通过少数标记的样本生成大量高质量的标注数据集。</strong></li></ul><h3 id="1-2-Related-Work"><a href="#1-2-Related-Work" class="headerlink" title="1.2 Related Work"></a>1.2 Related Work</h3><ul><li><strong>标注数据生成模型：</strong>使用大量标注数据对模型进行训练，如StyleGAN。而本文则利用少量人工标注样本去生成高质量语义标签。</li><li><strong>半监督学习：</strong>可用于对数据集的扩充。常用的adversarial loss只学习生成图像的语义而没有探索图像本身的语义。依赖卷积和残差结构的decoder，将中间层的特征映射到一个segmentation map，而本文则直接对解缠的特征向量(styleGAN)进行直接解释，逐像素进行操作。</li><li><strong>对比学习：</strong>使用对抗损失，测量样本对之间的相似度，学习图像的特征表示空间。本文用GAN特征图的语义信息，代替对比损失。</li></ul><h3 id="1-3-Method"><a href="#1-3-Method" class="headerlink" title="1.3 Method"></a>1.3 Method</h3><p><img src="/images/1-weeklynote/211114/1.png" alt=""></p><center><font color=blue >图1-1 DatasetGAN整体流程 </font></center><p><img src="/images/1-weeklynote/211114/2.png" alt=""></p><center><font color=blue >图1-2 DatasetGAN网络结构 </font></center><ul><li><strong>方法重点：</strong>获得高维latent space的语义信息；</li><li><strong>应用：</strong>语义分割、关键点预测；</li></ul><h3 id="1-4-Result"><a href="#1-4-Result" class="headerlink" title="1.4 Result"></a>1.4 Result</h3><p><img src="/images/1-weeklynote/211114/3.png" alt=""></p><center><font color=blue >图1-3 DatasetGAN实验结果 </font></center><h2 id="2-EditGAN-高精度语义图像编辑"><a href="#2-EditGAN-高精度语义图像编辑" class="headerlink" title="2. EditGAN:高精度语义图像编辑"></a>2. EditGAN:高精度语义图像编辑</h2><h3 id="2-1-Motivation"><a href="#2-1-Motivation" class="headerlink" title="2.1 Motivation"></a>2.1 Motivation</h3><ul><li><strong>其他研究：</strong>大多数基于GAN的图像编辑方法通常需要用于训练的语义分割标注的大规模数据集，并且仅提供高级控制，或仅在不同图像之间进行插值。</li><li><strong>本文目的：</strong>提供高质量、高精度的语义图像编辑，允许用户修改一些细节来对图像进行编辑。</li></ul><h3 id="2-2-Related-Work"><a href="#2-2-Related-Work" class="headerlink" title="2.2 Related Work"></a>2.2 Related Work</h3><ul><li><strong>图像编辑：</strong>基于GAN的图像编辑方法*6，详见论文页3。</li><li><strong>GAN与潜空间图像映射：</strong>DatasetGAN与SemanticGAN的半监督学习。</li></ul><h3 id="2-3-Method"><a href="#2-3-Method" class="headerlink" title="2.3 Method"></a>2.3 Method</h3><p><img src="/images/1-weeklynote/211114/5.png" alt=""></p><center><font color=blue >图2-1 EditGAN整体流程 </font></center><p><img src="/images/1-weeklynote/211114/6.png" alt=""></p><center><font color=blue >图2-2 EditGAN的latent space编辑 </font></center><ul><li><strong>Background：</strong>StyleGAN2；</li><li><strong>方法重点：</strong>侧重于encoder与优化器的设计，使图像在GAN的潜空间内更好地得到编辑；</li></ul><h3 id="2-4-Result"><a href="#2-4-Result" class="headerlink" title="2.4 Result"></a>2.4 Result</h3><p><img src="/images/1-weeklynote/211114/4.png" alt=""></p><center><font color=blue >图2-3 DatasetGAN实验结果 </font></center><h2 id="3-其他"><a href="#3-其他" class="headerlink" title="3. 其他"></a>3. 其他</h2><ul><li><strong>工程数学：</strong>矩阵的满秩分解、正交三角分解、奇异值分解、极分解、谱分解；</li><li><strong>网络安全：</strong>Hash函数的应用，消息认证与数字签名；</li><li><strong>高级算法分析：</strong>近似算法与多项式近似方案，0/1背包问题的大作业；</li><li><strong>高级机器学习：</strong>特征选择与稀疏学习；</li><li><strong>高级人机交互技术：</strong> Increasing Electrical Muscle Stimulation’s Dexterity by means of Back of the Hand Actuation；</li><li><strong>科技论文写作(蹭课)：</strong>IMRAD、论文类型、写作技巧、写作工具等；</li></ul><h2 id="4-下周计划"><a href="#4-下周计划" class="headerlink" title="4. 下周计划"></a>4. 下周计划</h2><ol><li>阅读NeurlPS的几篇论文。</li><li>准备人机交互与网络安全的论文的课堂展示。</li><li>继续阅读相关论文及代码。</li></ol><h3 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h3><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://nv-tlabs.github.io/datasetGAN/">https://nv-tlabs.github.io/datasetGAN/</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://nv-tlabs.github.io/editGAN/">https://nv-tlabs.github.io/editGAN/</a><a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Weekly Note</category>
      
    </categories>
    
    
    <tags>
      
      <tag>GAN</tag>
      
      <tag>semantic task</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>hexo博客的图表及ppt嵌入</title>
    <link href="/2021/11/08/0-hexo%E5%8D%9A%E5%AE%A2%E7%9A%84%E5%9B%BE%E8%A1%A8%E5%8F%8Appt%E5%B5%8C%E5%85%A5/"/>
    <url>/2021/11/08/0-hexo%E5%8D%9A%E5%AE%A2%E7%9A%84%E5%9B%BE%E8%A1%A8%E5%8F%8Appt%E5%B5%8C%E5%85%A5/</url>
    
    <content type="html"><![CDATA[<h1 id="网页嵌套网页"><a href="#网页嵌套网页" class="headerlink" title="网页嵌套网页"></a>网页嵌套网页</h1><figure class="highlight html"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">iframe</span> <span class="hljs-attr">src</span>=<span class="hljs-string">&quot;https://ellenwong.github.io/2021/09/20/0-my-first-blog/&quot;</span> <span class="hljs-attr">width</span>=<span class="hljs-string">&quot;100%&quot;</span> <span class="hljs-attr">height</span>=<span class="hljs-string">&quot;600&quot;</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;topFrame&quot;</span> <span class="hljs-attr">scrolling</span>=<span class="hljs-string">&quot;yes&quot;</span> <span class="hljs-attr">noresize</span>=<span class="hljs-string">&quot;noresize&quot;</span> <span class="hljs-attr">frameborder</span>=<span class="hljs-string">&quot;0&quot;</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;topFrame&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">iframe</span>&gt;</span><br></code></pre></div></td></tr></table></figure><p><iframe src="https://ellenwong.github.io/2021/09/20/0-my-first-blog/" width="100%" height="600" name="topFrame" scrolling="yes" noresize="noresize" frameborder="0" id="topFrame"></iframe><br>但是有些网页会请求失败，比如github拒绝了我的请求。</p><p><a href="https://ellenwong.github.io/html/index.html">https://ellenwong.github.io/html/index.html</a></p><h1 id="html"><a href="#html" class="headerlink" title="html"></a>html</h1><p>这个是关于echart图表的。还挺高级。<br>似乎可以直接写在html文件中，还可以访问。</p><p><a class="btn" href="https://ellenwong.github.io/html/index.html" title="按钮">这是个按钮</a></p><p>前端还真没我想的那么简单啊，不过蛮好玩的。</p><p><a class="btn" href="https://hexo.fluid-dev.com/posts/hexo-echarts/" title="echart">别人的图表，很炫。</a></p>]]></content>
    
    
    <categories>
      
      <category>Hello World</category>
      
    </categories>
    
    
    <tags>
      
      <tag>tools</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Weekly-211107</title>
    <link href="/2021/11/07/1-Weekly-211107/"/>
    <url>/2021/11/07/1-Weekly-211107/</url>
    
    <content type="html"><![CDATA[<h3 id="本周学习汇报"><a href="#本周学习汇报" class="headerlink" title="本周学习汇报"></a>本周学习汇报</h3><span id="more"></span><blockquote><div>            <input type="checkbox" disabled checked="checked">Projected GANs Converge Faster(NeurlPS2021)<sup id=fnref:1 class=footnote-ref><a href=#fn:1 rel=footnote><span class=hint--top hint--rounded aria-label=https://sites.google.com/view/projected-gan/>[1]</span></a></sup>          </div><div>            <input type="checkbox" disabled checked="checked">Adaptive Convolutions for Structure-Aware Style Transfer(CVPR2021)<sup id=fnref:2 class=footnote-ref><a href=#fn:2 rel=footnote><span class=hint--top hint--rounded aria-label=https://openaccess.thecvf.com/content/CVPR2021/papers/Chandran_Adaptive_Convolutions_for_Structure-Aware_Style_Transfer_CVPR_2021_paper.pdf>[2]</span></a></sup>          </div> <div>            <input type="checkbox" disabled checked="checked">分支限界法与堆排序的原理与实现          </div></blockquote><h2 id="1-Projected-GAN的加速收敛"><a href="#1-Projected-GAN的加速收敛" class="headerlink" title="1. Projected GAN的加速收敛"></a>1. Projected GAN的加速收敛</h2><h3 id="1-1-Motivation"><a href="#1-1-Motivation" class="headerlink" title="1.1 Motivation"></a>1.1 Motivation</h3><p>&emsp;&emsp;此论文针对任务为图像翻译(I2I)，由于判别器不能充分利用来自预训练模型更深层的特征，作者提出Projected GAN的方式，以提升图像质量、采样效率、收敛速度等。</p><h3 id="1-2-Related-Work"><a href="#1-2-Related-Work" class="headerlink" title="1.2 Related Work"></a>1.2 Related Work</h3><ul><li>GAN的预训练模型：StyleGAN；</li><li>判别器结构设计；</li></ul><h3 id="1-3-Method"><a href="#1-3-Method" class="headerlink" title="1.3 Method"></a>1.3 Method</h3><ul><li>多规模判别器设计</li><li>随机投影设计<br><img src="/images/1-weeklynote/211107/5.png" alt=""></li><li>预训练特征网络</li></ul><h3 id="1-4-Result"><a href="#1-4-Result" class="headerlink" title="1.4 Result"></a>1.4 Result</h3><p><img src="/images/1-weeklynote/211107/4.png" alt=""></p><h2 id="2-结构感知风格迁移的自适应卷积"><a href="#2-结构感知风格迁移的自适应卷积" class="headerlink" title="2. 结构感知风格迁移的自适应卷积"></a>2. 结构感知风格迁移的自适应卷积</h2><p class="note note-warning">风格迁移 + AdaConv</p> <h3 id="2-1-Motivation"><a href="#2-1-Motivation" class="headerlink" title="2.1 Motivation"></a>2.1 Motivation</h3><ul><li><strong>自适应实例规范化(AdaIN)：</strong>是一种全局操作，所以转变过程中图像的局部几何结构容易被忽略。</li><li><strong>自适应卷积(AdaConv)：</strong>允许同时转换统计与结构特征。可以应用在基于风格的图像生成，以及其他AdaIN被采用的任务中。</li></ul><h3 id="2-2-Related-Work"><a href="#2-2-Related-Work" class="headerlink" title="2.2 Related Work"></a>2.2 Related Work</h3><ul><li><strong>风格迁移：</strong>BN、IN、CIN、DIN、AdaIN；</li><li><strong>生成模型的调制层：</strong>SPADE，StyleGAN；</li><li><strong>内核预测；</strong></li></ul><h3 id="2-3-Method"><a href="#2-3-Method" class="headerlink" title="2.3 Method"></a>2.3 Method</h3><ul><li><strong>自适应实例归一化</strong></li></ul><script type="math/tex; mode=display">\operatorname{AdaIN}(x ; a, b)=a\left(\frac{x-\mu_{x}}{\sigma_{x}}\right)+b</script><ul><li><strong>自适应卷积</strong></li></ul><p>在AdaIN的基础上引入条件滤波</p><script type="math/tex; mode=display">\begin{aligned}\operatorname{AdaConv}_{\mathrm{dw}}(x ; \mathbf{f}, b) &=\sum_{x_{i} \in \mathcal{N}(x)} f_{i}\left(\frac{x_{i}-\mu_{x}}{\sigma_{x}}\right)+b \\&=\sum_{x_{i} \in \mathcal{N}(x)} \operatorname{AdaIN}\left(x ; f_{i}, b\right)\end{aligned}</script><p>引入一个可分离逐点卷积tensor</p><script type="math/tex; mode=display">\operatorname{AdaConv}(x ; \mathbf{p}, \mathbf{f}, \mathbf{b})=\sum_{c} p_{c} \operatorname{AdaConv}_{\mathrm{dw}}\left(x_{c} ; \mathbf{f}_{c}, b_{c}\right)</script><ul><li><strong>模型流程图</strong></li></ul><p><img src="/images/1-weeklynote/211107/2.png" alt=""><br>其中编码器结构为：<br><img src="/images/1-weeklynote/211107/3.png" alt=""></p><h3 id="2-4-Result"><a href="#2-4-Result" class="headerlink" title="2.4 Result"></a>2.4 Result</h3><p><img src="/images/1-weeklynote/211107/1.png" alt=""></p><h2 id="3-其他"><a href="#3-其他" class="headerlink" title="3. 其他"></a>3. 其他</h2><ul><li><strong>工程数学：</strong>对角矩阵、酉变化、正规矩阵、二次型等；</li><li><strong>网络安全：</strong>Hash函数；</li><li><strong>高级算法分析：</strong>NP完全问题的一些例子；</li><li><strong>高级机器学习：</strong>降维与度量学习；</li><li><strong>高级人机交互技术：</strong> Towards an Understanding of Situated AR Visualization for Basketball Free-Throw Training + Text Input and AI-infused Human-Computer Interaction 的报告；</li></ul><h2 id="4-下周计划"><a href="#4-下周计划" class="headerlink" title="4. 下周计划"></a>4. 下周计划</h2><ol><li>整理这段时期的关于I2IT的论文。</li><li>完成分支限界法解决0/1背包问题的大作业。</li><li>继续阅读相关论文及代码。</li></ol><h3 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h3><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://sites.google.com/view/projected-gan/">https://sites.google.com/view/projected-gan/</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Chandran_Adaptive_Convolutions_for_Structure-Aware_Style_Transfer_CVPR_2021_paper.pdf">https://openaccess.thecvf.com/content/CVPR2021/papers/Chandran_Adaptive_Convolutions_for_Structure-Aware_Style_Transfer_CVPR_2021_paper.pdf</a><a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Weekly Note</category>
      
    </categories>
    
    
    <tags>
      
      <tag>GAN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Weekly-211031</title>
    <link href="/2021/10/30/1-Weekly-211031/"/>
    <url>/2021/10/30/1-Weekly-211031/</url>
    
    <content type="html"><![CDATA[<h3 id="本周学习汇报"><a href="#本周学习汇报" class="headerlink" title="本周学习汇报"></a>本周学习汇报</h3><span id="more"></span><blockquote><div>            <input type="checkbox" disabled checked="checked">《Bridging the Gap between Label- and Reference-based Synthesis in Multi-attribute Image-to-Image Translation》（ICCV2021）<sup id=fnref:1 class=footnote-ref><a href=#fn:1 rel=footnote><span class=hint--top hint--rounded aria-label=https://arxiv.org/abs/2110.05055>[1]</span></a></sup>          </div><div>            <input type="checkbox" disabled checked="checked">《LSC-GAN: Latent Style Code Modeling for Continuous Image-to-image Translation》<sup id=fnref:2 class=footnote-ref><a href=#fn:2 rel=footnote><span class=hint--top hint--rounded aria-label=https://arxiv.org/abs/2110.05052>[2]</span></a></sup>          </div><div>            <input type="checkbox" disabled checked="checked">Code of BridgeGAN<sup id=fnref:3 class=footnote-ref><a href=#fn:3 rel=footnote><span class=hint--top hint--rounded aria-label=https://github.com/huangqiusheng/BridgeGAN>[3]</span></a></sup>          </div></blockquote><h2 id="1-关于label-based与reference-based图像翻译-I2IT-问题"><a href="#1-关于label-based与reference-based图像翻译-I2IT-问题" class="headerlink" title="1. 关于label-based与reference-based图像翻译(I2IT)问题"></a>1. 关于label-based与reference-based图像翻译(I2IT)问题</h2><p>&emsp;&emsp;当前主要有两种I2I方式，一是基于标签的图像生成，即输入图像+label（如微笑、发色等）到模型中，以得到根据label输出的生成图像；另一种是基于参考图像的生成，即输入图像+reference图像到模型中，以得到输入图像与参考图像混合样式的生成图像。文章考虑综合两种图像生成的方式，使其互相引导，以得到效果的提升。</p><p><img src="/images/3-dailynote/211028/3.png" alt=""></p><p><a href="https://ellenwong.github.io/2021/10/28/3-Daily-211028/">详细内容</a></p><h2 id="2-关于连续域的图像生成问题——LSC-GAN"><a href="#2-关于连续域的图像生成问题——LSC-GAN" class="headerlink" title="2. 关于连续域的图像生成问题——LSC-GAN"></a>2. 关于连续域的图像生成问题——LSC-GAN</h2><p>&emsp;&emsp;当前I2I工作主要在离散域上进行操作（如CycleGAN、StarGAN），而缺少对连续域的研究，不同域之间的相关性常常被忽略，生成图像的多样性不佳，而实际上，标签映射的特定属性往往不是分散的，它们往往被描述为一个连续的数值。本文旨在建立一个模型，实现图像的连续域之间的转换。</p><p><img src="/images/3-dailynote/211027/3-2.png" alt=""></p><p><a href="https://ellenwong.github.io/2021/10/27/3-Daily-211027/">详细内容</a></p><h2 id="3-其他"><a href="#3-其他" class="headerlink" title="3. 其他"></a>3. 其他</h2><ul><li><strong>工程数学：</strong>Hermite矩阵、标准正交基、Schmidt方法、幂等矩阵、正交投影等；</li><li><strong>网络安全：</strong>公钥加密；</li><li><strong>高级算法分析：</strong>NP完全问题；</li><li><strong>高级机器学习：</strong>聚类；</li><li><strong>高级人机交互技术：</strong> Remote and Collaborative Virtual Reality Experiments via<br>Social VR Platforms</li></ul><h2 id="4-下周计划"><a href="#4-下周计划" class="headerlink" title="4. 下周计划"></a>4. 下周计划</h2><ol><li>了解并整理关于I2IT的目前相关研究进度。</li><li>继续学习相关论文。</li><li>完成算法课课程作业。</li></ol><h3 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h3><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://arxiv.org/abs/2110.05055">https://arxiv.org/abs/2110.05055</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://arxiv.org/abs/2110.05052">https://arxiv.org/abs/2110.05052</a><a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span><a href="https://github.com/huangqiusheng/BridgeGAN">https://github.com/huangqiusheng/BridgeGAN</a><a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Weekly Note</category>
      
    </categories>
    
    
    <tags>
      
      <tag>GAN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Daily-211028</title>
    <link href="/2021/10/28/3-Daily-211028/"/>
    <url>/2021/10/28/3-Daily-211028/</url>
    
    <content type="html"><![CDATA[<ul><li>2021年10月28日-星期四-晴<span id="more"></span></li><li>主要收获：<blockquote><ul><li>阅读论文《Bridging the Gap between Label- and Reference-based Synthesis in Multi-attribute Image-to-Image Translation》（ICCV2021）</li><li>上午与晚上的课：英语、机器学习（聚类）</li></ul></blockquote></li></ul><hr><h1 id="关于label-based与reference-based图像翻译-I2IT-问题"><a href="#关于label-based与reference-based图像翻译-I2IT-问题" class="headerlink" title="关于label-based与reference-based图像翻译(I2IT)问题"></a>关于label-based与reference-based图像翻译(I2IT)问题</h1><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>&emsp;&emsp;当前主要有两种I2I方式，一是基于标签的图像生成，即输入图像+label（如微笑、发色等）到模型中，以得到根据label输出的生成图像；另一种是基于参考图像的生成，即输入图像+reference图像到模型中，以得到输入图像与参考图像混合样式的生成图像。</p><ul><li>label-based：<br><img src="/images/3-dailynote/211028/1.png" alt=""></li><li>reference-based：<br><img src="/images/3-dailynote/211028/2.png" alt=""></li></ul><p>文章考虑综合两种图像生成的方式，使其互相引导，以得到效果的提升。</p><h2 id="Related-Works"><a href="#Related-Works" class="headerlink" title="Related Works"></a>Related Works</h2><ul><li>单一属性的I2IT问题：UNIT、MUNIT、DRIT等；</li><li>多属性的I2IT问题：StarGAN、AttGAN等。</li></ul><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="/images/3-dailynote/211028/3.png" alt=""></p><ul><li>目标及目标函数：使LEM多样性更强，使REM准确度更高。</li></ul><script type="math/tex; mode=display">L_{s t y}=\left\|S_{\text {rand }}-S_{r e f}\right\|_{1}</script><ul><li>对抗损失：</li></ul><script type="math/tex; mode=display">L_{a d v}=\mathbb{E}_{X_{s} \sim p_{d}}\left[\mathrm{D}\left(X_{s}\right)\right]-\mathbb{E}_{X_{g} \in\left\{X_{g}^{l}, X_{g}^{r}, X_{g}^{i}\right\}}\left[\mathrm{D}\left(X_{g}\right)\right]</script><ul><li>属性分类损失：</li></ul><script type="math/tex; mode=display">L_{c l s}=-\frac{1}{N_{C}} \sum_{i=0}^{N_{C}}\left[y_{i} \log \mathrm{C}_{i}(X)+\left(1-y_{i}\right) \log \left(1-\mathrm{C}_{i}(X)\right)\right]</script><ul><li>重构损失：</li></ul><script type="math/tex; mode=display">L_{r e c}=\left\|X_{s}-\mathrm{G}\left(X_{s}, \operatorname{LEM} \mid \operatorname{REM}\left(X_{s}, R, 0\right)\right)\right\|_{1}</script><ul><li>循环损失：</li></ul><script type="math/tex; mode=display">L_{c y c}=\left\|S_{r a n d}-\tilde{S}_{r a n d}\right\|_{1}+\left\|S_{r e f}-\tilde{S}_{r e f}\right\|_{1}</script><ul><li>模式寻找目标：</li></ul><script type="math/tex; mode=display">L_{m s}=\frac{1}{\left\|\mathrm{G}\left(X_{s}, S_{\text {rand }}\right)-\mathrm{G}\left(X_{s}, S_{\text {rand }}^{\prime}\right)\right\|_{1}}\left\|R-R^{\prime}\right\|_{1}</script><ul><li><p>属性约束：</p><script type="math/tex; mode=display">L_{a k}=\left\|\mathrm{E}\left(X_{s}, \operatorname{att}_{a k}^{Y_{s} \downarrow}\right)-\mathrm{E}\left(\mathrm{G}\left(X_{s}, R, \operatorname{att}_{d i f f}^{Y_{s} \rightarrow Y_{t}}\right), \operatorname{att}_{a k}^{Y_{s} \downarrow}\right)\right\|_{1}</script></li><li><p>总体目标：</p><script type="math/tex; mode=display">\begin{array}{l}L_{\mathrm{G}}=L_{a d v}+\lambda_{c l s} L_{c l s}+\lambda_{r e c} L_{r e c}+\lambda_{s t y} L_{s t y}+\lambda_{m s} L_{m s} \\+\lambda_{a k} L_{a k} \\L_{\mathrm{ME}}=L_{\mathrm{G}}+\lambda_{c y c} L_{c y c} \\L_{\mathrm{DC}}=-L_{a d v}+\lambda_{c l s} L_{c l s} \quad L_{r}=\lambda_{s t y} L_{s t y}\end{array}</script></li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><img src="/images/3-dailynote/211028/4.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>Daily Note</category>
      
    </categories>
    
    
    <tags>
      
      <tag>GAN</tag>
      
      <tag>I2I</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Daily-211027</title>
    <link href="/2021/10/27/3-Daily-211027/"/>
    <url>/2021/10/27/3-Daily-211027/</url>
    
    <content type="html"><![CDATA[<ul><li>2021年10月27日-星期三-晴<span id="more"></span></li><li>主要收获：<blockquote><ul><li>阅读论文《LSC-GAN: Latent Style Code Modeling for Continuous Image-to-image Translation》</li><li>一堆的课：英语、网络安全、算法分析…… </li></ul></blockquote></li></ul><hr><h1 id="关于连续域的图像生成问题——LSC-GAN"><a href="#关于连续域的图像生成问题——LSC-GAN" class="headerlink" title="关于连续域的图像生成问题——LSC-GAN"></a>关于连续域的图像生成问题——LSC-GAN</h1><p>（Image-to-image问题）</p><h2 id="1-Motivation"><a href="#1-Motivation" class="headerlink" title="1.Motivation"></a>1.Motivation</h2><p>&emsp;&emsp;当前I2I工作主要在离散域上进行操作（如CycleGAN、StarGAN），而缺少对连续域的研究，不同域之间的相关性常常被忽略，生成图像的多样性不佳，而实际上，标签映射的特定属性往往不是分散的，它们往往被描述为一个连续的数值。本文旨在建立一个模型，实现图像的连续域之间的转换。</p><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2.Related Work"></a>2.Related Work</h2><ul><li><strong>两个域之间的转换：</strong>Pix2Pix、CycleGAN、DualGAN、DiscoGAN等；</li><li><strong>多个域之间的转换：</strong>StarGAN、AttGAN、ST-GAN、SMIT、ELEGANT、HomoGAN等；</li><li><strong>连续域I2I转换：</strong>LIFE、SAM、DLOW等；</li></ul><h2 id="3-Method"><a href="#3-Method" class="headerlink" title="3.Method"></a>3.Method</h2><h3 id="3-1-baseline"><a href="#3-1-baseline" class="headerlink" title="3.1 baseline"></a>3.1 baseline</h3><p><strong>StarGANv2的基础上进行</strong></p><ul><li><strong>生成器backbone：</strong>自编码器，每层的特征被样式向量影响。</li><li><strong>针对label-based方式：</strong><ul><li>映射网络F：直接映射域特征到样式代码，应用多分支结构。</li><li>分支的输出会被label影响。</li></ul></li><li><strong>针对reference-based方式：</strong><ul><li>直接编码参考图像到样式向量中，CNN采用多分支结构。</li><li>应用AdaIN的方式完成样式向量到G的映射。</li></ul></li></ul><h3 id="3-2-整体流程"><a href="#3-2-整体流程" class="headerlink" title="3.2 整体流程"></a>3.2 整体流程</h3><p><img src="/images/3-dailynote/211027/3-2.png" alt=""></p><p><center><font color=blue >图3-1 LSC-GAN流程 </font></center><br>（我的一些理解：）</p><ul><li><strong>图a：</strong>表示两种图像生成的方式，label-based与reference-based。其中s代表样式代码(style code)，与域的风格有关。生成器G则根据s与输入图像进行训练，style code会有参数共享。</li><li><strong>图b：</strong>yt表示一个连续域的一部分，yi与yj不重叠，yt通过一个多分支网络结构F学习到域中心的特征，也就是ct，代表yt这个域区间内最普遍的特征，ct与R输入M中，即学习label-based图像生成对应的样式sl；ct与reference image输入E中，即学习reference-based图像生成对应样式sr。</li><li><strong>图c：</strong>Y与O代表两个模块，分别学习相反过程的域的特征的转换，输入和输出均为code，也就是图b由F学到的ct。</li><li><strong>生成器：</strong>就是一个Auto-encoder，用AdaIN的方式将style code输入到G中。</li><li><strong>判别器：</strong>判断图像来源于哪一个域，区分图像是否真实。</li></ul><p>总的来说，文章的创新点在于连续域的提出，并且大部分工作在于域的特征的一些提取与处理，以更好地得到图像的多样性。</p><h3 id="3-3-损失函数"><a href="#3-3-损失函数" class="headerlink" title="3.3 损失函数"></a>3.3 损失函数</h3><p>首先定义：</p><script type="math/tex; mode=display">\overrightarrow{c}_{t}=\mathrm{O}^{y_{t}-y_{i}}\left(c_{i}\right) \quad \overleftarrow{c}_{t}=\mathrm{Y}^{y_{i}-y_{t}}\left(c_{i}\right)</script><ul><li><p>连续域一致性：</p><script type="math/tex; mode=display">L_{c d c}=\left\{\begin{array}{ll}\left|c_{t}-\vec{c}_{t}\right| & y_{t}-y_{i}>0 \\\left|c_{t}-\overleftarrow{c}_{t}\right| & y_{t}-y_{i}<0 \\0 & y_{t}-y_{i}=0\end{array}\right.</script></li><li><p>三元组损失：</p><script type="math/tex; mode=display">L_{t a m}=\left\{\begin{array}{ll}\max _{j}\left(d\left(\vec{c}_{t}, c_{t}\right)-d\left(\vec{c}_{t}, c_{j}\right)+m, 0\right) & y_{t}-y_{i}>0 \\\max _{j}\left(d\left(\overleftarrow{c}_{t}, c_{t}\right)-d\left(\overleftarrow{c}_{t}, c_{j}\right)+m, 0\right) & y_{t}-y_{i}<0 \\0 & y_{t}-y_{i}=0\end{array}\right.</script></li><li><p>Cycle连续一致损失：</p><script type="math/tex; mode=display">L_{c c c}=\left\{\begin{array}{ll}\left|c_{i}-\mathrm{Y}^{y_{t}-y_{i}}\left(\vec{c}_{t}\right)\right| & y_{t}-y_{i}>0 \\\left|c_{i}-\mathrm{O}^{y_{i}-y_{t}}\left(\overleftarrow{c}_{t}\right)\right| & y_{t}-y_{i}<0 \\0 & y_{t}-y_{i}=0\end{array}\right.</script></li><li><p>总体优化函数：</p><script type="math/tex; mode=display">\begin{array}{l}L_{\mathrm{EDG}}=L_{S t a r} \\L_{\mathrm{FMYO}}=L_{S t a r}+\lambda_{c d c} L_{c d c}+\lambda_{t a m} L_{t a m}+\lambda_{c c c} L_{c c c}\end{array}</script></li></ul><h3 id="3-4-实验结果"><a href="#3-4-实验结果" class="headerlink" title="3.4 实验结果"></a>3.4 实验结果</h3><p><img src="/images/3-dailynote/211027/3-3.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>Daily Note</category>
      
    </categories>
    
    
    <tags>
      
      <tag>GAN</tag>
      
      <tag>I2I</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>smallcutepig</title>
    <link href="/2021/10/22/4-smallcutepig/"/>
    <url>/2021/10/22/4-smallcutepig/</url>
    
    <content type="html"><![CDATA[<h3 id="Today-is-a-happy-day"><a href="#Today-is-a-happy-day" class="headerlink" title="Today is a happy day."></a>Today is a happy day.</h3><span id="more"></span><p>We study together,althought a little,still be happy.</p><blockquote><p>There are three things my love in the world,sun,moon and you.<br>Sun for day,moon for night,and you forever.</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>else</category>
      
    </categories>
    
    
    <tags>
      
      <tag>story</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Weekly-211024</title>
    <link href="/2021/10/22/1-Weekly-211024/"/>
    <url>/2021/10/22/1-Weekly-211024/</url>
    
    <content type="html"><![CDATA[<h3 id="本周学习汇报"><a href="#本周学习汇报" class="headerlink" title="本周学习汇报"></a>本周学习汇报</h3><span id="more"></span><blockquote><div>            <input type="checkbox" disabled checked="checked">《Alias-Free Generative Adversarial Networks》<sup id=fnref:1 class=footnote-ref><a href=#fn:1 rel=footnote><span class=hint--top hint--rounded aria-label=https://nvlabs-fi-cdn.nvidia.com/stylegan3/stylegan3-paper.pdf>[1]</span></a></sup>          </div><div>            <input type="checkbox" disabled checked="checked">Code of StyleGAN3<sup id=fnref:2 class=footnote-ref><a href=#fn:2 rel=footnote><span class=hint--top hint--rounded aria-label=https://github.com/NVlabs/stylegan3>[2]</span></a></sup>          </div></blockquote><h2 id="1-StyleGAN3（NeurIPS-2021）"><a href="#1-StyleGAN3（NeurIPS-2021）" class="headerlink" title="1. StyleGAN3（NeurIPS 2021）"></a>1. StyleGAN3（NeurIPS 2021）</h2><p><img src="/images/1-weeklynote/211024/alias_free_poster.gif" alt=""></p><h3 id="1-1-Motivation"><a href="#1-1-Motivation" class="headerlink" title="1.1 Motivation"></a>1.1 Motivation</h3><p>&emsp;&emsp;如上图所示，StyleGAN2在图像生成中，图像细节特征的平移与旋转效果很差，即纹理特征会很大程度地依赖于坐标。StyleGAN3的提出，为解决StyleGAN2图像坐标与特征粘连的问题，主要修改生成器的设计，得到一个更自然的转换层次结构，使纹理与坐标一起移动。视觉效果上，可以使生成物体在空间中更连贯地移动，使GAN更适合视频与动画的生成。</p><ul><li><strong>Alias：</strong>描述样本重建过程中，与原始样本不同的失真或伪影（如摩尔纹）。</li></ul><h3 id="1-2-Method"><a href="#1-2-Method" class="headerlink" title="1.2 Method"></a>1.2 Method</h3><h4 id="1-2-1-BaseLine——StyleGAN2"><a href="#1-2-1-BaseLine——StyleGAN2" class="headerlink" title="1.2.1 BaseLine——StyleGAN2"></a>1.2.1 BaseLine——StyleGAN2</h4><p><img src="/images/1-weeklynote/211024/1-2.png" alt=""></p><center><font color=blue >图1-1 StyleGAN2网络架构 </font></center><h4 id="1-2-2-Fourier-features"><a href="#1-2-2-Fourier-features" class="headerlink" title="1.2.2 Fourier features"></a>1.2.2 Fourier features</h4><p>&emsp;&emsp;为了从输入的潜代码中更好地提取平移与旋转的连续特征，将生成器的学习常数输入替换为与原始 4x4 分辨率匹配的固定傅立叶特征（圆形频带内的均匀采样频率 = 2），这一改变略微提高了FID的指标，更重要的贡献在于不必估算操作t，而可以直接计算图像不变性的指标。</p><h4 id="1-2-3-No-noise-inputs"><a href="#1-2-3-No-noise-inputs" class="headerlink" title="1.2.3 No noise inputs"></a>1.2.3 No noise inputs</h4><p>&emsp;&emsp;由于逐像素噪声输入独立于底层特征的任意变换，因此去除这些输入，并从下层粗糙特征中继承亚像素的位置信息。</p><h4 id="1-2-4-Simplified-generator"><a href="#1-2-4-Simplified-generator" class="headerlink" title="1.2.4 Simplified generator"></a>1.2.4 Simplified generator</h4><p>&emsp;&emsp;映射网络深度减少，禁用混合正则化和路径长度正则化。在每次卷积之前，与输出的跳过连接被特征图的归一化所取代。所有这些变化都使 FID 保持不变，并略微提高了不变性的测量指标。</p><h4 id="1-2-5-Boundaries-amp-upsampling"><a href="#1-2-5-Boundaries-amp-upsampling" class="headerlink" title="1.2.5 Boundaries &amp; upsampling"></a>1.2.5 Boundaries &amp; upsampling</h4><p>&emsp;&emsp;图像边界填充过程中，会导致图像绝对坐标泄漏到内部表示中（不利于平移旋转的不变性），因此留出10像素的间隔。在理论模型的引导下，采用临界采样的方式，用Kaiser窗口n=6（每个输入像素影响6个输出像素，每个输出像素受6个输入像素影响）的sinc滤波器替换原有的双线性上采样滤波器。</p><h4 id="1-2-6-Filtered-nonlinearities"><a href="#1-2-6-Filtered-nonlinearities" class="headerlink" title="1.2.6 Filtered nonlinearities"></a>1.2.6 Filtered nonlinearities</h4><p>&emsp;&emsp;在连续域中应用ReLU激活函数，会导致非常高的频率无法在特征中表达，因此将“上采样-&gt;LReLU-&gt;下采样”的方式应用在网络结构中，以近似解决此问题。</p><h4 id="1-2-7-Non-critical-sampling"><a href="#1-2-7-Non-critical-sampling" class="headerlink" title="1.2.7 Non-critical sampling"></a>1.2.7 Non-critical sampling</h4><p>&emsp;&emsp;除高分辨率层外，其他层使用较低的截止频率，截止频率与feature map的数量成反比，以补偿信号中减少的空间信息。</p><h4 id="1-2-8-Transformed-Fourier-features"><a href="#1-2-8-Transformed-Fourier-features" class="headerlink" title="1.2.8 Transformed Fourier features"></a>1.2.8 Transformed Fourier features</h4><p>&emsp;&emsp;在傅立叶特征之前添加了额外的仿射变换，允许生成器基于样式向量具有明确的旋转和平移参数。</p><h4 id="1-2-9-Flexible-layers-StyleGAN3-T"><a href="#1-2-9-Flexible-layers-StyleGAN3-T" class="headerlink" title="1.2.9 Flexible layers (StyleGAN3-T)"></a>1.2.9 Flexible layers (StyleGAN3-T)</h4><p>&emsp;&emsp;在14层的生成器中，采用不同的截止频率，以最大程度地抑制Alias。</p><h4 id="1-2-10-Rotation-equiv-StyleGAN3-R"><a href="#1-2-10-Rotation-equiv-StyleGAN3-R" class="headerlink" title="1.2.10 Rotation equiv. (StyleGAN3-R)"></a>1.2.10 Rotation equiv. (StyleGAN3-R)</h4><p>&emsp;&emsp;为实现旋转不变性，文章采用了两种措施，其一是用1x1卷积代替3x3卷积，此时只有上下采样分散像素间的信息；其二是采用一个径向对称jinc滤波器代替基于sinc的下采样滤波器。</p><p><img src="/images/1-weeklynote/211024/1-3.png" alt=""></p><center><font color=blue >图1-2 StyleGAN3 网络结构 </font></center><h3 id="1-3-Result"><a href="#1-3-Result" class="headerlink" title="1.3 Result"></a>1.3 Result</h3><p><img src="/images/1-weeklynote/211024/1-1.png" alt=""></p><center><font color=blue >图1-3 实验结果 </font></center><h2 id="2-StyleGAN3实现"><a href="#2-StyleGAN3实现" class="headerlink" title="2. StyleGAN3实现"></a>2. StyleGAN3实现</h2><ul><li><strong>生成器结构</strong><sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://github.com/NVlabs/stylegan3">[2]</span></a></sup><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Generator</span>(<span class="hljs-params">torch.nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,</span></span><br><span class="hljs-params"><span class="hljs-function">        z_dim,                      <span class="hljs-comment"># Input latent (Z) dimensionality.</span></span></span><br><span class="hljs-params"><span class="hljs-function">        c_dim,                      <span class="hljs-comment"># Conditioning label (C) dimensionality.</span></span></span><br><span class="hljs-params"><span class="hljs-function">        w_dim,                      <span class="hljs-comment"># Intermediate latent (W) dimensionality.</span></span></span><br><span class="hljs-params"><span class="hljs-function">        img_resolution,             <span class="hljs-comment"># Output resolution.</span></span></span><br><span class="hljs-params"><span class="hljs-function">        img_channels,               <span class="hljs-comment"># Number of output color channels.</span></span></span><br><span class="hljs-params"><span class="hljs-function">        mapping_kwargs      = &#123;&#125;,   <span class="hljs-comment"># Arguments for MappingNetwork.</span></span></span><br><span class="hljs-params"><span class="hljs-function">        **synthesis_kwargs,         <span class="hljs-comment"># Arguments for SynthesisNetwork.</span></span></span><br><span class="hljs-params"><span class="hljs-function">    </span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.z_dim = z_dim<br>        self.c_dim = c_dim<br>        self.w_dim = w_dim<br>        self.img_resolution = img_resolution<br>        self.img_channels = img_channels<br>        self.synthesis = SynthesisNetwork(w_dim=w_dim, img_resolution=img_resolution, img_channels=img_channels, **synthesis_kwargs)<br>        self.num_ws = self.synthesis.num_ws<br>        self.mapping = MappingNetwork(z_dim=z_dim, c_dim=c_dim, w_dim=w_dim, num_ws=self.num_ws, **mapping_kwargs)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, z, c, truncation_psi=<span class="hljs-number">1</span>, truncation_cutoff=<span class="hljs-literal">None</span>, update_emas=<span class="hljs-literal">False</span>, **synthesis_kwargs</span>):</span><br>        ws = self.mapping(z, c, truncation_psi=truncation_psi, truncation_cutoff=truncation_cutoff, update_emas=update_emas)<br>        img = self.synthesis(ws, update_emas=update_emas, **synthesis_kwargs)<br>        <span class="hljs-keyword">return</span> img<br></code></pre></div></td></tr></table></figure></li><li><strong>映射网络</strong><sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://github.com/NVlabs/stylegan3">[2]</span></a></sup><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MappingNetwork</span>(<span class="hljs-params">torch.nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,</span></span><br><span class="hljs-params"><span class="hljs-function">        z_dim,                      <span class="hljs-comment"># Input latent (Z) dimensionality.</span></span></span><br><span class="hljs-params"><span class="hljs-function">        c_dim,                      <span class="hljs-comment"># Conditioning label (C) dimensionality, 0 = no labels.</span></span></span><br><span class="hljs-params"><span class="hljs-function">        w_dim,                      <span class="hljs-comment"># Intermediate latent (W) dimensionality.</span></span></span><br><span class="hljs-params"><span class="hljs-function">        num_ws,                     <span class="hljs-comment"># Number of intermediate latents to output.</span></span></span><br><span class="hljs-params"><span class="hljs-function">        num_layers      = <span class="hljs-number">2</span>,        <span class="hljs-comment"># Number of mapping layers.</span></span></span><br><span class="hljs-params"><span class="hljs-function">        lr_multiplier   = <span class="hljs-number">0.01</span>,     <span class="hljs-comment"># Learning rate multiplier for the mapping layers.</span></span></span><br><span class="hljs-params"><span class="hljs-function">        w_avg_beta      = <span class="hljs-number">0.998</span>,    <span class="hljs-comment"># Decay for tracking the moving average of W during training.</span></span></span><br><span class="hljs-params"><span class="hljs-function">    </span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.z_dim = z_dim<br>        self.c_dim = c_dim<br>        self.w_dim = w_dim<br>        self.num_ws = num_ws<br>        self.num_layers = num_layers<br>        self.w_avg_beta = w_avg_beta<br><br>        <span class="hljs-comment"># Construct layers.</span><br>        self.embed = FullyConnectedLayer(self.c_dim, self.w_dim) <span class="hljs-keyword">if</span> self.c_dim &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br>        features = [self.z_dim + (self.w_dim <span class="hljs-keyword">if</span> self.c_dim &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>)] + [self.w_dim] * self.num_layers<br>        <span class="hljs-keyword">for</span> idx, in_features, out_features <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(<span class="hljs-built_in">range</span>(num_layers), features[:-<span class="hljs-number">1</span>], features[<span class="hljs-number">1</span>:]):<br>            layer = FullyConnectedLayer(in_features, out_features, activation=<span class="hljs-string">&#x27;lrelu&#x27;</span>, lr_multiplier=lr_multiplier)<br>            <span class="hljs-built_in">setattr</span>(self, <span class="hljs-string">f&#x27;fc<span class="hljs-subst">&#123;idx&#125;</span>&#x27;</span>, layer)<br>        self.register_buffer(<span class="hljs-string">&#x27;w_avg&#x27;</span>, torch.zeros([w_dim]))<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, z, c, truncation_psi=<span class="hljs-number">1</span>, truncation_cutoff=<span class="hljs-literal">None</span>, update_emas=<span class="hljs-literal">False</span></span>):</span><br>        misc.assert_shape(z, [<span class="hljs-literal">None</span>, self.z_dim])<br>        <span class="hljs-keyword">if</span> truncation_cutoff <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            truncation_cutoff = self.num_ws<br><br>        <span class="hljs-comment"># Embed, normalize, and concatenate inputs.</span><br>        x = z.to(torch.float32)<br>        x = x * (x.square().mean(<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>) + <span class="hljs-number">1e-8</span>).rsqrt()<br>        <span class="hljs-keyword">if</span> self.c_dim &gt; <span class="hljs-number">0</span>:<br>            misc.assert_shape(c, [<span class="hljs-literal">None</span>, self.c_dim])<br>            y = self.embed(c.to(torch.float32))<br>            y = y * (y.square().mean(<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>) + <span class="hljs-number">1e-8</span>).rsqrt()<br>            x = torch.cat([x, y], dim=<span class="hljs-number">1</span>) <span class="hljs-keyword">if</span> x <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> y<br><br>        <span class="hljs-comment"># Execute layers.</span><br>        <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.num_layers):<br>            x = <span class="hljs-built_in">getattr</span>(self, <span class="hljs-string">f&#x27;fc<span class="hljs-subst">&#123;idx&#125;</span>&#x27;</span>)(x)<br><br>        <span class="hljs-comment"># Update moving average of W.</span><br>        <span class="hljs-keyword">if</span> update_emas:<br>            self.w_avg.copy_(x.detach().mean(dim=<span class="hljs-number">0</span>).lerp(self.w_avg, self.w_avg_beta))<br><br>        <span class="hljs-comment"># Broadcast and apply truncation.</span><br>        x = x.unsqueeze(<span class="hljs-number">1</span>).repeat([<span class="hljs-number">1</span>, self.num_ws, <span class="hljs-number">1</span>])<br>        <span class="hljs-keyword">if</span> truncation_psi != <span class="hljs-number">1</span>:<br>            x[:, :truncation_cutoff] = self.w_avg.lerp(x[:, :truncation_cutoff], truncation_psi)<br>        <span class="hljs-keyword">return</span> x<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">extra_repr</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-string">f&#x27;z_dim=<span class="hljs-subst">&#123;self.z_dim:d&#125;</span>, c_dim=<span class="hljs-subst">&#123;self.c_dim:d&#125;</span>, w_dim=<span class="hljs-subst">&#123;self.w_dim:d&#125;</span>, num_ws=<span class="hljs-subst">&#123;self.num_ws:d&#125;</span>&#x27;</span><br></code></pre></div></td></tr></table></figure></li><li><strong>分析网络</strong><sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://github.com/NVlabs/stylegan3">[2]</span></a></sup><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SynthesisNetwork</span>(<span class="hljs-params">torch.nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,</span></span><br><span class="hljs-params"><span class="hljs-function">        w_dim,                          <span class="hljs-comment"># Intermediate latent (W) dimensionality.</span></span></span><br><span class="hljs-params"><span class="hljs-function">        img_resolution,                 <span class="hljs-comment"># Output image resolution.</span></span></span><br><span class="hljs-params"><span class="hljs-function">        img_channels,                   <span class="hljs-comment"># Number of color channels.</span></span></span><br><span class="hljs-params"><span class="hljs-function">        channel_base        = <span class="hljs-number">32768</span>,    <span class="hljs-comment"># Overall multiplier for the number of channels.</span></span></span><br><span class="hljs-params"><span class="hljs-function">        channel_max         = <span class="hljs-number">512</span>,      <span class="hljs-comment"># Maximum number of channels in any layer.</span></span></span><br><span class="hljs-params"><span class="hljs-function">        num_layers          = <span class="hljs-number">14</span>,       <span class="hljs-comment"># Total number of layers, excluding Fourier features and ToRGB.</span></span></span><br><span class="hljs-params"><span class="hljs-function">        num_critical        = <span class="hljs-number">2</span>,        <span class="hljs-comment"># Number of critically sampled layers at the end.</span></span></span><br><span class="hljs-params"><span class="hljs-function">        first_cutoff        = <span class="hljs-number">2</span>,        <span class="hljs-comment"># Cutoff frequency of the first layer (f_&#123;c,0&#125;).</span></span></span><br><span class="hljs-params"><span class="hljs-function">        first_stopband      = <span class="hljs-number">2</span>**<span class="hljs-number">2.1</span>,   <span class="hljs-comment"># Minimum stopband of the first layer (f_&#123;t,0&#125;).</span></span></span><br><span class="hljs-params"><span class="hljs-function">        last_stopband_rel   = <span class="hljs-number">2</span>**<span class="hljs-number">0.3</span>,   <span class="hljs-comment"># Minimum stopband of the last layer, expressed relative to the cutoff.</span></span></span><br><span class="hljs-params"><span class="hljs-function">        margin_size         = <span class="hljs-number">10</span>,       <span class="hljs-comment"># Number of additional pixels outside the image.</span></span></span><br><span class="hljs-params"><span class="hljs-function">        output_scale        = <span class="hljs-number">0.25</span>,     <span class="hljs-comment"># Scale factor for the output image.</span></span></span><br><span class="hljs-params"><span class="hljs-function">        num_fp16_res        = <span class="hljs-number">4</span>,        <span class="hljs-comment"># Use FP16 for the N highest resolutions.</span></span></span><br><span class="hljs-params"><span class="hljs-function">        **layer_kwargs,                 <span class="hljs-comment"># Arguments for SynthesisLayer.</span></span></span><br><span class="hljs-params"><span class="hljs-function">    </span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.w_dim = w_dim<br>        self.num_ws = num_layers + <span class="hljs-number">2</span><br>        self.img_resolution = img_resolution<br>        self.img_channels = img_channels<br>        self.num_layers = num_layers<br>        self.num_critical = num_critical<br>        self.margin_size = margin_size<br>        self.output_scale = output_scale<br>        self.num_fp16_res = num_fp16_res<br><br>        <span class="hljs-comment"># Geometric progression of layer cutoffs and min. stopbands.</span><br>        last_cutoff = self.img_resolution / <span class="hljs-number">2</span> <span class="hljs-comment"># f_&#123;c,N&#125;</span><br>        last_stopband = last_cutoff * last_stopband_rel <span class="hljs-comment"># f_&#123;t,N&#125;</span><br>        exponents = np.minimum(np.arange(self.num_layers + <span class="hljs-number">1</span>) / (self.num_layers - self.num_critical), <span class="hljs-number">1</span>)<br>        cutoffs = first_cutoff * (last_cutoff / first_cutoff) ** exponents <span class="hljs-comment"># f_c[i]</span><br>        stopbands = first_stopband * (last_stopband / first_stopband) ** exponents <span class="hljs-comment"># f_t[i]</span><br><br>        <span class="hljs-comment"># Compute remaining layer parameters.</span><br>        sampling_rates = np.exp2(np.ceil(np.log2(np.minimum(stopbands * <span class="hljs-number">2</span>, self.img_resolution)))) <span class="hljs-comment"># s[i]</span><br>        half_widths = np.maximum(stopbands, sampling_rates / <span class="hljs-number">2</span>) - cutoffs <span class="hljs-comment"># f_h[i]</span><br>        sizes = sampling_rates + self.margin_size * <span class="hljs-number">2</span><br>        sizes[-<span class="hljs-number">2</span>:] = self.img_resolution<br>        channels = np.rint(np.minimum((channel_base / <span class="hljs-number">2</span>) / cutoffs, channel_max))<br>        channels[-<span class="hljs-number">1</span>] = self.img_channels<br><br>        <span class="hljs-comment"># Construct layers.</span><br>        self.<span class="hljs-built_in">input</span> = SynthesisInput(<br>            w_dim=self.w_dim, channels=<span class="hljs-built_in">int</span>(channels[<span class="hljs-number">0</span>]), size=<span class="hljs-built_in">int</span>(sizes[<span class="hljs-number">0</span>]),<br>            sampling_rate=sampling_rates[<span class="hljs-number">0</span>], bandwidth=cutoffs[<span class="hljs-number">0</span>])<br>        self.layer_names = []<br>        <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.num_layers + <span class="hljs-number">1</span>):<br>            prev = <span class="hljs-built_in">max</span>(idx - <span class="hljs-number">1</span>, <span class="hljs-number">0</span>)<br>            is_torgb = (idx == self.num_layers)<br>            is_critically_sampled = (idx &gt;= self.num_layers - self.num_critical)<br>            use_fp16 = (sampling_rates[idx] * (<span class="hljs-number">2</span> ** self.num_fp16_res) &gt; self.img_resolution)<br>            layer = SynthesisLayer(<br>                w_dim=self.w_dim, is_torgb=is_torgb, is_critically_sampled=is_critically_sampled, use_fp16=use_fp16,<br>                in_channels=<span class="hljs-built_in">int</span>(channels[prev]), out_channels= <span class="hljs-built_in">int</span>(channels[idx]),<br>                in_size=<span class="hljs-built_in">int</span>(sizes[prev]), out_size=<span class="hljs-built_in">int</span>(sizes[idx]),<br>                in_sampling_rate=<span class="hljs-built_in">int</span>(sampling_rates[prev]), out_sampling_rate=<span class="hljs-built_in">int</span>(sampling_rates[idx]),<br>                in_cutoff=cutoffs[prev], out_cutoff=cutoffs[idx],<br>                in_half_width=half_widths[prev], out_half_width=half_widths[idx],<br>                **layer_kwargs)<br>            name = <span class="hljs-string">f&#x27;L<span class="hljs-subst">&#123;idx&#125;</span>_<span class="hljs-subst">&#123;layer.out_size[<span class="hljs-number">0</span>]&#125;</span>_<span class="hljs-subst">&#123;layer.out_channels&#125;</span>&#x27;</span><br>            <span class="hljs-built_in">setattr</span>(self, name, layer)<br>            self.layer_names.append(name)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, ws, **layer_kwargs</span>):</span><br>        misc.assert_shape(ws, [<span class="hljs-literal">None</span>, self.num_ws, self.w_dim])<br>        ws = ws.to(torch.float32).unbind(dim=<span class="hljs-number">1</span>)<br><br>        <span class="hljs-comment"># Execute layers.</span><br>        x = self.<span class="hljs-built_in">input</span>(ws[<span class="hljs-number">0</span>])<br>        <span class="hljs-keyword">for</span> name, w <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(self.layer_names, ws[<span class="hljs-number">1</span>:]):<br>            x = <span class="hljs-built_in">getattr</span>(self, name)(x, w, **layer_kwargs)<br>        <span class="hljs-keyword">if</span> self.output_scale != <span class="hljs-number">1</span>:<br>            x = x * self.output_scale<br><br>        <span class="hljs-comment"># Ensure correct shape and dtype.</span><br>        misc.assert_shape(x, [<span class="hljs-literal">None</span>, self.img_channels, self.img_resolution, self.img_resolution])<br>        x = x.to(torch.float32)<br>        <span class="hljs-keyword">return</span> x<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">extra_repr</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27;\n&#x27;</span>.join([<br>            <span class="hljs-string">f&#x27;w_dim=<span class="hljs-subst">&#123;self.w_dim:d&#125;</span>, num_ws=<span class="hljs-subst">&#123;self.num_ws:d&#125;</span>,&#x27;</span>,<br>            <span class="hljs-string">f&#x27;img_resolution=<span class="hljs-subst">&#123;self.img_resolution:d&#125;</span>, img_channels=<span class="hljs-subst">&#123;self.img_channels:d&#125;</span>,&#x27;</span>,<br>            <span class="hljs-string">f&#x27;num_layers=<span class="hljs-subst">&#123;self.num_layers:d&#125;</span>, num_critical=<span class="hljs-subst">&#123;self.num_critical:d&#125;</span>,&#x27;</span>,<br>            <span class="hljs-string">f&#x27;margin_size=<span class="hljs-subst">&#123;self.margin_size:d&#125;</span>, num_fp16_res=<span class="hljs-subst">&#123;self.num_fp16_res:d&#125;</span>&#x27;</span>])<br><br></code></pre></div></td></tr></table></figure></li><li><strong>指标</strong>：FID、perceptual_path_length、Inception score等</li></ul><h2 id="3-其他"><a href="#3-其他" class="headerlink" title="3. 其他"></a>3. 其他</h2><ul><li><strong>工程数学：</strong>Jordan标准型、欧氏空间、酉空间；</li><li><strong>网络安全：</strong>Feistel密码；</li><li><strong>高级算法分析：</strong>分支限界法；</li><li><strong>高级机器学习：</strong>集成学习；</li><li><strong>高级人机交互技术：</strong> Touch&amp;Fold: A Foldable Haptic Actuator for Rendering Touch in<br>Mixed Reality</li></ul><h2 id="4-下周计划"><a href="#4-下周计划" class="headerlink" title="4. 下周计划"></a>4. 下周计划</h2><ol><li>阅读之前看过的一些论文的代码并整理其逻辑结构；</li><li>继续学习相关论文。</li></ol><h3 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h3><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://nvlabs-fi-cdn.nvidia.com/stylegan3/stylegan3-paper.pdf">https://nvlabs-fi-cdn.nvidia.com/stylegan3/stylegan3-paper.pdf</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://github.com/NVlabs/stylegan3">https://github.com/NVlabs/stylegan3</a><a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Weekly Note</category>
      
    </categories>
    
    
    <tags>
      
      <tag>GAN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Weekly-211017</title>
    <link href="/2021/10/16/1-Weekly-211017/"/>
    <url>/2021/10/16/1-Weekly-211017/</url>
    
    <content type="html"><![CDATA[<h3 id="本周学习汇报"><a href="#本周学习汇报" class="headerlink" title="本周学习汇报"></a>本周学习汇报</h3><span id="more"></span><blockquote><div>            <input type="checkbox" disabled >《Effects of Emotion and Agency on Presence in Virtual Reality》<sup id=fnref:1 class=footnote-ref><a href=#fn:1 rel=footnote><span class=hint--top hint--rounded aria-label=https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445588>[1]</span></a></sup>          </div><div>            <input type="checkbox" disabled checked="checked">《Semantic Photo Manipulation with a Generative Image Prior》<sup id=fnref:2 class=footnote-ref><a href=#fn:2 rel=footnote><span class=hint--top hint--rounded aria-label=http://gandissect.csail.mit.edu/>[2]</span></a></sup>          </div><div>            <input type="checkbox" disabled >《Alias-Free Generative Adversarial Networks》<sup id=fnref:3 class=footnote-ref><a href=#fn:3 rel=footnote><span class=hint--top hint--rounded aria-label=https://nvlabs-fi-cdn.nvidia.com/stylegan3/stylegan3-paper.pdf>[3]</span></a></sup>          </div></blockquote><h2 id="1-情绪与代理对虚拟环境的影响-Undone"><a href="#1-情绪与代理对虚拟环境的影响-Undone" class="headerlink" title="1. 情绪与代理对虚拟环境的影响  Undone"></a>1. 情绪与代理对虚拟环境的影响  <span class="label label-primary">Undone</span></h2><ul><li><strong>动机：</strong>探索情绪与代理(虚拟玩家)对VR中虚拟临场感的影响。</li><li><strong>结论：</strong>虚拟环境诱导的情绪与临场感呈现正相关；agency对临场感有正向影响，并且可以缓和情绪对临场感的影响。</li><li><strong>方式：</strong>agency对临场感有正向影响，并且可以缓和情绪对临场感的影响；用主观措施的分离因素进行建模，解释一些看似矛盾的结果。</li></ul><h2 id="2-自然图像语义操作-adding-removing-changing"><a href="#2-自然图像语义操作-adding-removing-changing" class="headerlink" title="2. 自然图像语义操作(adding, removing, changing)"></a>2. 自然图像语义操作(adding, removing, changing)</h2><ul><li><strong>主要思想：</strong>运用GAN的图像重构思想，实现照片的交互式操作，包括增删改<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://ganpaint.io/#paper">[4]</span></a></sup>。</li></ul><p><img src="/images/1-weeklynote/211017/2-2.png" alt=""></p><center><font color=blue >图2-1 本文工作 </font></center><ul><li><strong>论文难点：</strong>图像重构问题(很难找到一个潜代码z做到图像的重构)，合成图像部分与原始图像不匹配问题(生成图像的真实性)。</li><li><strong>相关工作：</strong>GAN、图像的高级语义操纵。</li><li><strong>网络结构：</strong>主要包含图像映射、潜空间编辑、图像生成三步，如图所示：</li></ul><p><img src="/images/1-weeklynote/211017/2-1.png" alt=""></p><center><font color=blue >图2-2 网络结构 </font></center><ul><li><p><strong>主要方法：</strong></p><blockquote><ol><li>使用生成器进行图像重构，使每一层生成feature map与原始图像都更加接近。<script type="math/tex; mode=display">\mathcal{L}_{r}(\mathrm{x}, G(\mathrm{z}))=\|\mathrm{x}-G(\mathrm{z})\|_{1}+\lambda_{\mathrm{VGG}} \sum_{i=1}^{N} \frac{1}{M_{i}}\left\|F^{(i)}(\mathrm{x})-F^{(i)}(G(\mathrm{z}))\right\|_{1}</script></li><li>特定图像适应问题，应用mask的方法，更好地保留图像的细节。</li><li>保留语义表示， 生成器是有层级结构的，第一层对z进行操作g1(z)，最后一层更接近像素级别并具有更多的细节信息，因此提出一个拆解的思想：底层与图像结构信息相关，高层与图像的细节信息相关，因此当G‘进行重构时，仅调整Gf(即高层网络)<script type="math/tex; mode=display">\begin{aligned}G_{F}^{\prime}\left(z_{h}\right) & \equiv g_{n}\left(\left(1+\delta_{n-1}\right) \odot g_{n-1}\left(\cdots\left(\left(1+\delta_{h+1}\right) \odot g_{h+1}\left(z_{h}\right) \cdots\right)\right)\right) \\G^{\prime}(z) & \equiv G_{F}^{\prime}\left(G_{H}(z)\right)\end{aligned}</script></li><li>生成器整体优化，其中 <script type="math/tex">{L}_{\text {reg }}</script> 代表正则化项，防止过拟合。<script type="math/tex; mode=display">\mathcal{L}=\mathcal{L}_{\text {match }}+\lambda_{\text {reg }} \mathcal{L}_{\text {reg }}</script></li></ol></blockquote></li><li><p><strong>仍存在的问题：</strong></p><blockquote><ol><li>实时性与效率问题；</li><li>潜空间内的特征具有耦合性；</li><li>图像生成的质量与分辨率受GAN的限制。 </li></ol></blockquote></li></ul><h2 id="3-StyleGAN3（NeurIPS-2021）-Undone"><a href="#3-StyleGAN3（NeurIPS-2021）-Undone" class="headerlink" title="3. StyleGAN3（NeurIPS 2021） Undone"></a>3. StyleGAN3（NeurIPS 2021） <span class="label label-primary">Undone</span></h2><ul><li><strong>提出动机：</strong>从根本上解决了StyleGAN2 图像坐标与特征粘连的问题，实现了真正的图像平移、旋转等不变性，大幅提高了图像合成质量。其目标在于，创建更自然的转换层次的体系结构，让每个特征的精确亚像素位置都从底层粗特征中获得。</li><li><strong>应用领域：</strong>视频与动画的生成模型（在亚像素尺度上实现了图像平移和旋转不变性）。</li><li><p><strong>等变中的连续信号处理：</strong>根据Nyquist–Shannon sampling理论与Whittaker–Shanno差值公式。<br><img src="/images/1-weeklynote/211017/3-1.jpg" alt=""></p><center><font color=blue >图3-1 网络结构 </font></center></li><li><p><strong>网络层改进：</strong>等变网络层，有卷积、上采样下采样、非线性等方面的改进。</p></li></ul><h2 id="4-其他"><a href="#4-其他" class="headerlink" title="4. 其他"></a>4. 其他</h2><ul><li><strong>工程数学：</strong>λ-矩阵、Smith标准形、不变因子、行列式因子；</li><li><strong>网络安全：</strong>多表加密之Hill密码；</li><li><strong>高级算法分析：</strong>动态规划算法，分支限界法；</li><li><strong>高级机器学习：</strong>贝叶斯分类器；</li><li><strong>高级人机交互技术：</strong>PenSight； </li><li><strong>随机森林：</strong>学习并练习一些数据处理的相关内容。</li></ul><h2 id="5-下周计划"><a href="#5-下周计划" class="headerlink" title="5. 下周计划"></a>5. 下周计划</h2><ol><li>阅读StyleGAN3的论文。</li><li>阅读StyleGAN3相关代码。</li></ol><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445588">https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445588</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="http://gandissect.csail.mit.edu/">http://gandissect.csail.mit.edu/</a><a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span><a href="https://nvlabs-fi-cdn.nvidia.com/stylegan3/stylegan3-paper.pdf">https://nvlabs-fi-cdn.nvidia.com/stylegan3/stylegan3-paper.pdf</a><a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:4" class="footnote-text"><span><a href="https://ganpaint.io/#paper">https://ganpaint.io/#paper</a><a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Weekly Note</category>
      
    </categories>
    
    
    <tags>
      
      <tag>GAN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Weekly-211010</title>
    <link href="/2021/10/10/1-Weekly-211010/"/>
    <url>/2021/10/10/1-Weekly-211010/</url>
    
    <content type="html"><![CDATA[<h3 id="关于论文及代码阅读"><a href="#关于论文及代码阅读" class="headerlink" title=" 关于论文及代码阅读 "></a><font color=red> 关于论文及代码阅读 </font></h3><span id="more"></span><blockquote><p><strong>任务</strong><br><div>            <input type="checkbox" disabled checked="checked">《Evaluation of Latent Space Learning with Procedurally-Generated Datasets of Shapes》<sup id=fnref:1 class=footnote-ref><a href=#fn:1 rel=footnote><span class=hint--top hint--rounded aria-label=Evaluation of Latent Space Learning with Procedurally-Generated Datasets of Shapes>[1]</span></a></sup>          </div><br><div>            <input type="checkbox" disabled >《WarpedGANSpace: Finding non-linear RBF paths in GAN latent space》——code <sup id=fnref:2 class=footnote-ref><a href=#fn:2 rel=footnote><span class=hint--top hint--rounded aria-label=https://github.com/chi0tzp/WarpedGANSpace>[2]</span></a></sup>          </div></p></blockquote><h2 id="1-潜空间学习评估"><a href="#1-潜空间学习评估" class="headerlink" title="1. 潜空间学习评估"></a>1. 潜空间学习评估</h2><ul><li><strong>主要思想：</strong>比较不同神经网络模型学习latent space的质量，以生成3D模型。</li><li><strong>相关工作：</strong>3D形状编码（AEs，VAEs，GANs）、latent space评估。</li><li><strong>评价指标：</strong> 协方差。</li></ul><script type="math/tex; mode=display">\mathrm{d} \operatorname{Cov}^{2}(X, Y)=\frac{1}{n^{2}} \sum_{j=1}^{n} \sum_{k=1}^{n} A_{j, k} B_{j, k}</script><ul><li><strong>方法设计：</strong>基于split grammar的思想，使用递归的方式生成新的3D结构，与box与patch的使用。其中形状编码结构有：<ul><li>3D-AE</li><li>3D-VAE 在interpolation质量上效果最弱。</li><li>3D-GAN </li><li>3D-PGAE 在latent space directions任务上效果最优。</li></ul></li></ul><h2 id="2-WarpedGANSpace"><a href="#2-WarpedGANSpace" class="headerlink" title="2. WarpedGANSpace"></a>2. WarpedGANSpace</h2><ul><li><strong>目的：</strong>尝试在GAN的潜在空间内寻找非线性RBF路径。</li><li><strong>扭曲函数：</strong>用于扭曲latnet space，并赋予梯度。对给定latent code，在确定的向量场上遍历latent space。每个扭曲函数都有一组支持向量定义，并产生一系列非线性路径。</li><li><strong>预训练模型：</strong>generators、arcface、fairface、hopenet、sfd等。</li><li><strong>潜在空间遍历：</strong>路径潜在代码和生成的图像序列，</li><li><strong>属性空间遍历：</strong>包括面部边界框、身份分数、年龄、种族和性别估计，在偏航、俯仰和滚动角度方面的姿态估计，以及12个面部动作单元预测。</li></ul><h2 id="3-其他"><a href="#3-其他" class="headerlink" title="3.其他"></a>3.其他</h2><ul><li><strong>工程数学：</strong>线性变换的矩阵与运算、同构、特征向量、不变子空间、相似对角形等。</li><li><strong>网络安全：</strong>对称密码。</li><li><strong>高级算法分析：</strong>贪心、动态规划算法。</li><li><strong>高级机器学习：</strong>SVM核函数与软间隔正则化ppt讲解。</li><li><strong>高级人机交互技术：</strong>脑机交互技术、评估。 </li></ul><h2 id="4-下周计划"><a href="#4-下周计划" class="headerlink" title="4.下周计划"></a>4.下周计划</h2><ol><li>阅读WarpedGANSpace代码。</li><li>阅读其他相关论文。</li></ol><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://openaccess.thecvf.com/content/ICCV2021W/DLGC/html/Ali_Evaluation_of_Latent_Space_Learning_With_Procedurally-Generated_Datasets_of_Shapes_ICCVW_2021_paper.html">Evaluation of Latent Space Learning with Procedurally-Generated Datasets of Shapes</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://github.com/chi0tzp/WarpedGANSpace">https://github.com/chi0tzp/WarpedGANSpace</a><a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Weekly Note</category>
      
    </categories>
    
    
    <tags>
      
      <tag>GAN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Weekly-211003</title>
    <link href="/2021/10/03/1-Weekly-211003/"/>
    <url>/2021/10/03/1-Weekly-211003/</url>
    
    <content type="html"><![CDATA[<h3 id="关于论文及代码阅读"><a href="#关于论文及代码阅读" class="headerlink" title=" 关于论文及代码阅读 "></a><font color=red> 关于论文及代码阅读 </font></h3><span id="more"></span><blockquote><p><strong>任务</strong><br><div>            <input type="checkbox" disabled >《Deep Learning for Generic Object Detection: A Survey》<sup id=fnref:1 class=footnote-ref><a href=#fn:1 rel=footnote><span class=hint--top hint--rounded aria-label=https://link.springer.com/article/10.1007/s11263-019-01247-4>[1]</span></a></sup>          </div></p><div>            <input type="checkbox" disabled checked="checked">《Improving Visual Quality of Unrestricted Adversarial Examples with Wavelet-VAE》<sup id=fnref:2 class=footnote-ref><a href=#fn:2 rel=footnote><span class=hint--top hint--rounded aria-label=https://arxiv.org/abs/2108.11032>[2]</span></a></sup>          </div><div>            <input type="checkbox" disabled checked="checked">《WarpedGANSpace: Finding non-linear RBF paths in GAN latent space》<sup id=fnref:3 class=footnote-ref><a href=#fn:3 rel=footnote><span class=hint--top hint--rounded aria-label=https://arxiv.org/abs/2109.13357>[3]</span></a></sup>          </div></blockquote><h2 id="1-小波VAE生成对抗样本"><a href="#1-小波VAE生成对抗样本" class="headerlink" title="1. 小波VAE生成对抗样本"></a>1. 小波VAE生成对抗样本</h2><ul><li><strong>主要思想：</strong>使用小波VAE结构，通过修改一些latent code，由原始image得到生成对抗样本。</li><li><strong>相关工作：</strong>FGSM、PGD、MIM、DIM等其他对抗样本生成方法。</li><li><strong>评价指标：</strong>ASR用来测量attack能力，FID与LPIPS用来测量图片质量。</li></ul><script type="math/tex; mode=display">{ Score }_{ASR}=100 \% \times \frac{\left\|\left\{x_{a d v} \mid f\left(x_{a d v}\right) \neq y\right\}\right\|}{N}</script><script type="math/tex; mode=display">{Score}_{F I D}=100 \% \times \sqrt{1-\frac{\min \left(F I D\left(x, x_{a d v}\right), 200\right)}{200}}</script><script type="math/tex; mode=display">{Score}_{L P I P S}=100 \% \times \sum_{l} \frac{1}{H_{l} W_{l}} \sum_{h, w} d\left(f_{h w}^{l}, f_{0 h w}^{l}\right)</script><ul><li><strong>数据集：</strong>1000张随机选择的ImageNet validation images。</li><li><strong>方法设计：</strong>使用小波VAE网络结构，图像由WPT分解，并将实际图像投影到潜空间中，即高纬像素空间到低纬流形空间。其中，第一阶段设计小波VAE学习输入图像的潜代码分布，第二阶段固定参数，由目标图像编码得到z（梯度优化方式）。<br><img src="/images/1-weeklynote/211003/1-1.png" alt=""><center><font color=blue >图1-1 wavelet-VAE网络结构 </font></center></li></ul><h2 id="2-寻找非线性RBF路径"><a href="#2-寻找非线性RBF路径" class="headerlink" title="2. 寻找非线性RBF路径"></a>2. 寻找非线性RBF路径</h2><ul><li><strong>主要思想：</strong>探索GAN潜在空间(latent space)的结构，增强图像生成的可解释性，以一种非线性的方式，控制样本图像的生成。</li><li><strong>相关工作：</strong>生成性学习中的解耦，预训练GAN生成器中发现可解释路径，如StyleGAN2等。</li><li><strong>评价指标：</strong>accuracy、 coefficient、 traversal path length等等。</li><li><strong>对比模型：</strong>SNGAN（MNIST）、SNGAN（Anime）、BigGAN(ImageNet)、ProGAN(CelebAHQ)、StyleGAN2(FFHQ)等。</li><li><strong>方法设计：</strong><br><img src="/images/1-weeklynote/211003/2-1.png" alt=""><center><font color=blue >图2-1 WarpedGANSpace设计流程 </font></center></li></ul><h2 id="3-下周计划"><a href="#3-下周计划" class="headerlink" title="3. 下周计划"></a>3. 下周计划</h2><ol><li>尝试读懂WarpedGANSpace方法到StyleGAN2模型中。</li><li>阅读其他相关论文。</li></ol><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://link.springer.com/article/10.1007/s11263-019-01247-4">https://link.springer.com/article/10.1007/s11263-019-01247-4</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://arxiv.org/abs/2108.11032">https://arxiv.org/abs/2108.11032</a><a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span><a href="https://arxiv.org/abs/2109.13357">https://arxiv.org/abs/2109.13357</a><a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Weekly Note</category>
      
    </categories>
    
    
    <tags>
      
      <tag>GAN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Mac部分配置介绍</title>
    <link href="/2021/09/27/0-Mac%E9%83%A8%E5%88%86%E9%85%8D%E7%BD%AE%E4%BB%8B%E7%BB%8D/"/>
    <url>/2021/09/27/0-Mac%E9%83%A8%E5%88%86%E9%85%8D%E7%BD%AE%E4%BB%8B%E7%BB%8D/</url>
    
    <content type="html"><![CDATA[<p>对Mac的一些美化，以及部分工具的介绍<br><span id="more"></span></p><h2 id="1-HomeBrew"><a href="#1-HomeBrew" class="headerlink" title="1. HomeBrew"></a>1. HomeBrew</h2><p><strong>HomeBrew</strong>是一款软件包管理工具，支持macOS系统和Linux系统，有安装、卸载、更新、查看、搜索等功能。这里主要用来管理我的电脑的一部分软件。</p><p>常用命令：<br><figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 显示帮助</span><br>brew –help<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 安装软件</span><br>brew install xxx<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 卸载软件</span><br>brew uninstall xxx<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 搜索软件</span><br>brew search xxx<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 查看经安装软件列表</span><br>brew list<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 更新所有软件</span><br>brew update<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 更新某具体软件</span><br>brew upgrade xxx<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash">显示软件内容信息</span><br>brew info xxx<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 查看哪些已安装的程序需要更新</span><br>brew outdated<br></code></pre></div></td></tr></table></figure><br>cask常用命令与brew相同，可以用<code>brew install --cask xxx</code>来表示，或者直接使用<code>brew install xxx</code> .</p><p>我的homebrew所在路径：<code>/opt/homebrew/Library/Taps/homebrew</code></p><h2 id="2-关于终端（主要使用iterm2）"><a href="#2-关于终端（主要使用iterm2）" class="headerlink" title="2. 关于终端（主要使用iterm2）"></a>2. 关于终端（主要使用iterm2）</h2><p><strong>配置流程：</strong><br><pre><code class=" mermaid">graph LR    id0[iterm2]    id1[oh-my-zsh]    id2[tmux]    id3[vim]    id0 --&gt; id1    id1 --&gt; id2    id2 --&gt; id3</code></pre><br><strong>注意事项：</strong></p><ul><li>主要采用oh-my-zsh以及某些配色文件在iterm2中进行配置。</li><li>已安装并使用的插件有git，autojump，zsh-syntax- highlighting，zsh-autosuggestion等，在zshrc文件中修改插件名称，并通过<code>source ~/.zshrc</code>指令进行刷新配置，这些插件在github上有详细说明。</li><li>终端代理配置位于zshrc文件内，开启方式为<code>proxy_on</code> 关闭方式为<code>proxy_off</code>。使用了clashx以及AgentNEO来设置代理，端口详见clashx的配置文件（与AgentNEO对应）。可以使用<code>curl www.google.com</code>进行测试，不能用<code>ping</code>进程测试，因为代理使用http或者socket5进行通信。</li></ul><h3 id="2-1-iTerm与Terminal常用快捷键"><a href="#2-1-iTerm与Terminal常用快捷键" class="headerlink" title="2.1 iTerm与Terminal常用快捷键"></a>2.1 iTerm与Terminal常用快捷键</h3><ul><li><span class="label label-primary">command + d</span> 垂直分割视窗</li><li><span class="label label-primary">command + shift + d</span> 水平分割视窗</li><li><span class="label label-primary">command + t</span> 打开新的视窗</li><li><span class="label label-primary">command + w</span> 关闭当前视窗</li><li><span class="label label-primary">command + ;</span> 显示历史命令</li><li><span class="label label-primary">command + enter</span> 全屏</li><li><span class="label label-primary">command + f</span> 查找</li><li><span class="label label-primary">ctrl + l</span> 清屏</li><li><span class="label label-primary">ctrl + d</span> 删除</li><li><span class="label label-primary">ctrl + k</span> 删除光标后的内容</li></ul><h3 id="2-2-tmux终端复用"><a href="#2-2-tmux终端复用" class="headerlink" title="2.2 tmux终端复用"></a>2.2 tmux终端复用</h3><p><strong>tmux</strong> 是一个终端复用开发工具，采用client/server模型，即在一个终端管理多个分离的会话（session）、窗口（window）、面板（panel），其中，一个session有多个window，一个window有多个panel，提高开发效率，使用多个命令行完成多任务，tmux支持窗口与会话解绑，即窗口关闭时，会话不会终止，而是继续运行。（如ssh登录远程服务器）</p><p><strong>前缀键：</strong> <span class="label label-primary">ctrl b</span>或者<span class="label label-primary">ctrl a</span></p><h4 id="2-2-1-启动与退出"><a href="#2-2-1-启动与退出" class="headerlink" title="2.2.1 启动与退出"></a>2.2.1 启动与退出</h4><figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">tmux #启动<br>exit #退出<br></code></pre></div></td></tr></table></figure><h4 id="2-2-2-会话管理"><a href="#2-2-2-会话管理" class="headerlink" title="2.2.2 会话管理"></a>2.2.2 会话管理</h4><figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 新建会话</span><br><span class="hljs-meta">$</span><span class="bash"> tmux new -s &lt;session-name&gt;</span> <br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 分离会话 或者Ctrl+b d</span><br><span class="hljs-meta">$</span><span class="bash"> tmux detach</span> <br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 查看当前所有会话 或者tumx list- session</span><br><span class="hljs-meta">$</span><span class="bash"> tmux ls</span> <br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 接入会话</span><br><span class="hljs-meta">#</span><span class="bash"> 使用会话编号</span><br><span class="hljs-meta">$</span><span class="bash"> tmux attach -t 0</span><br><span class="hljs-meta">#</span><span class="bash"> 使用会话名称</span><br><span class="hljs-meta">$</span><span class="bash"> tmux attach -t &lt;session-name&gt;</span><br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 切换会话 或者Ctrl b + s</span><br><span class="hljs-meta">#</span><span class="bash"> 使用会话编号</span><br><span class="hljs-meta">$</span><span class="bash"> tmux switch -t 0</span><br><span class="hljs-meta">#</span><span class="bash"> 使用会话名称</span><br><span class="hljs-meta">$</span><span class="bash"> tmux switch -t &lt;session-name&gt;</span><br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 重命名会话 或者 Ctrl b + $</span><br><span class="hljs-meta">$</span><span class="bash"> tmux rename-session -t 0 &lt;new-name&gt;</span><br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 杀死会话</span><br><span class="hljs-meta">$</span><span class="bash"> tmux kill-session -t 0 &lt;new-name&gt;</span><br><span class="hljs-meta">#</span><span class="bash"> kill-server可以杀死所有会话</span><br></code></pre></div></td></tr></table></figure><h4 id="2-2-3-窗口管理"><a href="#2-2-3-窗口管理" class="headerlink" title="2.2.3 窗口管理"></a>2.2.3 窗口管理</h4><figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 新建窗口</span><br><span class="hljs-meta">$</span><span class="bash"> tmux new-window</span><br><span class="hljs-meta">#</span><span class="bash"> 新建一个指定名称的窗口</span><br><span class="hljs-meta">$</span><span class="bash"> tmux new-window -n &lt;window-name&gt;</span><br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 切换窗口</span><br><span class="hljs-meta">#</span><span class="bash"> 切换到指定编号的窗口</span><br><span class="hljs-meta">$</span><span class="bash"> tmux select-window -t &lt;window-number&gt;</span><br><span class="hljs-meta">#</span><span class="bash"> 切换到指定名称的窗口</span><br><span class="hljs-meta">$</span><span class="bash"> tmux select-window -t &lt;window-name&gt;</span><br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 重命名</span><br><span class="hljs-meta">$</span><span class="bash"> tmux rename-window &lt;new-name&gt;</span><br></code></pre></div></td></tr></table></figure><div class="table-container"><table><thead><tr><th style="text-align:center">快捷键</th><th style="text-align:center">含义</th></tr></thead><tbody><tr><td style="text-align:center"><span class="label label-primary">ctrl+b c</span></td><td style="text-align:center">创建一个新窗口，状态栏会显示多个窗口的信息</td></tr><tr><td style="text-align:center"><span class="label label-primary">Ctrl+b p</span></td><td style="text-align:center">切换到上一个窗口（按照状态栏上的顺序）</td></tr><tr><td style="text-align:center"><span class="label label-primary">ctrl+b n</span></td><td style="text-align:center">切换到下一个窗口</td></tr><tr><td style="text-align:center"><span class="label label-primary">ctrl+b number</span></td><td style="text-align:center">切换到指定编号的窗口，其中的[number]是状态栏上的窗口编号</td></tr><tr><td style="text-align:center"><span class="label label-primary">ctrl+b w</span></td><td style="text-align:center">从列表中选择窗口</td></tr><tr><td style="text-align:center"><span class="label label-primary">ctrl+b ,</span></td><td style="text-align:center">窗口重命名</td></tr></tbody></table></div><h4 id="2-2-4-面板管理"><a href="#2-2-4-面板管理" class="headerlink" title="2.2.4 面板管理"></a>2.2.4 面板管理</h4><figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 划分上下两个窗格</span><br><span class="hljs-meta">$</span><span class="bash"> tmux split-window</span><br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 划分左右两个窗格</span><br><span class="hljs-meta">$</span><span class="bash"> tmux split-window -h</span><br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 光标切换到上方窗格</span><br><span class="hljs-meta">$</span><span class="bash"> tmux select-pane -U</span><br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 光标切换到下方窗格</span><br><span class="hljs-meta">$</span><span class="bash"> tmux select-pane -D</span><br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 光标切换到左边窗格</span><br><span class="hljs-meta">$</span><span class="bash"> tmux select-pane -L</span><br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 光标切换到右边窗格</span><br><span class="hljs-meta">$</span><span class="bash"> tmux select-pane -R</span><br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 当前窗格上移</span><br><span class="hljs-meta">$</span><span class="bash"> tmux swap-pane -U</span><br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 当前窗格下移</span><br><span class="hljs-meta">$</span><span class="bash"> tmux swap-pane -D</span><br></code></pre></div></td></tr></table></figure><p><strong>快捷键：</strong></p><ul><li><code>Ctrl+b &quot;</code>：划分上下两个窗格。</li><li><code>Ctrl+b %</code>：划分左右两个窗格。</li><li><code>Ctrl+b &lt;arrow key&gt;</code>：光标切换到其他窗格。<code>&lt;arrow key&gt;</code>是指向要切换到的窗格的方向键，比如切换到下方窗格，就按方向键<code>↓</code>。</li><li><code>Ctrl+b ;</code>：光标切换到上一个窗格。</li><li><code>Ctrl+b o</code>：光标切换到下一个窗格。</li><li><code>Ctrl+b &#123;</code>：当前窗格与上一个窗格交换位置。</li><li><code>Ctrl+b &#125;</code>：当前窗格与下一个窗格交换位置。</li><li><code>Ctrl+b Ctrl+o</code>：所有窗格向前移动一个位置，第一个窗格变成最后一个窗格。</li><li><code>Ctrl+b Alt+o</code>：所有窗格向后移动一个位置，最后一个窗格变成第一个窗格。</li><li><code>Ctrl+b x</code>：关闭当前窗格。</li><li><code>Ctrl+b !</code>：将当前窗格拆分为一个独立窗口。</li><li><code>Ctrl+b z</code>：当前窗格全屏显示，再使用一次会变回原来大小。</li><li><code>Ctrl+b Ctrl+&lt;arrow key&gt;</code>：按箭头方向调整窗格大小。</li><li><code>Ctrl+b q</code>：显示窗格编号。</li></ul><h2 id="3-关于anaconda"><a href="#3-关于anaconda" class="headerlink" title="3. 关于anaconda"></a>3. 关于anaconda</h2><p>首先需要更新文件<code>source ~/.bash_profile</code>,然后再去激活pytorch的环境。（此部分略）</p><ul><li>.bash_profile文件内保存与bash相关的内容，包含anaconda，以及pip与python的位置。</li><li>.zshrc文件保存iterm相关配置</li></ul><h2 id="4-关于vim的使用"><a href="#4-关于vim的使用" class="headerlink" title="4. 关于vim的使用"></a>4. 关于vim的使用</h2><p>vim是一种vi发展出的文本编辑器，包含许多编程的功能；其相关配置文件保存在了～/.vimrc文件中“。</p><ul><li>参考视频<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://www.imooc.com/video/19472">[1]</span></a></sup></li><li>参考GitHub仓库<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://github.com/PegasusWang/vim-config">[2]</span></a></sup></li></ul><p><strong>键盘图（图源网络）：</strong><br><img src="/images/0-helloworld/vim.png" alt=""></p><h3 id="4-1-基本介绍"><a href="#4-1-基本介绍" class="headerlink" title="4.1 基本介绍"></a>4.1 基本介绍</h3><blockquote><ul><li>insert-&gt;normal ESC</li><li>a append </li><li>i insert</li><li>o open a line below</li><li>A append after line</li><li>I insert before line</li><li>O append a line above</li></ul><p><strong>命令模式下的使用</strong></p><ul><li>Insert\Normal\Command</li><li>set nu  #显示行号</li><li><strong>vs #竖分屏</strong></li><li><strong>sp #横分屏</strong></li><li>% s/foo/bar/g #全局替换</li><li>/ #查找</li><li>v #块状选择</li><li>V #选择整行</li><li>Ctrl v #方块选择</li></ul></blockquote><h3 id="4-2-快速删除"><a href="#4-2-快速删除" class="headerlink" title="4.2 快速删除"></a>4.2 快速删除</h3><blockquote><ul><li>Ctrl h #删除字符</li><li>Ctrl w #删除单词</li><li>Ctrl u #删除整行</li><li>Ctrl a #移到开头</li><li>Ctrl e #移到结尾</li><li>Ctrl b #前移</li><li>Ctrl f #后移</li><li>Ctrl [/c #相当于Esc</li><li>gi #快速跳转到最后编辑的地方进行插入</li></ul></blockquote><h3 id="4-3-快速移动"><a href="#4-3-快速移动" class="headerlink" title="4.3 快速移动"></a>4.3 快速移动</h3><blockquote><ul><li>w:下一个单词开头 e:单词结尾</li><li>b:上一个单词的开头</li><li>f+单词：寻找</li><li>F+char：向前寻找</li><li>；下一个</li><li>，上一个</li><li>t：单词前一个</li><li>0:行首</li><li>$:行尾</li><li>syntax on:代码高亮</li></ul></blockquote><h3 id="4-4-安装插件"><a href="#4-4-安装插件" class="headerlink" title="4.4 安装插件"></a>4.4 安装插件</h3><blockquote><p>vim-plug</p><ol><li>在plug里增加插件名称</li><li>source ～/.vimrc</li><li>PlugInstall执行安装相应的插件</li></ol><p><strong>插件介绍——~/.vimrc文件</strong></p><ul><li>, + g:跳转到目录树</li><li>，+ v:跳转到当前文件对应目录</li><li>Ctrl w + p: 切换窗口</li><li>Ctrl p:模糊搜索文件</li><li>Ctrl k:代码高亮————Ctrl K: 取消代码高亮</li><li>ss :easy motion快速在整个文件中跳转相应的位置。</li><li>Vim surround:成双成对的编辑//未配置</li></ul></blockquote><h2 id="5-关于Hexo"><a href="#5-关于Hexo" class="headerlink" title="5. 关于Hexo"></a>5. 关于Hexo</h2><ul><li><p>初始化<code>hexo init</code></p></li><li><p>写博客流程</p><figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 在blog文件夹下进行修改设置</span><br>hexo clean<br><span class="hljs-meta">#</span><span class="bash"> 生成blog文件</span><br>hexo g/generate<br><span class="hljs-meta">#</span><span class="bash"> 开启服务</span><br>hexo s/server<br><span class="hljs-meta">#</span><span class="bash"> 此时可以根据本地连接查看当前博客内容</span><br><span class="hljs-meta">#</span><span class="bash"> 部署到github上</span><br>hexo d/deploy<br></code></pre></div></td></tr></table></figure></li><li>使用主题Fluid<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://fluid-dev.github.io/hexo-fluid-docs/">[3]</span></a></sup></li></ul><h2 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://www.imooc.com/video/19472">https://www.imooc.com/video/19472</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://github.com/PegasusWang/vim-config">https://github.com/PegasusWang/vim-config</a><a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span><a href="https://fluid-dev.github.io/hexo-fluid-docs/">https://fluid-dev.github.io/hexo-fluid-docs/</a><a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Hello World</category>
      
    </categories>
    
    
    <tags>
      
      <tag>tools</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>关于我遇到的猫</title>
    <link href="/2021/09/26/4-Cats/"/>
    <url>/2021/09/26/4-Cats/</url>
    
    <content type="html"><![CDATA[<h1 id="关于我遇到的一些猫"><a href="#关于我遇到的一些猫" class="headerlink" title="关于我遇到的一些猫"></a>关于我遇到的一些猫</h1><span id="more"></span><h1 id="Cat1"><a href="#Cat1" class="headerlink" title="Cat1"></a>Cat1</h1><p><img src="/images/4-else/cat/cat1.jpg" alt=""></p><ul><li>Name：胡胡｜胡女士</li><li>Home：宿舍楼下</li><li>Story：她可能是我上学期拍摄的最多的猫，有次我还拍到了她赏雪的照片，胡胡和大橘关系比较好。</li></ul><h1 id="Cat2"><a href="#Cat2" class="headerlink" title="Cat2"></a>Cat2</h1><p><img src="/images/4-else/cat/cat2.jpg" alt=""></p><ul><li>Name：小舞</li><li>Home：快递点旁的草坪</li><li>Story：我遇到的第一只猫，短尾，目前她处于半领养状态。</li></ul><h1 id="Cat3"><a href="#Cat3" class="headerlink" title="Cat3"></a>Cat3</h1><p><img src="/images/4-else/cat/cat3.jpg" alt=""></p><ul><li>Name：焦糖</li><li>Home：商学院门口（可能，因为没见到它第二面///现在见了好多次了）</li><li>Story：她和砂糖关系比较好，超会撒娇，但是吃饭时就是个订书机。</li></ul><h1 id="Cat4"><a href="#Cat4" class="headerlink" title="Cat4"></a>Cat4</h1><p><img src="/images/4-else/cat/cat4.jpg" alt=""></p><ul><li>Name：小橘</li><li>Home：操场，旁边的麻袋</li><li>Story：我有次晚上跑步，每跑一圈都要看它在不在，后来有段时间晨练时也每天都能看到它。</li></ul><h1 id="Cat5"><a href="#Cat5" class="headerlink" title="Cat5"></a>Cat5</h1><p><img src="/images/4-else/cat/cat5.jpg" alt=""></p><ul><li>Name：白糖</li><li>Home：图书馆前的红色建筑</li><li>Story： 我目前最喜欢的一只小猫咪，她能听懂自己的名字。</li></ul><h1 id="试试插入视频与音乐"><a href="#试试插入视频与音乐" class="headerlink" title="试试插入视频与音乐"></a>试试插入视频与音乐</h1><center><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=501747707&auto=1&height=66"></iframe></center><center><iframe src="//player.bilibili.com/player.html?aid=763769001&bvid=BV1gr4y117d5&cid=430929972&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe></center>]]></content>
    
    
    <categories>
      
      <category>else</category>
      
    </categories>
    
    
    <tags>
      
      <tag>story</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Weekly-210926</title>
    <link href="/2021/09/26/1-Weekly-210926/"/>
    <url>/2021/09/26/1-Weekly-210926/</url>
    
    <content type="html"><![CDATA[<h1 id="关于GAN的部分论文梳理"><a href="#关于GAN的部分论文梳理" class="headerlink" title="关于GAN的部分论文梳理"></a>关于GAN的部分论文梳理</h1><span id="more"></span><p><strong>参考论文：</strong> How Generative Adversarial Networks and Their Variants Work: An Overview（<a href="https://arxiv.org/pdf/1711.05914.pdf）">https://arxiv.org/pdf/1711.05914.pdf）</a></p><p><img src="/images/1-weeklynote/GAN论文整理.jpg" alt=""></p><h1 id="关于特征解缠"><a href="#关于特征解缠" class="headerlink" title="关于特征解缠"></a>关于特征解缠</h1><p><strong>参考论文：</strong> Orthogonal Jacobian Regularization for Unsupervised Disentanglement in Image Generation（ICCV2021）<br>（<a href="https://arxiv.org/abs/2108.07668）">https://arxiv.org/abs/2108.07668）</a></p><ul><li><strong>概念</strong> 在给定的数据集中，学习解开的特征表示，每一维度对应一个变异因子（FOV）的变化，且FOV之间相互独立，各自代表某一图像连续变换的特征。</li><li><strong>适用领域</strong> 域适应、可控图像生成、图像处理。</li><li><strong>相关工作</strong> VAE相关（ β-VAE ,  FactorVAE,  β-TCVAE等通过加强潜变量之间的独立性来解缠），GAN相关（ SeFa，Hessian Penalty）</li><li><strong>正交雅可比正则化（OroJaR）</strong> 由于对网络输入的单个维度进行扰动时，其输出的变化是独立的。基于这一直觉，计算输出的雅可比矩阵，表示不同的潜在输入引起的变化，为使这些变化不相关，需约束雅可比矩阵每个维度的向量是正交的。</li><li><strong>评价指标</strong> PPL（感知路径长度），FID，VP（变化可预测性），其中FID是图像生成常用指标，表示生成图像分布与真实图像分布之间的距离，越小越好；PPL表示潜码空间质量，其度量值与形状的一致性与稳定性有关，越小越好；VP即变化可预测性，鼓励z的每一个维度引起的变化可区分，越大越好。</li><li><strong>数据集</strong> Edges+Shoes、CLEVR-Simple、CLEVR-Complex</li></ul><h1 id="下周计划"><a href="#下周计划" class="headerlink" title="下周计划"></a>下周计划</h1><ol><li>阅读关于检测、分割的相关论文；</li><li>阅读Latent Space Disentanglement相关论文。</li></ol>]]></content>
    
    
    <categories>
      
      <category>Weekly Note</category>
      
    </categories>
    
    
    <tags>
      
      <tag>GAN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>my first blog</title>
    <link href="/2021/09/20/0-my-first-blog/"/>
    <url>/2021/09/20/0-my-first-blog/</url>
    
    <content type="html"><![CDATA[<!-- primarysecondarysuccessdangerwarninginfolight --><!-- <p class="note note-warning">人生天地间，忽如远行客</p> --><!-- <span class="label label-primary">Label</span> --><blockquote><h3 id="第一篇博客"><a href="#第一篇博客" class="headerlink" title="第一篇博客"></a>第一篇博客</h3></blockquote><span id="more"></span><blockquote><h4 id="人生天地间，忽如远行客。"><a href="#人生天地间，忽如远行客。" class="headerlink" title="人生天地间，忽如远行客。"></a>人生天地间，忽如远行客。</h4></blockquote><h3 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h3><blockquote><p>小学时无意看到了某本书，名叫《培根随笔》，那时的我或许冒出了星星眼，想着虽然没怎么读懂，但是我也想要写这样的议论文，于是在一个破旧的本子上，写了“随笔”两字，然后就再也没有然后了……</p></blockquote><h3 id="初衷："><a href="#初衷：" class="headerlink" title="初衷："></a>初衷：</h3><blockquote><p>走在路上，满脑子都是怎么规划我的博客，总是想着，做一件事要做到最好，而当我沉浸其中时，就像某个电影里描述的那样，仿佛整个世界都与我无关了。</p><p>那么，怎么设计板块才不会显得混乱？我要记录哪些内容，要表达什么思想，要怎么去组织每篇文章，都需要仔细斟酌。所以，这里应当关系到我爱的世界，无论是虚拟还是现实。</p><p>读书，科研，论文，绘画，哲学，未来，过去……能写在这里的太多太多了，于我而言，它像是一个开放的，精致的世界，可以记录我想记录的一切。它属于我想要展示给世界的那一部分，非常非常有限的一部分，毕竟我太懒了。</p></blockquote><h3 id="关于我："><a href="#关于我：" class="headerlink" title="关于我："></a>关于我：</h3><blockquote><p>计算机萌新级别，CV方向，人菜瘾还大。如有专业相关建议，欢迎联系：<br><!-- <span class="label label-primary">Label</span> --></p><ul><li>QQ：<strong>1529814119</strong></li><li>邮箱：<strong>sdgxwym@163.com</strong></li></ul></blockquote><!-- <div>            <input type="checkbox" disabled checked="checked">text          </div> --><!-- <a class="btn" href="CatsOfSDU" title="title">next</a><a href="#">Post not found: markdown-learning-by-maxiang 点击这里查看这篇文章</a> --><!-- $$E=mc^2$$ --><!-- <pre><code class="mermaid" >ganttdateFormat  YYYY-MM-DDtitle Adding GANTT diagram to mermaidsection A sectionCompleted task            :done,    des1, 2014-01-06,2014-01-08Active task               :active,  des2, 2014-01-09, 3dFuture task               :         des3, after des2, 5dFuture task2               :         des4, after des3, 5d</code></pre> --><!-- ```mermaid```  -->]]></content>
    
    
    <categories>
      
      <category>Hello World</category>
      
    </categories>
    
    
    <tags>
      
      <tag>test</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
